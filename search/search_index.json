{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ApiLinker","text":"<p>A universal bridge to connect, map, and automate data transfer between any two REST APIs.</p> <p>ApiLinker is an open-source Python package that simplifies API integration. It allows you to connect different systems without writing repetitive boilerplate code.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li> <p>SSE Real-Time Streaming: Built-in SSE connector with reconnection, chunked processing, and backpressure controls.</p> </li> <li> <p>\ud83d\udd04 Universal Connectivity: Connect any two REST APIs.</p> </li> <li>\ud83d\uddfa\ufe0f Powerful Mapping: Transform data with field mapping and expressions.</li> <li>\ud83d\udd12 Security: Enterprise-grade secret management and auth support.</li> <li>\ud83d\udd0c Plugins: Extensible architecture for custom connectors.</li> <li>\ud83e\uddec Science Ready: Specialized connectors for research APIs (NCBI, arXiv).</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation</li> <li>Quick Start</li> <li>Configuration Guide</li> </ul>"},{"location":"#community","title":"Community","text":"<ul> <li>GitHub Repository</li> <li>Issue Tracker</li> </ul>"},{"location":"architecture/","title":"ApiLinker Architecture","text":"<p>This document explains the architecture and data flow of ApiLinker.</p>"},{"location":"architecture/#system-architecture","title":"System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              \u2502    \u2502                    ApiLinker                     \u2502    \u2502              \u2502\n\u2502              \u2502    \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502    \u2502              \u2502\n\u2502    Source    \u2502    \u2502   \u2502    Source   \u2502          \u2502    Target   \u2502      \u2502    \u2502    Target    \u2502\n\u2502     API      \u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u25ba\u2502  Connector  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u25ba\u2502  Connector  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u25ba\u2502     API      \u2502\n\u2502              \u2502    \u2502   \u2502(ApiConnector)\u2502      \u2502   \u2502(ApiConnector)\u2502      \u2502    \u2502              \u2502\n\u2502              \u2502    \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502    \u2502              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502          \u25b2             \u2502          \u25b2             \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502          \u2502             \u2502          \u2502             \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510       \u2502     \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502              \u2502    \u2502   \u2502    Auth   \u2502       \u2502     \u2502   Auth   \u2502       \u2502\n\u2502 Config File  \u2502    \u2502   \u2502  Manager  \u2502       \u2502     \u2502 Manager  \u2502       \u2502\n\u2502  YAML/JSON   \u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u25ba\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502              \u2502    \u2502                       \u2502                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502                        \u2502\n                    \u2502   \u2502 Field Mapper \u251c\u2500\u2500\u2500\u2500\u2518                        \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                              \u2502\n\u2502              \u2502    \u2502          \u25b2                                     \u2502\n\u2502  Scheduler   \u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                      \u2502\n\u2502              \u2502    \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502   \u2502   Logger  \u2502                                \u2502\n                    \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#actual-component-relationships","title":"Actual Component Relationships","text":"<p><pre><code>           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502  ApiLinker  \u2502\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502             \u2502                   \u2502\n\u25bc             \u25bc                   \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ApiConn. \u2502  \u2502FieldMapper\u2502  \u2502Scheduler \u2502\n\u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n     \u2502            \u2502             \u2502\n     \u25bc            \u25bc             \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502AuthManager\u2502  \u2502Transformers\u2502 \u2502Background\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 Thread   \u2502\n                            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>## Data Flow\n\nThe flow of data through ApiLinker follows these steps:\n\n1. **Configuration**: The system is configured either programmatically or via YAML/JSON files.\n\n2. **Source Connection**: ApiLinker connects to the source API using the configured connector and authentication.\n\n3. **Data Retrieval**: Data is fetched from the source API, with automatic handling of pagination if configured.\n\n4. **Data Transformation**: The mapper applies field mappings and transformations to convert the data structure from the source format to the target format.\n\n5. **Target Connection**: ApiLinker connects to the target API using the configured connector and authentication.\n\n6. **Data Transmission**: The transformed data is sent to the target API.\n\n7. **Result Processing**: Results are collected and returned to the caller or handled by the error handler.\n\n8. **Optional Validation**: When configured, source responses and target requests are validated against JSON Schemas. In strict mode, schema mismatches cause the operation to fail early with readable diffs explaining the mismatch.\n\n## Core Components\n\n### 1. ApiLinker\n\nThe main class that orchestrates the entire process. It:\n- Manages configuration\n- Coordinates between components\n- Handles synchronization processes\n- Provides the public API\n\n### 2. Connectors\n\nResponsible for communication with APIs. They:\n- Establish connections\n- Send requests\n- Handle responses\n- Manage pagination\n- Handle retries\n\n### 3. Mapper\n\nManages field mappings and transformations. It:\n- Maps fields from source to target format\n- Applies transformations\n- Handles nested data structures\n- Implements conditional mapping\n\nValidation complements (but does not replace) mapping. Validation checks structural compatibility of responses/requests; mapping performs the semantic transformation between schemas.\n\n### 4. Transformers\n\nTransform data values during the mapping process. They:\n- Convert data formats\n- Format values\n- Validate data\n- Apply business logic\n\n### 5. Auth Plugins\n\nHandle different authentication methods. They:\n- Generate authentication tokens\n- Format headers and parameters\n- Refresh credentials when needed\n- Validate auth responses\n\n### 6. Plugin Manager\n\nDiscovers and manages plugins. It:\n- Loads plugins from different sources\n- Registers plugins\n- Instantiates plugins\n- Provides plugin discovery mechanisms\n\n### 7. Scheduler\n\nManages periodic execution of synchronization jobs. It:\n- Schedules sync operations\n- Handles cron expressions and intervals\n- Manages execution threads\n- Tracks job status\n\n## Visual Representation of the Mapping Process\n</code></pre> Source Data                  Field Mapping                  Target Data \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 {             \u2502            \u2502 fields: [     \u2502            \u2502 {             \u2502 \u2502  \"id\": 123,   \u2502            \u2502   {           \u2502            \u2502  \"external_id\": \u2502 \u2502  \"name\": \"John\",           \u2502     source: \"id\",          \u2502     \"123\",    \u2502 \u2502  \"created\":   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502     target:   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502  \"fullName\":  \u2502 \u2502    \"2023-01-01\",           \u2502       \"external_id\"\u2502       \u2502     \"JOHN DOE\", \u2502 \u2502  \"last_name\": \u2502            \u2502   },          \u2502            \u2502  \"created_at\":\u2502 \u2502    \"Doe\",     \u2502            \u2502   {           \u2502            \u2502     1672531200\u2502 \u2502  \"active\":    \u2502            \u2502     source:   \u2502            \u2502 }             \u2502 \u2502    true       \u2502            \u2502       \"name\", \u2502            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 }             \u2502            \u2502     target:   \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502       \"fullName\", \u2502                              \u2502     transform:  \u2502                              \u2502       \"uppercase\" \u2502                              \u2502   },          \u2502                              \u2502   {           \u2502                              \u2502     source:   \u2502                              \u2502       \"created\", \u2502                              \u2502     target:   \u2502                              \u2502       \"created_at\", \u2502                              \u2502     transform:  \u2502                              \u2502       \"iso_to_timestamp\" \u2502                              \u2502   }           \u2502                              \u2502 ]             \u2502                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 <pre><code>## Plugin System Architecture\n</code></pre> \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                      Plugin Manager                           \u2502 \u2502                                                               \u2502 \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502 \u2502   \u2502  Plugin Registry \u2502    \u2502  Plugin Loader  \u2502    \u2502  Plugin Validator\u2502   \u2502 \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502 \u2502            \u2502                     \u2502                       \u2502            \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502                     \u2502                       \u2502              \u25bc                     \u25bc                       \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   Base Plugin   \u2502    \u2502  Plugin Sources  \u2502    \u2502  Plugin Types   \u2502 \u2502    Interface    \u2502    \u2502                  \u2502    \u2502                 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502         \u25b2              \u2502  \u2502 Built-in    \u2502 \u2502    \u2502 \u2502 Transformer \u2502 \u2502         \u2502              \u2502  \u2502 Plugins     \u2502 \u2502    \u2502 \u2502 Plugins     \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502    \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502               \u2502      \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502  Plugin Base  \u2502      \u2502  \u2502 User        \u2502 \u2502    \u2502 \u2502 Connector   \u2502 \u2502 \u2502     Class     \u2502      \u2502  \u2502 Plugins     \u2502 \u2502    \u2502 \u2502 Plugins     \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502    \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502         \u25b2              \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502         \u2502              \u2502  \u2502 Third-party \u2502 \u2502    \u2502 \u2502 Auth        \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502  \u2502 Plugins     \u2502 \u2502    \u2502 \u2502 Plugins     \u2502 \u2502 \u2502   Specific    \u2502      \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502    \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502  Plugin Types \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 <pre><code>## Configuration Architecture\n</code></pre> \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502     Config Sources        \u2502 \u2502                           \u2502 \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502 \u2502  \u2502  YAML/JSON Files  \u2502    \u2502 \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502 \u2502                           \u2502 \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502 \u2502  \u2502  API Parameters   \u2502    \u2502 \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502 \u2502                           \u2502 \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502 \u2502  \u2502  Environment Vars \u2502    \u2502 \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502             \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    Config Processor       \u2502     \u2502   Template Engine     \u2502 \u2502                           \u2502     \u2502                       \u2502 \u2502 - Parse config formats    \u2502     \u2502 - Process variables   \u2502 \u2502 - Validate structure      \u2502\u25c4\u2500\u2500\u2500\u2500\u2524 - Handle expressions  \u2502 \u2502 - Apply defaults          \u2502     \u2502 - Environment lookups \u2502 \u2502 - Normalize values        \u2502     \u2502                       \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502             \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502              Config Objects               \u2502 \u2502                                           \u2502 \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502  \u2502  Source Config \u2502    \u2502  Target Config \u2502 \u2502 \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502                                           \u2502 \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502  \u2502 Mapping Config \u2502    \u2502 Schedule Config\u2502 \u2502 \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 <pre><code>## Error Handling Flow\n</code></pre> \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   Operation   \u2502     \u2502    Error      \u2502     \u2502  Error Type   \u2502 \u2502   Execution   \u2502\u2500\u2500\u2500\u2500\u25ba\u2502   Detection   \u2502\u2500\u2500\u2500\u2500\u25ba\u2502 Classification \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502                                           \u2502         \u2502 Success                                   \u2502 Error         \u2502                                           \u25bc         \u2502                                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502                                   \u2502  Custom Error \u2502         \u2502                                   \u2502   Handlers    \u2502         \u2502                                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502                                           \u2502         \u2502                                           \u25bc         \u2502                                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502                                   \u2502   Retry       \u2502 Yes         \u2502                                   \u2502  Decision     \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502                                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502         \u2502                                           \u2502 No           \u2502         \u2502                                           \u25bc              \u2502         \u2502                                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502         \u2502                                   \u2502   Error       \u2502      \u2502         \u2502                                   \u2502   Reporting   \u2502      \u2502         \u2502                                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502         \u25bc                                                          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                  \u2502 \u2502   Result      \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   Processing  \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 <pre><code>## Integration with External Systems\n\nApiLinker can integrate with various external systems:\n</code></pre>                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502               \u2502                   \u2502  Data Sources \u2502                   \u2502               \u2502                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                           \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502               \u2502  \u2502             \u2502  \u2502               \u2502 \u2502  Databases    \u2502\u25c4\u2500\u2524  ApiLinker  \u251c\u2500\u25ba\u2502  CRM Systems  \u2502 \u2502               \u2502  \u2502             \u2502  \u2502               \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                           \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502               \u2502  \u2502             \u2502  \u2502               \u2502 \u2502 File Systems  \u2502\u25c4\u2500\u2524  Plugins &amp;  \u251c\u2500\u25ba\u2502  Messaging    \u2502 \u2502               \u2502  \u2502 Extensions  \u2502  \u2502  Systems      \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                           \u2502                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502               \u2502                   \u2502  Analytics &amp;  \u2502                   \u2502  Reporting    \u2502                   \u2502               \u2502                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ```</p>"},{"location":"benchmarks/","title":"Benchmarks","text":""},{"location":"benchmarks/#benchmarks","title":"Benchmarks","text":"<p>This document describes how to run reproducible, local benchmarks for ApiLinker.</p>"},{"location":"benchmarks/#goals","title":"Goals","text":"<ul> <li>Measure end-to-end sync latency for small/medium/large payloads</li> <li>Measure throughput (requests per second) and memory peak</li> <li>Provide repeatable results using a built-in mock HTTP server</li> </ul>"},{"location":"benchmarks/#scenarios","title":"Scenarios","text":"<p>Benchmarks run against a lightweight local server that emulates a simple <code>GET /users</code> and <code>POST /users</code> workflow. Scenarios:</p> <ul> <li>small_batch: 10 users (sync)</li> <li>medium_batch: 1,000 users (sync)</li> <li>large_batch: 10,000 users (sync)</li> <li>async_small_batch: 10 users (async + concurrency=10)</li> <li>async_large_batch: 10,000 users (async + concurrency=100)</li> </ul>"},{"location":"benchmarks/#running","title":"Running","text":"<pre><code>python -m benchmarks.run_benchmarks --iterations 10 --out benchmarks/results\n</code></pre> <p>This will produce:</p> <ul> <li><code>benchmarks/results/results.json</code>: machine-readable stats</li> <li><code>benchmarks/results/README.md</code>: human-readable summary</li> <li><code>benchmarks/results/mean_latency_ms.png</code>, <code>throughput_rps.png</code> if <code>matplotlib</code> is installed</li> </ul>"},{"location":"benchmarks/#reported-metrics","title":"Reported Metrics","text":"<ul> <li>mean_ms, median_ms, p95_ms, min_ms, max_ms</li> <li>duration_seconds for all iterations</li> <li>rps (iterations per second)</li> <li>peak_memory_mb (via <code>tracemalloc</code>)</li> </ul>"},{"location":"benchmarks/#methodology-and-notes","title":"Methodology and Notes","text":"<ul> <li>Each scenario warms up before timing to stabilize the interpreter and caches.</li> <li>The async scenarios use <code>httpx.AsyncClient</code> with bounded concurrency to drive POSTs.</li> <li>Install optional charts support: <code>pip install matplotlib</code>.</li> </ul>"},{"location":"benchmarks/#notes","title":"Notes","text":"<ul> <li>These benchmarks are local and synthetic; they do not hit third-party APIs</li> <li>For external API benchmarks, adapt <code>benchmarks/scenarios.py</code> to point to real endpoints with proper auth and rate limiting</li> </ul>"},{"location":"citation/","title":"How to Cite ApiLinker","text":"<p>If you use ApiLinker in your research, please cite it using the information below.</p>"},{"location":"citation/#citation-formats","title":"Citation Formats","text":""},{"location":"citation/#bibtex","title":"BibTeX","text":"<pre><code>@software{apilinker2025,\n  author = {Kartas, Kyriakos},\n  title = {ApiLinker: A Universal Bridge for REST API Integrations},\n  version = {0.5.3},\n  year = {2025},\n  url = {https://github.com/kkartas/apilinker},\n  doi = {10.5281/zenodo.XXXXXXX}\n}\n</code></pre>"},{"location":"citation/#apa","title":"APA","text":"<p>Kartas, K. (2025). ApiLinker: A Universal Bridge for REST API Integrations (Version 0.5.3) [Computer software]. https://github.com/kkartas/apilinker</p>"},{"location":"citation/#ieee","title":"IEEE","text":"<p>K. Kartas, \"ApiLinker: A Universal Bridge for REST API Integrations,\" version 0.5.3, 2025. [Online]. Available: https://github.com/kkartas/apilinker</p>"},{"location":"citation/#chicago","title":"Chicago","text":"<p>Kartas, Kyriakos. 2025. ApiLinker: A Universal Bridge for REST API Integrations. Version 0.5.3. https://github.com/kkartas/apilinker.</p>"},{"location":"citation/#citation-file","title":"Citation File","text":"<p>A machine-readable citation file is available at the root of the repository: <code>CITATION.cff</code></p> <p>This file can be used with tools like Zotero, Mendeley, and GitHub's \"Cite this repository\" feature.</p>"},{"location":"citation/#research-use-cases","title":"Research Use Cases","text":"<p>If you use ApiLinker in published research, we'd love to hear about it! Please consider:</p> <ol> <li>Opening an issue on GitHub to share your paper</li> <li>Adding your publication to our list of scientific uses</li> <li>Citing specific features you used (e.g., research connectors, benchmarking framework)</li> </ol>"},{"location":"citation/#version-specific-citations","title":"Version-Specific Citations","text":"<p>When citing ApiLinker, please include the version number to ensure reproducibility. You can find the version in:</p> <pre><code>import apilinker\nprint(apilinker.__version__)\n</code></pre>"},{"location":"citation/#acknowledgments","title":"Acknowledgments","text":"<p>If you use ApiLinker's research connectors (NCBI, arXiv, PubChem, etc.), please also cite the respective API providers according to their guidelines:</p> <ul> <li>NCBI/PubMed: NCBI Citation Guidelines</li> <li>arXiv: arXiv Citation</li> <li>CrossRef: CrossRef Citation</li> <li>Semantic Scholar: Semantic Scholar API</li> </ul>"},{"location":"citation/#license","title":"License","text":"<p>ApiLinker is released under the MIT License.</p>"},{"location":"comparison/","title":"ApiLinker vs. Other Integration Tools","text":"<p>This guide compares ApiLinker with other popular API integration tools to help you choose the right solution for your needs.</p>"},{"location":"comparison/#feature-comparison","title":"Feature Comparison","text":"Feature ApiLinker Zapier n8n Apache Airflow MuleSoft Basics Open Source \u2705 \u274c \u2705 (Community) \u2705 \u274c Self-Hosted \u2705 \u274c \u2705 \u2705 \u2705 (Enterprise) Code-Driven \u2705 \u274c Partial \u2705 Partial Config-Driven \u2705 \u2705 \u2705 Partial \u2705 Learning Curve Medium Low Medium High High Features REST API Support \u2705 \u2705 \u2705 \u2705 \u2705 GraphQL Support \u274c Partial \u2705 \u2705 \u2705 SOAP Support \u274c \u274c \u2705 \u2705 \u2705 Pagination Handling \u2705 (Basic) \u2705 \u2705 \u2705 \u2705 Field Mapping \u2705 \u2705 \u2705 \u2705 \u2705 Data Transformations \u2705 (Built-in &amp; Custom) Limited \u2705 \u2705 \u2705 Scheduling \u2705 (Interval, Cron) \u2705 \u2705 \u2705 \u2705 SSE Streaming \u2705 \u274c Partial \u274c Partial Error Handling \u2705 (Basic Retries) Limited \u2705 \u2705 \u2705 Authentication API Key \u2705 \u2705 \u2705 \u2705 \u2705 Bearer Token \u2705 \u2705 \u2705 \u2705 \u2705 Basic Auth \u2705 \u2705 \u2705 \u2705 \u2705 OAuth2 \u2705 (Client Credentials) \u2705 \u2705 \u2705 \u2705 Custom Auth \u274c \u274c \u2705 \u2705 \u2705 Extensions Custom Plugins \u2705 Limited \u2705 \u2705 \u2705 Developer SDK \u274c Limited \u2705 \u2705 \u2705 Use Cases Personal Projects \u2705 \u2705 \u2705 \u274c \u274c Small Business \u2705 \u2705 \u2705 \u274c \u274c Enterprise \u2705 \u2705 \u2705 \u2705 \u2705 Research/Academic \u2705 Limited Limited \u2705 Limited Performance Large Data Handling \u2705 (Pagination) Limited Limited \u2705 \u2705 Low Resource Usage \u2705 N/A Moderate High High Dependencies External Dependencies Minimal (httpx, pydantic, yaml) N/A Moderate Many Many Docker Support \u274c N/A \u2705 \u2705 \u2705"},{"location":"comparison/#when-to-use-apilinker","title":"When to Use ApiLinker","text":"<p>ApiLinker is the best choice when you need:</p> <ol> <li>Code-first approach - You prefer writing and maintaining Python code rather than using GUI tools</li> <li>Minimal dependencies - You need a lightweight solution with few external dependencies</li> <li>Full customizability - You need to implement custom logic for data transformations or API handling</li> <li>Research applications - You need reproducible data pipelines for research and academic work</li> <li>Embedding in other applications - You need to integrate API connectivity into your existing Python application</li> </ol>"},{"location":"comparison/#when-to-consider-alternatives","title":"When to Consider Alternatives","text":"<p>Consider other tools when:</p> <ol> <li>GUI-based workflow - You prefer a visual interface for designing integrations (consider Zapier or n8n)</li> <li>Pre-built connectors - You need a large library of pre-built API integrations (consider Zapier)</li> <li>Complex orchestration - You need advanced workflow orchestration with DAGs (consider Apache Airflow)</li> <li>Enterprise integration - You need a full enterprise integration platform with governance (consider MuleSoft)</li> </ol>"},{"location":"comparison/#detailed-comparisons","title":"Detailed Comparisons","text":""},{"location":"comparison/#apilinker-vs-zapier","title":"ApiLinker vs. Zapier","text":"<p>ApiLinker advantages: - Open source and self-hosted - Full code control in Python - Lower long-term cost - More flexible transformations - No limits on task executions</p> <p>Zapier advantages: - No-code visual interface - 3,000+ pre-built integrations - Hosted solution (no infrastructure management) - Simpler for non-developers - Built-in templates for common workflows</p>"},{"location":"comparison/#apilinker-vs-n8n","title":"ApiLinker vs. n8n","text":"<p>ApiLinker advantages: - Python-native (better for data science/ML workflows) - Lighter weight with fewer dependencies - Designed for embedding in other applications - Simpler learning curve for Python developers</p> <p>n8n advantages: - Visual workflow editor - More built-in integrations - Browser-based interface - Support for more protocols out of the box</p>"},{"location":"comparison/#apilinker-vs-apache-airflow","title":"ApiLinker vs. Apache Airflow","text":"<p>ApiLinker advantages: - Focused on API integration specifically - Much lighter weight and simpler to use - Lower learning curve - Better for simple integration tasks</p> <p>Apache Airflow advantages: - More powerful workflow orchestration - Better for complex multi-step pipelines - More monitoring and observability features - Stronger community and ecosystem</p>"},{"location":"comparison/#code-example-comparisons","title":"Code Example Comparisons","text":""},{"location":"comparison/#simple-api-integration","title":"Simple API Integration","text":""},{"location":"comparison/#apilinker","title":"ApiLinker","text":"<pre><code>from apilinker import ApiLinker\n\n# Initialize\nlinker = ApiLinker()\n\n# Configure source\nlinker.add_source(\n    type=\"rest\",\n    base_url=\"https://api.source.com\",\n    endpoints={\"get_users\": {\"path\": \"/users\"}}\n)\n\n# Configure target\nlinker.add_target(\n    type=\"rest\",\n    base_url=\"https://api.target.com\",\n    endpoints={\"create_users\": {\"path\": \"/users\", \"method\": \"POST\"}}\n)\n\n# Map fields\nlinker.add_mapping(\n    source=\"get_users\",\n    target=\"create_users\",\n    fields=[\n        {\"source\": \"id\", \"target\": \"user_id\"},\n        {\"source\": \"name\", \"target\": \"full_name\"}\n    ]\n)\n\n# Run the sync\nresult = linker.sync()\n</code></pre>"},{"location":"comparison/#apache-airflow","title":"Apache Airflow","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\nimport requests\nimport json\n\ndef fetch_users():\n    response = requests.get(\"https://api.source.com/users\")\n    return response.json()\n\ndef transform_users(ti):\n    users = ti.xcom_pull(task_ids=['fetch_users'])[0]\n    transformed = []\n    for user in users:\n        transformed.append({\n            \"user_id\": user[\"id\"],\n            \"full_name\": user[\"name\"]\n        })\n    return transformed\n\ndef push_users(ti):\n    users = ti.xcom_pull(task_ids=['transform_users'])[0]\n    for user in users:\n        requests.post(\"https://api.target.com/users\", json=user)\n\nwith DAG('api_sync', start_date=datetime(2023, 1, 1), schedule_interval='@daily') as dag:\n    fetch = PythonOperator(\n        task_id='fetch_users',\n        python_callable=fetch_users\n    )\n\n    transform = PythonOperator(\n        task_id='transform_users',\n        python_callable=transform_users\n    )\n\n    push = PythonOperator(\n        task_id='push_users',\n        python_callable=push_users\n    )\n\n    fetch &gt;&gt; transform &gt;&gt; push\n</code></pre>"},{"location":"comparison/#with-error-handling-and-scheduling","title":"With Error Handling and Scheduling","text":""},{"location":"comparison/#apilinker_1","title":"ApiLinker","text":"<pre><code>from apilinker import ApiLinker\n\n# Initialize with error handling\nlinker = ApiLinker()\n\n# Add error handler\ndef handle_error(error, context):\n    print(f\"Error: {error}\")\n    return True  # Retry\n\nlinker.add_error_handler(handle_error)\n\n# Configure APIs and mapping\n# ...\n\n# Add scheduling\nlinker.add_schedule(interval_minutes=60)\n\n# Start scheduled sync\nlinker.start_scheduled_sync()\n</code></pre>"},{"location":"comparison/#n8n-javascript","title":"n8n (JavaScript)","text":"<pre><code>// Node 1: HTTP Request (Source API)\nconst sourceResponse = await $node[\"HTTP Request\"].makeRequest({\n  url: \"https://api.source.com/users\",\n  method: \"GET\"\n});\n\n// Node 2: Function (Transform data)\nconst transformedData = sourceResponse.data.map(user =&gt; {\n  return {\n    user_id: user.id,\n    full_name: user.name\n  };\n});\n\n// Node 3: HTTP Request (Target API)\nfor (const user of transformedData) {\n  try {\n    await $node[\"HTTP Request 2\"].makeRequest({\n      url: \"https://api.target.com/users\",\n      method: \"POST\",\n      body: user\n    });\n  } catch (error) {\n    // Error handling\n    $node[\"Error Handler\"].record(error);\n\n    // Retry logic\n    await new Promise(r =&gt; setTimeout(r, 5000));\n    await $node[\"HTTP Request 2\"].makeRequest({\n      url: \"https://api.target.com/users\",\n      method: \"POST\",\n      body: user\n    });\n  }\n}\n</code></pre>"},{"location":"comparison/#conclusion","title":"Conclusion","text":"<p>ApiLinker offers a unique combination of simplicity, flexibility, and performance with a code-first approach that makes it ideal for developers, data engineers, and researchers who need programmatic control over their API integrations.</p> <p>While visual tools like Zapier and n8n excel for business users and simple integrations, ApiLinker shines when you need deeper customization, tighter integration with Python code, or when working with complex data transformations.</p> <p>Choose ApiLinker when you value: - Lightweight implementation - Python-native development - Full code control - Reproducible data pipelines - Integration with existing Python applications</p>"},{"location":"configuration/","title":"Configuration Guide","text":"<p>ApiLinker uses a YAML-based configuration format to define API connections, field mappings, scheduling, and other settings. This guide explains all available configuration options in detail.</p>"},{"location":"configuration/#configuration-file-structure","title":"Configuration File Structure","text":"<p>A complete ApiLinker configuration file has the following top-level sections:</p> <pre><code>source:\n  # Source API configuration\n\ntarget:\n  # Target API configuration\n\nmapping:\n  # Field mappings between APIs\n\nschedule:\n  # Optional scheduling configuration\n\nlogging:\n  # Optional logging configuration\n</code></pre>"},{"location":"configuration/#source-and-target-api-configuration","title":"Source and Target API Configuration","text":"<p>Both <code>source</code> and <code>target</code> sections use the same format:</p> <pre><code>source:  # or target:\n  type: rest\n  base_url: https://api.example.com/v1\n  auth:\n    # Authentication configuration\n  endpoints:\n    # Endpoint definitions\n  timeout: 30  # Request timeout in seconds (optional)\n  retry_count: 3  # Number of retries on failure (optional)\n  retry_delay: 1  # Delay between retries in seconds (optional)\n</code></pre>"},{"location":"configuration/#authentication","title":"Authentication","text":"<p>ApiLinker supports multiple authentication methods:</p>"},{"location":"configuration/#api-key","title":"API Key","text":"<pre><code>auth:\n  type: api_key\n  key: your_api_key  # Or use ${API_KEY_ENV_VAR} for environment variables\n  header: X-API-Key  # Header name (default: X-API-Key)\n  # OR for query parameter:\n  in: query\n  param_name: apikey\n</code></pre>"},{"location":"configuration/#bearer-token","title":"Bearer Token","text":"<pre><code>auth:\n  type: bearer\n  token: your_bearer_token  # Or use ${TOKEN_ENV_VAR}\n</code></pre>"},{"location":"configuration/#basic-authentication","title":"Basic Authentication","text":"<pre><code>auth:\n  type: basic\n  username: your_username  # Or use ${USERNAME_ENV_VAR}\n  password: your_password  # Or use ${PASSWORD_ENV_VAR}\n</code></pre>"},{"location":"configuration/#oauth2-client-credentials","title":"OAuth2 Client Credentials","text":"<pre><code>auth:\n  type: oauth2_client_credentials\n  client_id: your_client_id  # Or use ${CLIENT_ID_ENV_VAR}\n  client_secret: your_client_secret  # Or use ${CLIENT_SECRET_ENV_VAR}\n  token_url: https://auth.example.com/oauth/token\n  scope: read write  # Optional\n</code></pre>"},{"location":"configuration/#endpoint-configuration","title":"Endpoint Configuration","text":"<p>Define endpoints for each API:</p> <pre><code>endpoints:\n  list_users:  # Endpoint name used in mapping\n    path: /users\n    method: GET  # Default: GET\n    params:  # Query parameters\n      limit: 100\n      updated_since: \"{{last_sync}}\"  # Template variable\n    headers:  # Additional headers\n      Accept: application/json\n\n    # Pagination configuration (optional)\n    pagination:\n      data_path: data  # Path to data items in response\n      next_page_path: meta.next_page  # Path to next page token/URL\n      page_param: page  # Query parameter for page number/token\n      max_pages: 10  # Maximum pages to fetch (optional)\n\n    # Response configuration (optional)\n    response_path: results  # Path to extract from response\n\n    # JSON Schema validation (optional)\n    response_schema:\n      type: object\n      properties:\n        results:\n          type: array\n          items:\n            type: object\n            properties:\n              id: { type: string }\n              name: { type: string }\n</code></pre>"},{"location":"configuration/#sse-streaming-endpoints","title":"SSE Streaming Endpoints","text":"<p>For Server-Sent Events streams, add an optional <code>sse</code> block to an endpoint:</p> <pre><code>endpoints:\n  events:\n    path: /stream\n    method: GET\n    sse:\n      reconnect: true\n      reconnect_delay: 1.0\n      max_reconnect_attempts: 10\n      read_timeout: 60\n      decode_json: true\n      chunk_size: 50\n      backpressure_buffer_size: 500\n      drop_policy: block  # block | drop_oldest\n</code></pre> <p>This endpoint can be consumed with <code>stream_sse(...)</code> (event-by-event) or <code>consume_sse(...)</code> (chunked/backpressure-aware).</p> <p>For endpoints that send data:</p> <pre><code>endpoints:\n  create_user:\n    path: /users\n    method: POST\n    headers:\n      Content-Type: application/json\n\n    # Optional body template\n    body_template:\n      source: \"apilinker\"\n      created_by: \"integration\"\n\n    # Optional JSON Schema for validating request payloads\n    request_schema:\n      type: object\n      properties:\n        external_id: { type: string }\n        title: { type: string }\n      required: [external_id, title]\n</code></pre>"},{"location":"configuration/#field-mappings","title":"Field Mappings","text":"<p>Mappings define how data is transformed between source and target:</p> <pre><code>mapping:\n  - source: list_users  # Source endpoint name\n    target: create_user  # Target endpoint name\n    fields:  # Field mappings\n      # Simple field mapping\n      - source: id\n        target: external_id\n\n      # Nested field mapping\n      - source: profile.name\n        target: user.full_name\n\n      # Field with transformation\n      - source: created_at\n        target: metadata.created\n        transform: iso_to_timestamp\n\n      # Multiple transformations\n      - source: tags\n        target: labels\n        transform:\n          - lowercase\n          - none_if_empty\n\n      # Conditional field (only included if condition is met)\n      - source: phone\n        target: contact.phone\n        condition:\n          field: phone\n          operator: exists  # exists, not_exists, eq, ne, gt, lt\n          # value: \"\"  # Comparison value (for eq, ne, gt, lt)\n\n      # Include null values (by default nulls are skipped)\n      - source: status\n        target: status\n        include_nulls: true\n</code></pre>"},{"location":"configuration/#built-in-transformers","title":"Built-in Transformers","text":"<p>ApiLinker includes several built-in transformers:</p> Name Description <code>iso_to_timestamp</code> Convert ISO date string to Unix timestamp <code>timestamp_to_iso</code> Convert Unix timestamp to ISO date string <code>lowercase</code> Convert string to lowercase <code>uppercase</code> Convert string to uppercase <code>strip</code> Remove whitespace from start/end of string <code>to_string</code> Convert value to string <code>to_int</code> Convert value to integer <code>to_float</code> Convert value to float <code>to_bool</code> Convert value to boolean <code>default_empty_string</code> Return empty string if value is null <code>default_zero</code> Return 0 if value is null <code>none_if_empty</code> Return null if value is empty string"},{"location":"configuration/#scheduling","title":"Scheduling","text":"<p>Configure automatic sync intervals:</p> <pre><code># Run every X minutes/hours/days\nschedule:\n  type: interval\n  minutes: 30\n  # Or use hours: 1, days: 1, seconds: 30\n\n# Or use cron expression\nschedule:\n  type: cron\n  expression: \"0 */6 * * *\"  # Every 6 hours\n\n# Or run once at a specific time\nschedule:\n  type: once\n  datetime: \"2023-12-31T23:59:59\"\n</code></pre>"},{"location":"configuration/#logging","title":"Logging","text":"<p>Configure logging behavior:</p> <pre><code>logging:\n  level: INFO  # DEBUG, INFO, WARNING, ERROR\n  file: logs/apilinker.log  # Optional log file path\n</code></pre>"},{"location":"configuration/#provenance-idempotency","title":"Provenance &amp; Idempotency","text":"<p>Add optional provenance capture and idempotency to improve reproducibility and safe replays:</p> <pre><code>provenance:\n  output_dir: runs/                 # Write a sidecar JSON per run\n  jsonl_log: logs/runs.jsonl        # Append-only JSONL event log\n\nidempotency:\n  enabled: true\n  salt: \"example-integration\"       # Optional salt for key generation\n</code></pre> <p>Notes: - The config hash is computed from the active config file; git SHA is recorded when the repository is available. - Idempotency de-duplicates items within the same process. For distributed runs, use an external store or target-side idempotency.</p>"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"<p>You can use environment variables in your configuration file:</p> <pre><code>auth:\n  type: bearer\n  token: ${API_TOKEN}  # Will be replaced with value of API_TOKEN env var\n</code></pre>"},{"location":"configuration/#template-variables","title":"Template Variables","text":"<p>Some special template variables are available:</p> <ul> <li><code>{{last_sync}}</code>: Timestamp of the last successful sync</li> <li><code>{{now}}</code>: Current timestamp</li> <li><code>{{yesterday}}</code>: Timestamp 24 hours ago</li> </ul>"},{"location":"configuration/#complete-example","title":"Complete Example","text":"<pre><code>source:\n  type: rest\n  base_url: https://api.source.com/v1\n  auth:\n    type: api_key\n    header: X-API-Key\n    key: ${SOURCE_API_KEY}\n  endpoints:\n    list_products:\n      path: /products\n      method: GET\n      params:\n        updated_since: \"{{last_sync}}\"\n        limit: 100\n      pagination:\n        data_path: data\n        next_page_path: meta.next_page\n        page_param: page\n\ntarget:\n  type: rest\n  base_url: https://api.destination.com/v2\n  auth:\n    type: bearer\n    token: ${TARGET_API_TOKEN}\n  endpoints:\n    create_product:\n      path: /products\n      method: POST\n    update_product:\n      path: /products/{id}\n      method: PUT\n\nmapping:\n  - source: list_products\n    target: create_product\n    fields:\n      - source: id\n        target: external_id\n      - source: name\n        target: title\n      - source: description\n        target: body.content\n      - source: price\n        target: pricing.amount\n        transform: to_float\n      - source: created_at\n        target: metadata.created\n        transform: iso_to_timestamp\n      - source: tags\n        target: categories\n        transform: \n          - lowercase\n          - none_if_empty\n\nschedule:\n  type: cron\n  expression: \"0 */6 * * *\"  # Every 6 hours\n\nlogging:\n  level: INFO\n  file: logs/apilinker.log\n</code></pre>"},{"location":"configuration/#state-resumability","title":"State &amp; Resumability","text":"<p>Persist last sync cursors and checkpoints to resume safely on subsequent runs.</p> <pre><code>state:\n  type: file\n  path: .apilinker/state.json\n  default_last_sync: \"2024-01-01T00:00:00Z\"  # Used if no previous value exists\n</code></pre> <p>Behavior: - The <code>updated_since</code> parameter is injected automatically from <code>state.last_sync[&lt;source_endpoint&gt;]</code> when not provided explicitly. - After a successful sync, <code>last_sync</code> is updated to the current time (UTC ISO 8601). - Checkpoints and DLQ pointers are available via the state store API for advanced workflows.</p>"},{"location":"connectors_index/","title":"Connectors index","text":""},{"location":"connectors_index/#connectors-index","title":"Connectors Index","text":"<p>This page lists built-in connectors with links to source files and basic usage.</p>"},{"location":"connectors_index/#scientific-connectors","title":"Scientific Connectors","text":"<ul> <li>NCBI (PubMed, GenBank) \u2014 <code>apilinker/connectors/scientific/ncbi.py</code></li> <li>Usage: <code>from apilinker import NCBIConnector</code></li> <li>arXiv \u2014 <code>apilinker/connectors/scientific/arxiv.py</code></li> <li>Usage: <code>from apilinker import ArXivConnector</code></li> <li>CrossRef \u2014 <code>apilinker/connectors/scientific/crossref.py</code></li> <li>Usage: <code>from apilinker import CrossRefConnector</code></li> <li>Semantic Scholar \u2014 <code>apilinker/connectors/scientific/semantic_scholar.py</code></li> <li>Usage: <code>from apilinker import SemanticScholarConnector</code></li> <li>PubChem \u2014 <code>apilinker/connectors/scientific/pubchem.py</code></li> <li>Usage: <code>from apilinker import PubChemConnector</code></li> <li>ORCID \u2014 <code>apilinker/connectors/scientific/orcid.py</code></li> <li>Usage: <code>from apilinker import ORCIDConnector</code></li> </ul>"},{"location":"connectors_index/#general-research-connectors","title":"General Research Connectors","text":"<ul> <li>GitHub \u2014 <code>apilinker/connectors/general/github.py</code></li> <li>Usage: <code>from apilinker import GitHubConnector</code></li> <li>NASA \u2014 <code>apilinker/connectors/general/nasa.py</code></li> <li>Usage: <code>from apilinker import NASAConnector</code></li> <li>SSE - <code>apilinker/connectors/general/sse.py</code></li> <li>Usage: <code>from apilinker import SSEConnector</code></li> </ul>"},{"location":"connectors_index/#notes","title":"Notes","text":"<ul> <li>All connectors inherit from the common <code>ApiConnector</code> base and expose endpoints via their <code>.endpoints</code> mapping.</li> <li>Authentication varies per provider (see each source file for details).</li> <li>Respect rate limits and terms of service; include a descriptive User-Agent where applicable.</li> </ul> <p>The following connectors are exported at the top level in <code>apilinker/__init__.py</code> and available for import as shown above.</p>"},{"location":"cookbook/","title":"ApiLinker Cookbook","text":"<p>This cookbook provides practical recipes for common integration tasks.</p>"},{"location":"cookbook/#working-with-pagination","title":"Working with Pagination","text":""},{"location":"cookbook/#problem","title":"Problem","text":"<p>You need to fetch large datasets that span multiple pages.</p>"},{"location":"cookbook/#solution","title":"Solution","text":"<pre><code>linker.add_source(\n    type=\"rest\",\n    base_url=\"https://api.example.com\",\n    endpoints={\n        \"get_users\": {\n            \"path\": \"/users\",\n            \"method\": \"GET\",\n            # Pagination configuration based on the API response format\n            \"pagination\": {\n                \"data_path\": \"data\",                # Path to items array in response\n                \"next_page_path\": \"meta.next_page\", # Path to next page URL/token\n                \"page_param\": \"page\"                # Query param name for page number\n            }\n        }\n    }\n)\n\n# ApiLinker will automatically handle pagination\nall_users = linker.fetch(\"get_users\")\n</code></pre>"},{"location":"cookbook/#common-pagination-patterns","title":"Common Pagination Patterns","text":""},{"location":"cookbook/#page-number-pagination","title":"Page Number Pagination","text":"<pre><code>\"pagination\": {\n    \"data_path\": \"data\",\n    \"page_param\": \"page\"  # Will increment: page=1, page=2, etc.\n}\n</code></pre>"},{"location":"cookbook/#next-url-pagination","title":"Next URL Pagination","text":"<pre><code>\"pagination\": {\n    \"data_path\": \"data\",\n    \"next_page_path\": \"links.next\"  # Response contains full next URL\n}\n</code></pre>"},{"location":"cookbook/#implementing-robust-error-handling","title":"Implementing Robust Error Handling","text":""},{"location":"cookbook/#problem_1","title":"Problem","text":"<p>You need reliable API integrations that can handle service outages and temporary network issues.</p>"},{"location":"cookbook/#solution_1","title":"Solution","text":"<p>Use APILinker's robust error handling and recovery system:</p> <pre><code># In your config.yaml\nerror_handling:\n  # Configure circuit breakers to prevent cascading failures\n  circuit_breakers:\n    source_customers_api:\n      failure_threshold: 5        # Open circuit after 5 consecutive failures\n      reset_timeout_seconds: 60   # Wait 60 seconds before testing service again\n      half_open_max_calls: 1      # Allow 1 test call in half-open state\n\n  # Configure error handling strategies by error category\n  recovery_strategies:\n    network:                      # Network connectivity issues\n      - exponential_backoff       # First try with increasing delays\n      - circuit_breaker           # Then use circuit breaker if still failing\n    # rate limiting: not built-in; use server guidance and retries\n      - exponential_backoff       # Back off and retry\n    server:                       # Server errors (5xx)\n      - exponential_backoff\n      - circuit_breaker\n    timeout:                      # Request timeout errors\n      - exponential_backoff\n\n  # Configure Dead Letter Queue for failed operations\n  dlq:\n    directory: \"./dlq\"           # Store failed operations here\n</code></pre>"},{"location":"cookbook/#accessing-error-analytics","title":"Accessing Error Analytics","text":"<pre><code>from apilinker import ApiLinker\n\n# Initialize with error handling config\nlinker = ApiLinker(config_path=\"config.yaml\")\n\n# Get error statistics\nanalytics = linker.get_error_analytics()\nprint(f\"Recent error rate: {analytics['recent_error_rate']} errors/minute\")\nprint(f\"Most common errors: {analytics['top_errors']}\")\n\n# Check for failed operations in DLQ\nitems = linker.dlq.get_items(limit=10)\nif items:\n    print(f\"Found {len(items)} failed operations in DLQ\")\n\n    # Process specific types of failed operations\n    results = linker.process_dlq(operation_type=\"source_customers_api\")\n    print(f\"Processed {results['successful']} items successfully\")\n</code></pre>"},{"location":"cookbook/#handling-different-error-types","title":"Handling Different Error Types","text":"<pre><code>from apilinker.core.error_handling import ErrorCategory, RecoveryStrategy\n\n# Configure specific recovery strategies programmatically\nlinker.error_recovery_manager.set_strategy(\n    ErrorCategory.RATE_LIMIT,\n    [\n        RecoveryStrategy.EXPONENTIAL_BACKOFF,\n        RecoveryStrategy.SKIP  # Skip rate-limited operations\n    ],\n    operation_type=\"fetch_users\"  # Only for this operation\n)\n\n# Execute with enhanced error handling\ntry:\n    result = linker.sync(\"fetch_users\", \"create_users\")\n    print(f\"Synced {result.count} users\")\n\n    # Check if any errors occurred\n    if not result.success:\n        print(f\"Completed with {len(result.errors)} errors\")\n        for error in result.errors:\n            print(f\"- {error['message']}\")\n\nexcept Exception as e:\n    print(f\"Critical error: {str(e)}\")\n</code></pre>"},{"location":"cookbook/#explanation","title":"Explanation","text":"<p>The ApiConnector's _handle_pagination method automatically: 1. Extracts data items from each response using the data_path 2. Determines the next page using next_page_path if available 3. Increments page parameters for subsequent requests 4. Combines results from all pages</p>"},{"location":"cookbook/#handling-api-rate-limits","title":"Handling API Rate Limits","text":""},{"location":"cookbook/#problem_2","title":"Problem","text":"<p>Your API requests are getting rate limited (HTTP 429).</p>"},{"location":"cookbook/#solution_2","title":"Solution","text":"<pre><code>linker.add_source(\n    type=\"rest\",\n    base_url=\"https://api.example.com\",\n    # Configure retry settings\n    retry_count=3,         # Try 3 times before failing\n    retry_delay=2,         # Wait 2 seconds between retries\n    # Add longer timeout for slow APIs\n    timeout=30,            # 30 second timeout\n    endpoints={\n        # Your endpoints here\n    }\n)\n\n# For manual handling of 429s with backoff\ndef handle_rate_limits(func):\n    def wrapper(*args, **kwargs):\n        max_attempts = 3\n        for attempt in range(max_attempts):\n            try:\n                return func(*args, **kwargs)\n            except Exception as e:\n                if \"rate limit\" in str(e).lower() and attempt &lt; max_attempts - 1:\n                    wait_time = (attempt + 1) * 5  # 5, 10, 15 seconds\n                    print(f\"Rate limited. Waiting {wait_time} seconds...\")\n                    time.sleep(wait_time)\n                else:\n                    raise\n    return wrapper\n\n# Apply to your function\n@handle_rate_limits\ndef fetch_data():\n    return linker.fetch(\"get_data\")\n</code></pre>"},{"location":"cookbook/#explanation_1","title":"Explanation","text":"<ul> <li>The retry mechanism is built into ApiConnector for temporary failures</li> <li>For specific rate limit handling, implement a decorator or wrapper function in your app</li> <li>The connector will automatically use exponential backoff between retries</li> </ul>"},{"location":"cookbook/#transforming-nested-json-structures","title":"Transforming Nested JSON Structures","text":""},{"location":"cookbook/#problem_3","title":"Problem","text":"<p>You need to extract and transform data from complex nested JSON structures.</p>"},{"location":"cookbook/#solution_3","title":"Solution","text":"<pre><code>linker.add_mapping(\n    source=\"get_data\",\n    target=\"save_data\",\n    fields=[\n        # Access nested properties with dot notation\n        {\"source\": \"user.profile.name\", \"target\": \"fullName\"},\n\n        # Access array items with indices\n        {\"source\": \"addresses[0].street\", \"target\": \"primary_address\"},\n\n        # Custom transformer for nested objects\n        {\n            \"source\": \"metadata\",  # This is an object\n            \"target\": \"meta_info\", \n            \"transform\": \"flatten_metadata\"\n        }\n    ]\n)\n\n# Define transformer for nested object\ndef flatten_metadata(value, **kwargs):\n    if not value or not isinstance(value, dict):\n        return {}\n\n    result = {}\n    # Extract selected fields with prefixes\n    for key in [\"created\", \"updated\", \"status\"]:\n        if key in value:\n            result[f\"meta_{key}\"] = value[key]\n\n    # Flatten nested object\n    if \"details\" in value and isinstance(value[\"details\"], dict):\n        for k, v in value[\"details\"].items():\n            result[f\"detail_{k}\"] = v\n\n    return result\n\n# Register the transformer\nlinker.mapper.register_transformer(\"flatten_metadata\", flatten_metadata)\n</code></pre>"},{"location":"cookbook/#explanation_2","title":"Explanation","text":"<ul> <li>Dot notation accesses nested properties</li> <li>Array indices access specific items in arrays</li> <li>Custom transformers handle complex transformations</li> </ul>"},{"location":"cookbook/#syncing-only-changed-records","title":"Syncing Only Changed Records","text":""},{"location":"cookbook/#problem_4","title":"Problem","text":"<p>You want to sync only records that have changed since the last sync.</p>"},{"location":"cookbook/#solution_4","title":"Solution","text":"<pre><code># Method 1: Using template variables\nlinker.add_source(\n    type=\"rest\",\n    base_url=\"https://api.example.com\",\n    endpoints={\n        \"get_users\": {\n            \"path\": \"/users\",\n            \"method\": \"GET\",\n            \"params\": {\n                \"updated_since\": \"{{last_sync}}\",  # Template variable\n                \"sort\": \"updated_at\"\n            }\n        }\n    }\n)\n\n# Method 2: Using a filter function\ndef filter_changed_records(data, **kwargs):\n    last_sync = kwargs.get(\"last_sync\")\n    if not last_sync:\n        return data  # Return all if no last sync\n\n    # Convert to datetime for comparison\n    from datetime import datetime\n    last_sync_dt = datetime.fromisoformat(last_sync.replace('Z', '+00:00'))\n\n    # Filter items updated since last sync\n    return [\n        item for item in data\n        if \"updated_at\" in item and \n           datetime.fromisoformat(item[\"updated_at\"].replace('Z', '+00:00')) &gt; last_sync_dt\n    ]\n\n# Register the filter\nlinker.add_source_processor(\"get_users\", filter_changed_records)\n\n# Get last sync time from storage\nlast_sync_time = linker.get_last_sync_time() or \"2023-01-01T00:00:00Z\"\n\n# Run the sync with context\nresult = linker.sync(context={\"last_sync\": last_sync_time})\n</code></pre>"},{"location":"cookbook/#explanation_3","title":"Explanation","text":"<ul> <li>Template variables like <code>{{last_sync}}</code> are replaced with values at runtime</li> <li>Source processors can filter or modify data before mapping</li> <li>Context variables can be passed to processors</li> </ul>"},{"location":"cookbook/#working-with-files-and-binary-data","title":"Working with Files and Binary Data","text":""},{"location":"cookbook/#problem_5","title":"Problem","text":"<p>You need to handle file uploads or downloads.</p>"},{"location":"cookbook/#solution_5","title":"Solution","text":"<pre><code># File download\nlinker.add_source(\n    type=\"rest\",\n    base_url=\"https://api.example.com\",\n    endpoints={\n        \"get_document\": {\n            \"path\": \"/documents/{doc_id}\",\n            \"method\": \"GET\",\n            \"response_type\": \"binary\"  # Treat response as binary\n        }\n    }\n)\n\n# Custom transformer to save files\ndef save_file(binary_data, **kwargs):\n    if not binary_data:\n        return None\n\n    filename = kwargs.get(\"filename\", \"document.pdf\")\n    file_path = f\"downloads/{filename}\"\n\n    # Create directory if it doesn't exist\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n\n    # Write binary data to file\n    with open(file_path, \"wb\") as f:\n        f.write(binary_data)\n\n    return {\n        \"path\": file_path,\n        \"size\": len(binary_data),\n        \"status\": \"downloaded\"\n    }\n\n# Register transformer\nlinker.mapper.register_transformer(\"save_file\", save_file)\n\n# Use it in a mapping\nlinker.add_mapping(\n    source=\"get_document\",\n    target=\"log_download\",\n    fields=[\n        {\n            \"source\": \"_response\",  # Special field with raw response\n            \"target\": \"file_info\",\n            \"transform\": \"save_file\",\n            \"filename\": \"report.pdf\"\n        }\n    ]\n)\n\n# File upload\nlinker.add_target(\n    type=\"rest\",\n    base_url=\"https://api.destination.com\",\n    endpoints={\n        \"upload_file\": {\n            \"path\": \"/upload\",\n            \"method\": \"POST\",\n            \"headers\": {\n                \"Content-Type\": \"application/octet-stream\"\n            }\n        }\n    }\n)\n\n# Read file transformer\ndef read_file(file_path, **kwargs):\n    if not file_path or not isinstance(file_path, str):\n        return None\n\n    try:\n        with open(file_path, \"rb\") as f:\n            return f.read()\n    except Exception as e:\n        print(f\"Error reading file: {e}\")\n        return None\n\n# Register transformer\nlinker.mapper.register_transformer(\"read_file\", read_file)\n</code></pre>"},{"location":"cookbook/#explanation_4","title":"Explanation","text":"<ul> <li>Set <code>response_type: \"binary\"</code> for file downloads</li> <li>Use <code>Content-Type: \"application/octet-stream\"</code> for file uploads</li> <li>Use custom transformers to handle file operations</li> </ul>"},{"location":"cookbook/#conditional-mapping","title":"Conditional Mapping","text":""},{"location":"cookbook/#problem_6","title":"Problem","text":"<p>You need to apply different mappings based on data conditions.</p>"},{"location":"cookbook/#solution_6","title":"Solution","text":"<pre><code>linker.add_mapping(\n    source=\"get_products\",\n    target=\"create_item\",\n    fields=[\n        # Basic fields always included\n        {\"source\": \"id\", \"target\": \"product_id\"},\n        {\"source\": \"name\", \"target\": \"title\"},\n\n        # Only include if value exists\n        {\n            \"source\": \"description\",\n            \"target\": \"description\",\n            \"condition\": {\n                \"field\": \"description\",\n                \"operator\": \"exists\"\n            }\n        },\n\n        # Apply different mapping based on status\n        {\n            \"source\": \"status\",\n            \"target\": \"status\",\n            \"transform\": \"map_active_status\",\n            \"condition\": {\n                \"field\": \"status\",\n                \"operator\": \"equals\",\n                \"value\": \"active\"\n            }\n        },\n\n        # Apply different mapping for inactive items\n        {\n            \"target\": \"status\",\n            \"value\": \"discontinued\",\n            \"condition\": {\n                \"field\": \"status\",\n                \"operator\": \"equals\",\n                \"value\": \"inactive\"\n            }\n        },\n\n        # Complex condition using multiple fields\n        {\n            \"source\": \"price\",\n            \"target\": \"discount_price\",\n            \"transform\": \"calculate_discount\",\n            \"condition\": {\n                \"field\": \"on_sale\",\n                \"operator\": \"equals\",\n                \"value\": True\n            },\n            \"discount_percent\": 15\n        }\n    ]\n)\n\n# Status mapper transformer\ndef map_active_status(value, **kwargs):\n    status_map = {\n        \"active\": \"in_stock\",\n        \"pending\": \"coming_soon\"\n    }\n    return status_map.get(value, value)\n\n# Calculate discount transformer\ndef calculate_discount(value, **kwargs):\n    if not isinstance(value, (int, float)) or value &lt;= 0:\n        return value\n\n    discount = kwargs.get(\"discount_percent\", 10)\n    return round(value * (1 - discount / 100), 2)\n\n# Register transformers\nlinker.mapper.register_transformer(\"map_active_status\", map_active_status)\nlinker.mapper.register_transformer(\"calculate_discount\", calculate_discount)\n</code></pre>"},{"location":"cookbook/#explanation_5","title":"Explanation","text":"<ul> <li>The <code>condition</code> property controls when a field mapping is applied</li> <li>Supported operators: <code>exists</code>, <code>not_exists</code>, <code>equals</code>, <code>not_equals</code>, <code>in</code>, <code>not_in</code></li> <li>You can use fixed values with <code>value</code> property</li> <li>Custom transformers can use additional parameters</li> </ul>"},{"location":"cookbook/#using-environment-variables-for-credentials","title":"Using Environment Variables for Credentials","text":""},{"location":"cookbook/#problem_7","title":"Problem","text":"<p>You need to securely manage API credentials without hardcoding them.</p>"},{"location":"cookbook/#solution_7","title":"Solution","text":"<pre><code># Method 1: Environment variables in configuration\nlinker.add_source(\n    type=\"rest\",\n    base_url=\"https://api.example.com\",\n    auth={\n        \"type\": \"bearer\",\n        \"token\": \"${API_TOKEN}\"  # Will be replaced with API_TOKEN env var\n    }\n)\n\n# Method 2: Load from .env file\nfrom dotenv import load_dotenv\nload_dotenv()  # Loads variables from .env file\n\n# Method 3: Set variables in script (for testing)\nimport os\nos.environ[\"API_TOKEN\"] = \"your_token\"  # Only for testing!\n\n# Method 4: Use a credential manager\ndef get_credential(name):\n    # Implement your secure credential retrieval logic here\n    # Examples: AWS Secrets Manager, HashiCorp Vault, etc.\n    return \"secure_credential\"\n\n# Use retrieved credentials\nlinker.add_source(\n    type=\"rest\",\n    base_url=\"https://api.example.com\",\n    auth={\n        \"type\": \"bearer\",\n        \"token\": get_credential(\"api_token\")\n    }\n)\n</code></pre>"},{"location":"cookbook/#explanation_6","title":"Explanation","text":"<ul> <li>Environment variables are the simplest secure method</li> <li>.env files are convenient for development</li> <li>Never hardcode credentials in source code</li> <li>Consider using a dedicated secrets manager for production</li> </ul>"},{"location":"cookbook/#error-handling-and-validation","title":"Error Handling and Validation","text":""},{"location":"cookbook/#problem_8","title":"Problem","text":"<p>You need to handle errors gracefully and validate data.</p>"},{"location":"cookbook/#solution_8","title":"Solution","text":"<pre><code># Custom error handler\ndef handle_sync_error(error, context):\n    import logging\n    logging.error(f\"Sync error: {error}\")\n\n    # Send notification\n    send_notification(f\"Sync failed: {error}\")\n\n    # Determine whether to retry based on error type\n    if \"rate limit\" in str(error).lower():\n        # Wait longer for rate limits\n        import time\n        time.sleep(60)\n        return True  # Retry\n\n    if \"connection\" in str(error).lower():\n        # Retry connection errors up to 3 times\n        attempt = context.get(\"attempt\", 1)\n        if attempt &lt;= 3:\n            context[\"attempt\"] = attempt + 1\n            return True  # Retry\n\n    return False  # Don't retry other errors\n\n# Register error handler\nlinker.add_error_handler(handle_sync_error)\n\n# Data validation\ndef validate_customer_data(data, **kwargs):\n    if not isinstance(data, list):\n        raise ValueError(\"Expected a list of customers\")\n\n    valid_items = []\n    for item in data:\n        # Skip invalid items\n        if not isinstance(item, dict):\n            continue\n\n        # Require email field\n        if \"email\" not in item or not item[\"email\"]:\n            continue\n\n        # Format validation\n        if \"phone\" in item and item[\"phone\"]:\n            # Clean phone number\n            item[\"phone\"] = ''.join(c for c in item[\"phone\"] if c.isdigit())\n\n        # Normalize fields\n        if \"name\" in item and item[\"name\"]:\n            item[\"name\"] = item[\"name\"].strip().title()\n\n        valid_items.append(item)\n\n    return valid_items\n\n# Register validator\nlinker.add_source_processor(\"get_customers\", validate_customer_data)\n</code></pre>"},{"location":"cookbook/#explanation_7","title":"Explanation","text":"<ul> <li>Error handlers determine whether to retry after errors</li> <li>Error context is preserved between retries</li> <li>Source processors can validate and normalize data</li> <li>Validators can filter out invalid records</li> </ul>"},{"location":"cookbook/#logging-and-debugging","title":"Logging and Debugging","text":""},{"location":"cookbook/#problem_9","title":"Problem","text":"<p>You need to track API operations and diagnose issues.</p>"},{"location":"cookbook/#solution_9","title":"Solution","text":"<pre><code>import logging\nimport time\n\n# Configure logging for ApiLinker\nlogging.basicConfig(\n    level=logging.DEBUG,  # Set to DEBUG for detailed logs\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(\"apilinker.log\"),\n        logging.StreamHandler()\n    ]\n)\n\n# Initialize ApiLinker with log level\nlinker = ApiLinker(log_level=\"DEBUG\", log_file=\"apilinker.log\")\n\n# Add custom timing measurement\nclass SimpleTimer:\n    def __init__(self):\n        self.start_times = {}\n        self.results = {}\n\n    def start(self, operation_name):\n        self.start_times[operation_name] = time.time()\n\n    def end(self, operation_name):\n        if operation_name in self.start_times:\n            duration = time.time() - self.start_times[operation_name]\n            self.results[operation_name] = duration\n            logging.info(f\"{operation_name} completed in {duration:.2f} seconds\")\n            return duration\n        return None\n\n# Use the timer in your code\ntimer = SimpleTimer()\n\n# Create a wrapper for timing operations\ndef timed_operation(func):\n    def wrapper(*args, **kwargs):\n        operation_name = func.__name__\n        timer.start(operation_name)\n        try:\n            result = func(*args, **kwargs)\n            timer.end(operation_name)\n            return result\n        except Exception as e:\n            logging.error(f\"Error in {operation_name}: {e}\")\n            timer.end(operation_name)\n            raise\n    return wrapper\n\n# Use the decorator for operations you want to time\n@timed_operation\ndef fetch_and_process():\n    # Fetch data\n    logging.info(\"Fetching data from source\")\n    source_data = linker.fetch(\"get_data\")\n\n    # Log response summary (without sensitive data)\n    logging.info(f\"Fetched {len(source_data) if isinstance(source_data, list) else 1} records\")\n\n    # Process data\n    logging.info(\"Processing data\")\n    result = linker.sync()\n\n    # Log processing results\n    logging.info(f\"Processed {result.count} records\")\n    if result.errors:\n        logging.warning(f\"Encountered {len(result.errors)} errors\")\n\n    return result\n\n# Execute with debug logging\ntry:\n    result = fetch_and_process()\n    print(f\"Sync completed successfully: {result.count} records\")\n    print(f\"Operation times: {timer.results}\")\nexcept Exception as e:\n    print(f\"Sync failed: {e}\")\n</code></pre>"},{"location":"cookbook/#explanation_8","title":"Explanation","text":"<ul> <li>ApiLinker has built-in logging that can be configured with different levels</li> <li>You can create simple timing utilities to measure performance</li> <li>Use the Python logging module for structured logs</li> <li>Use decorators to consistently measure and log operations</li> </ul>"},{"location":"coverage/","title":"Coverage","text":""},{"location":"coverage/#test-coverage","title":"Test Coverage","text":"<p>We recommend running tests with coverage locally and in CI.</p>"},{"location":"coverage/#locally","title":"Locally","text":"<pre><code>pytest --cov=apilinker --cov-report=term-missing\n</code></pre>"},{"location":"coverage/#in-ci","title":"In CI","text":"<p>The CI workflow runs tests; to enable coverage and fail under a threshold, add:</p> <pre><code>- name: Run tests with coverage\n  run: |\n    pytest --cov=apilinker --cov-report=xml --cov-fail-under=80\n</code></pre> <p>Optionally, upload coverage to a service (Codecov/Coveralls) and add a badge to the README.</p>"},{"location":"development/","title":"Development Guide","text":""},{"location":"development/#code-quality","title":"Code Quality","text":"<p>This project maintains code quality through automated testing and linting tools.</p>"},{"location":"development/#running-tests","title":"Running Tests","text":"<p>Run the full test suite: <pre><code>pytest\n</code></pre></p> <p>Run tests with coverage: <pre><code>pytest --cov=apilinker --cov-report=html\n</code></pre></p>"},{"location":"development/#code-formatting-and-linting","title":"Code Formatting and Linting","text":"<p>Check code style: <pre><code>flake8 apilinker\nmypy apilinker\nblack apilinker --check\n</code></pre></p> <p>Auto-format code: <pre><code>black apilinker\n</code></pre></p>"},{"location":"development/#best-practices","title":"Best Practices","text":"<ol> <li>Write tests for new features - Maintain or improve code coverage</li> <li>Run tests before committing - Ensure changes don't break existing functionality</li> <li>Follow PEP 8 style guidelines - Use black for consistent formatting</li> <li>Add type hints - Help with code maintainability and IDE support</li> </ol>"},{"location":"error_handling/","title":"Error handling","text":""},{"location":"error_handling/#error-handling-and-recovery","title":"Error Handling and Recovery","text":"<p>This page explains how to configure circuit breakers, retries, exponential backoff, and the Dead Letter Queue (DLQ).</p>"},{"location":"error_handling/#quick-start","title":"Quick start","text":"<p>Add error handling to your config file:</p> <pre><code>error_handling:\n  circuit_breakers:\n    source_list_items:\n      failure_threshold: 5\n      reset_timeout_seconds: 60\n      half_open_max_calls: 1\n    target_create_item:\n      failure_threshold: 3\n      reset_timeout_seconds: 30\n  recovery_strategies:\n    network: [exponential_backoff]\n    timeout: [exponential_backoff]\n    server: [circuit_breaker, exponential_backoff]\n    client: [fail_fast]\n  dlq:\n    directory: .apilinker_dlq\n</code></pre>"},{"location":"error_handling/#programmatic-usage","title":"Programmatic usage","text":"<pre><code>from apilinker import ApiLinker\n\nlinker = ApiLinker(\n    error_handling_config={\n        \"circuit_breakers\": {\"source_list\": {\"failure_threshold\": 5}},\n        \"recovery_strategies\": {\"server\": [\"circuit_breaker\", \"exponential_backoff\"]},\n        \"dlq\": {\"directory\": \".apilinker_dlq\"},\n    }\n)\n\n# Later, inspect analytics\nsummary = linker.get_error_analytics()\nprint(summary)\n\n# Process DLQ items\nresults = linker.process_dlq(limit=10)\n</code></pre> <p>See <code>apilinker/core/error_handling.py</code> for supported strategies and categories.</p>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":"<p>This document addresses common questions about ApiLinker.</p>"},{"location":"faq/#general-questions","title":"General Questions","text":""},{"location":"faq/#what-is-apilinker","title":"What is ApiLinker?","text":"<p>ApiLinker is a Python library that provides a universal bridge for connecting, mapping, and automating data transfer between any two REST APIs. It allows you to configure API integrations without writing repetitive boilerplate code.</p>"},{"location":"faq/#what-python-versions-are-supported","title":"What Python versions are supported?","text":"<p>ApiLinker supports Python 3.8 and above.</p>"},{"location":"faq/#is-apilinker-free-to-use","title":"Is ApiLinker free to use?","text":"<p>Yes, ApiLinker is open-source software released under the MIT license, which allows for free use, modification, and distribution.</p>"},{"location":"faq/#can-i-use-apilinker-in-commercial-projects","title":"Can I use ApiLinker in commercial projects?","text":"<p>Yes, the MIT license allows for commercial use.</p>"},{"location":"faq/#installation-setup","title":"Installation &amp; Setup","text":""},{"location":"faq/#how-do-i-install-apilinker","title":"How do I install ApiLinker?","text":"<pre><code>pip install apilinker\n</code></pre>"},{"location":"faq/#why-am-i-getting-an-error-during-installation","title":"Why am I getting an error during installation?","text":"<p>Common installation issues include:</p> <ol> <li>Python version: Make sure you're using Python 3.8 or newer</li> <li>Permission issues: Try using <code>pip install --user apilinker</code> or use a virtual environment</li> <li>Dependency conflicts: Create a clean virtual environment and try installing again</li> </ol>"},{"location":"faq/#how-can-i-verify-the-installation","title":"How can I verify the installation?","text":"<pre><code>import apilinker\nprint(apilinker.__version__)\n</code></pre>"},{"location":"faq/#configuration","title":"Configuration","text":""},{"location":"faq/#how-do-i-connect-to-an-api-that-requires-a-custom-authentication-method","title":"How do I connect to an API that requires a custom authentication method?","text":"<p>You can create a custom authentication plugin by extending the <code>AuthPlugin</code> class:</p> <pre><code>from apilinker.core.plugins import AuthPlugin\n\nclass CustomAuth(AuthPlugin):\n    plugin_name = \"custom_auth\"\n\n    def authenticate(self, **kwargs):\n        # Custom authentication logic\n        return {\n            \"headers\": {\"X-Custom-Auth\": generate_auth_header(kwargs)},\n            \"type\": \"custom\"\n        }\n</code></pre>"},{"location":"faq/#can-i-use-apilinker-with-graphql-apis","title":"Can I use ApiLinker with GraphQL APIs?","text":"<p>Yes, you can create a custom connector plugin for GraphQL or use the REST connector with POST requests and GraphQL queries in the body.</p>"},{"location":"faq/#how-do-i-handle-api-rate-limits","title":"How do I handle API rate limits?","text":"<p>ApiLinker does not include built-in rate limiting. Use provider guidance, exponential backoff, and retries to handle 429 responses.</p> <pre><code>source:\n  type: rest\n  base_url: \"https://api.example.com\"\n  rate_limit:\n    requests_per_second: 5\n  retry:\n    max_attempts: 3\n    delay_seconds: 2\n    backoff_factor: 1.5\n    status_codes: [429, 500, 502, 503, 504]\n</code></pre>"},{"location":"faq/#data-mapping","title":"Data Mapping","text":""},{"location":"faq/#how-do-i-transform-data-between-apis","title":"How do I transform data between APIs?","text":"<p>Use field mappings with transformers:</p> <pre><code>fields:\n  - source: user.profile.name\n    target: contact.fullName\n    transform: uppercase\n</code></pre>"},{"location":"faq/#can-i-apply-multiple-transformations-to-a-single-field","title":"Can I apply multiple transformations to a single field?","text":"<p>Yes, specify them as a list:</p> <pre><code>fields:\n  - source: tags\n    target: categories\n    transform:\n      - lowercase\n      - strip\n      - none_if_empty\n</code></pre>"},{"location":"faq/#how-do-i-create-custom-data-transformers","title":"How do I create custom data transformers?","text":"<p>Register a custom transformer function:</p> <pre><code>def phone_formatter(value, **kwargs):\n    if not value:\n        return \"\"\n    digits = ''.join(c for c in value if c.isdigit())\n    if len(digits) == 10:\n        return f\"({digits[0:3]}) {digits[3:6]}-{digits[6:10]}\"\n    return value\n\nlinker.mapper.register_transformer(\"phone_formatter\", phone_formatter)\n</code></pre>"},{"location":"faq/#scheduling","title":"Scheduling","text":""},{"location":"faq/#how-do-i-schedule-a-sync-to-run-periodically","title":"How do I schedule a sync to run periodically?","text":"<pre><code># Run every hour\nlinker.add_schedule(interval_minutes=60)\n\n# Or use cron expression\nlinker.add_schedule(cron_expression=\"0 */6 * * *\")  # Every 6 hours\n\n# Start the scheduler\nlinker.start_scheduled_sync()\n</code></pre>"},{"location":"faq/#how-do-i-stop-a-scheduled-sync","title":"How do I stop a scheduled sync?","text":"<pre><code>linker.stop_scheduled_sync()\n</code></pre>"},{"location":"faq/#can-i-run-multiple-schedules-with-different-frequencies","title":"Can I run multiple schedules with different frequencies?","text":"<p>Yes, you can create multiple ApiLinker instances with different schedules.</p>"},{"location":"faq/#error-handling","title":"Error Handling","text":""},{"location":"faq/#how-do-i-handle-errors-during-sync","title":"How do I handle errors during sync?","text":"<p>You can provide an error handler function:</p> <pre><code>def handle_error(error, context):\n    print(f\"Error during sync: {error}\")\n    print(f\"Context: {context}\")\n    # Log error, send notification, etc.\n    return True  # Return True to retry, False to abort\n\nlinker.add_error_handler(handle_error)\n</code></pre>"},{"location":"faq/#how-can-i-debug-issues-with-my-api-connections","title":"How can I debug issues with my API connections?","text":"<p>Enable debug logging:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Or in configuration\nlinker = ApiLinker(debug=True)\n</code></pre>"},{"location":"faq/#performance","title":"Performance","text":""},{"location":"faq/#how-can-i-optimize-apilinker-for-large-data-transfers","title":"How can I optimize ApiLinker for large data transfers?","text":"<ol> <li>Use pagination settings appropriate for the API</li> <li>Set batch sizes for processing large datasets</li> <li>Consider using async operations for concurrent requests</li> </ol> <pre><code>source:\n  endpoints:\n    get_data:\n      pagination:\n        limit: 1000  # Request larger page sizes\n      batch_size: 500  # Process in batches\n</code></pre>"},{"location":"faq/#does-apilinker-support-caching","title":"Does ApiLinker support caching?","text":"<p>Yes, ApiLinker includes response caching capabilities:</p> <pre><code>source:\n  cache:\n    enabled: true\n    ttl: 3600  # Cache TTL in seconds\n</code></pre>"},{"location":"faq/#contributing","title":"Contributing","text":""},{"location":"faq/#how-can-i-contribute-to-apilinker","title":"How can I contribute to ApiLinker?","text":"<ol> <li>Fork the repository on GitHub</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Add tests for your changes</li> <li>Submit a pull request</li> </ol>"},{"location":"faq/#where-can-i-report-bugs-or-request-features","title":"Where can I report bugs or request features?","text":"<p>Report issues on the GitHub Issues page.</p>"},{"location":"faq/#advanced-usage","title":"Advanced Usage","text":""},{"location":"faq/#can-apilinker-handle-binary-data-or-file-transfers","title":"Can ApiLinker handle binary data or file transfers?","text":"<p>Yes, ApiLinker can handle binary data transfers. Configure the content type and use appropriate encodings:</p> <pre><code>target:\n  endpoints:\n    upload_file:\n      path: /files/upload\n      method: POST\n      headers:\n        Content-Type: application/octet-stream\n</code></pre>"},{"location":"faq/#is-it-possible-to-extend-apilinker-with-custom-plugins","title":"Is it possible to extend ApiLinker with custom plugins?","text":"<p>Yes, ApiLinker's plugin architecture allows for extending all major components:</p> <ul> <li>Create custom transformers</li> <li>Create custom connectors for different API types</li> <li>Create custom authentication methods</li> <li>Create custom validation rules</li> </ul> <p>See the Extending with Plugins documentation for details.</p>"},{"location":"getting_started/","title":"Getting Started with ApiLinker","text":"<p>This guide is designed for Python beginners who want to learn how to connect APIs using ApiLinker.</p>"},{"location":"getting_started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or newer installed</li> <li>Basic understanding of Python (variables, functions)</li> <li>A text editor or IDE (like VS Code, PyCharm, or even Notepad)</li> <li>Internet connection</li> </ul>"},{"location":"getting_started/#installation","title":"Installation","text":"<p>First, let's install ApiLinker:</p> <pre><code>pip install apilinker\n</code></pre>"},{"location":"getting_started/#basic-concepts","title":"Basic Concepts","text":"<p>Before diving in, here are the key concepts in ApiLinker:</p> <ol> <li>Source - The API where you get data from</li> <li>Target - The API where you send data to</li> <li>Endpoint - A specific operation in an API (like \"get users\" or \"create post\")</li> <li>Mapping - Rules for how data moves from source to target</li> <li>Transformer - A function that changes data format during transfer</li> </ol>"},{"location":"getting_started/#your-first-apilinker-script","title":"Your First ApiLinker Script","text":"<p>Let's build a simple script that gets data from the free JSONPlaceholder API:</p> <pre><code># Step 1: Import the library\nfrom apilinker import ApiLinker\n\n# Step 2: Create an ApiLinker instance\nlinker = ApiLinker()\n\n# Step 3: Configure a source API\nlinker.add_source(\n    type=\"rest\",\n    base_url=\"https://jsonplaceholder.typicode.com\",\n    endpoints={\n        \"get_posts\": {\n            \"path\": \"/posts\",\n            \"method\": \"GET\",\n            \"params\": {\"_limit\": 5}  # Only get 5 posts\n        }\n    }\n)\n\n# Step 4: Fetch data from the source API\nposts = linker.fetch(\"get_posts\")\n\n# Step 5: Print the results\nprint(\"Posts retrieved:\")\nfor post in posts:\n    print(f\"- {post['title']}\")\n</code></pre> <p>Save this as <code>first_example.py</code> and run it:</p> <pre><code>python first_example.py\n</code></pre> <p>You should see a list of post titles printed to your console!</p>"},{"location":"getting_started/#connecting-two-apis","title":"Connecting Two APIs","text":"<p>Now let's connect two APIs to move data from one to another:</p> <pre><code>from apilinker import ApiLinker\n\n# Create ApiLinker instance\nlinker = ApiLinker()\n\n# Configure source API (JSONPlaceholder)\nlinker.add_source(\n    type=\"rest\",\n    base_url=\"https://jsonplaceholder.typicode.com\",\n    endpoints={\n        \"get_posts\": {\n            \"path\": \"/posts\",\n            \"method\": \"GET\",\n            \"params\": {\"_limit\": 3}  # Get 3 posts\n        }\n    }\n)\n\n# Configure target API (also JSONPlaceholder in this demo)\nlinker.add_target(\n    type=\"rest\",\n    base_url=\"https://jsonplaceholder.typicode.com\",\n    endpoints={\n        \"create_comment\": {\n            \"path\": \"/comments\",\n            \"method\": \"POST\"\n        }\n    }\n)\n\n# Create a mapping between source and target\nlinker.add_mapping(\n    source=\"get_posts\",\n    target=\"create_comment\",\n    fields=[\n        # Map the post id to the comment's postId\n        {\"source\": \"id\", \"target\": \"postId\"},\n\n        # Create a fixed name\n        {\"target\": \"name\", \"value\": \"API Connector Test\"},\n\n        # Map the post title to the comment's email (just for demo purposes)\n        {\"source\": \"title\", \"target\": \"email\"},\n\n        # Map the post body to the comment's body\n        {\"source\": \"body\", \"target\": \"body\"}\n    ]\n)\n\n# Run the sync\nresult = linker.sync(dry_run=True)  # Use dry_run=True to prevent actual API calls\n\nprint(f\"Synced {result.count} posts to comments\")\nprint(\"Preview of the first transformed item:\")\nprint(result.preview[0] if result.preview else \"No preview available\")\n</code></pre> <p>Save this as <code>connecting_apis.py</code> and run it:</p> <pre><code>python connecting_apis.py\n</code></pre>"},{"location":"getting_started/#understanding-results","title":"Understanding Results","text":"<p>When you run the code above, you'll see:</p> <ol> <li>How many records were synced</li> <li>A preview of what the data looks like after transformation</li> </ol>"},{"location":"getting_started/#next-steps","title":"Next Steps","text":"<p>Now that you've created your first ApiLinker script, you can:</p> <ol> <li>Connect to real APIs - Replace the example APIs with ones you want to use</li> <li>Add authentication - Learn about auth options in the documentation</li> <li>Create transformers - Write functions to format data between systems</li> <li>Set up scheduling - Make your sync run on a regular schedule</li> </ol>"},{"location":"getting_started/#common-questions-for-beginners","title":"Common Questions for Beginners","text":""},{"location":"getting_started/#how-do-i-find-api-endpoints","title":"How do I find API endpoints?","text":"<p>Check the API documentation for the service you're using. They should list available endpoints, required parameters, and authentication methods.</p>"},{"location":"getting_started/#what-if-my-api-requires-authentication","title":"What if my API requires authentication?","text":"<p>Add it to your source or target configuration:</p> <pre><code>linker.add_source(\n    type=\"rest\",\n    base_url=\"https://api.example.com\",\n    auth={\n        \"type\": \"api_key\",\n        \"header\": \"X-API-Key\",\n        \"key\": \"your_api_key_here\"  # Better to use environment variables!\n    },\n    endpoints={\n        # Your endpoints here\n    }\n)\n</code></pre>"},{"location":"getting_started/#how-do-i-debug-issues","title":"How do I debug issues?","text":"<p>Enable debugging to see what's happening:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n\nlinker = ApiLinker(debug=True)\n# Rest of your code...\n</code></pre>"},{"location":"getting_started/#how-do-i-handle-pagination","title":"How do I handle pagination?","text":"<p>ApiLinker can handle pagination automatically:</p> <pre><code>linker.add_source(\n    # Other configuration...\n    endpoints={\n        \"get_items\": {\n            \"path\": \"/items\",\n            \"method\": \"GET\",\n            \"pagination\": {\n                \"data_path\": \"data\",\n                \"next_page_path\": \"meta.next_page\",\n                \"page_param\": \"page\"\n            }\n        }\n    }\n)\n</code></pre>"},{"location":"getting_started/#getting-help","title":"Getting Help","text":"<p>If you get stuck, here are resources to help:</p> <ul> <li>Documentation: Visit https://apilinker.readthedocs.io</li> <li>Example Code: Look at the examples in the GitHub repository</li> <li>Issues: If you find a bug, report it on GitHub Issues</li> </ul> <p>Remember, everyone starts somewhere! API integration gets easier with practice.</p>"},{"location":"installation/","title":"Installation","text":"<p>There are several ways to install ApiLinker depending on your use case.</p>"},{"location":"installation/#standard-installation","title":"Standard Installation","text":"<p>The easiest way to install ApiLinker is via pip:</p> <pre><code>pip install apilinker\n</code></pre> <p>This will install ApiLinker with its core dependencies.</p>"},{"location":"installation/#development-installation","title":"Development Installation","text":"<p>If you're planning to contribute to ApiLinker or want to install the package with development dependencies:</p> <pre><code># Clone the repository\ngit clone https://github.com/kkartas/apilinker.git\ncd apilinker\n\n# Install in development mode with dev dependencies\npip install -e \".[dev]\"\n</code></pre>"},{"location":"installation/#install-with-optional-features","title":"Install with Optional Features","text":"<p>ApiLinker comes with optional dependencies for different features:</p> <pre><code># Install with documentation tools\npip install \"apilinker[docs]\"\n\n# Install with all optional dependencies\npip install \"apilinker[dev,docs]\"\n</code></pre>"},{"location":"installation/#requirements","title":"Requirements","text":"<p>ApiLinker requires:</p> <ul> <li>Python 3.8 or higher</li> <li>Core dependencies:</li> <li>httpx: For HTTP requests</li> <li>pydantic: For data validation</li> <li>pyyaml: For YAML config support</li> <li>typer: For CLI functionality</li> <li>croniter: For cron-based scheduling</li> </ul>"},{"location":"installation/#verifying-installation","title":"Verifying Installation","text":"<p>After installation, you can verify that ApiLinker is working correctly:</p> <pre><code># Check the version\napilinker version\n\n# Create a sample configuration\napilinker init\n</code></pre>"},{"location":"installation/#system-requirements","title":"System Requirements","text":"<p>ApiLinker is designed to work on all major operating systems:</p> <ul> <li>Linux</li> <li>macOS</li> <li>Windows</li> </ul> <p>The package is lightweight and doesn't require significant system resources for most use cases. However, performance may vary depending on the volume of data being processed and the frequency of API operations.</p>"},{"location":"installation/#docker","title":"Docker","text":"<p>You can run ApiLinker via Docker without installing Python locally.</p> <pre><code>docker build -t apilinker .\ndocker run --rm apilinker --help\n</code></pre> <p>To mount a config file:</p> <pre><code>docker run --rm -v $(pwd)/config.yaml:/app/config.yaml apilinker sync --config /app/config.yaml\n</code></pre>"},{"location":"quick_reference/","title":"ApiLinker Quick Reference","text":"<p>This quick reference provides a concise overview of ApiLinker's most common operations.</p>"},{"location":"quick_reference/#installation","title":"Installation","text":"<pre><code>pip install apilinker\n</code></pre>"},{"location":"quick_reference/#basic-usage","title":"Basic Usage","text":"<pre><code>from apilinker import ApiLinker\n\n# Initialize with optional logging configuration\nlinker = ApiLinker(log_level=\"INFO\", log_file=\"apilinker.log\")\n\n# Configure source API\nlinker.add_source(\n    type=\"rest\",\n    base_url=\"https://api.source.com\",\n    auth={\n        \"type\": \"bearer\",\n        \"token\": \"${TOKEN_ENV_VAR}\"  # Environment variable reference\n    },\n    endpoints={\n        \"get_data\": {\n            \"path\": \"/data\",\n            \"method\": \"GET\",\n            \"params\": {\"limit\": 100}\n        }\n    },\n    # Connection settings\n    timeout=30,           # 30 second timeout \n    retry_count=3,        # Retry failed requests 3 times\n    retry_delay=1         # Wait 1 second between retries\n)\n\n# Configure target API\nlinker.add_target(\n    type=\"rest\",\n    base_url=\"https://api.target.com\",\n    auth={\n        \"type\": \"api_key\",\n        \"header_name\": \"X-API-Key\",  # Note: 'header_name' not 'header'\n        \"key\": \"${API_KEY_ENV_VAR}\"\n    },\n    endpoints={\n        \"post_data\": {\n            \"path\": \"/data\",\n            \"method\": \"POST\"\n        }\n    }\n)\n\n# Add field mapping\nlinker.add_mapping(\n    source=\"get_data\",\n    target=\"post_data\",\n    fields=[\n        {\"source\": \"id\", \"target\": \"external_id\"},\n        {\"source\": \"name\", \"target\": \"full_name\"},\n        {\"source\": \"email\", \"target\": \"contact.email\"}\n    ]\n)\n\n# Run the sync\nresult = linker.sync()\nprint(f\"Synced {result.count} records\")\n\n# Check for errors\nif not result.success:\n    print(f\"Errors occurred: {result.errors}\")\n</code></pre>"},{"location":"quick_reference/#authentication-methods","title":"Authentication Methods","text":""},{"location":"quick_reference/#api-key","title":"API Key","text":"<pre><code>auth={\n    \"type\": \"api_key\",\n    \"key\": \"your-api-key\",  # Better: \"${API_KEY_ENV_VAR}\"\n    \"header_name\": \"X-API-Key\",  # Default header name\n    \"in_header\": True,  # Send in header (default)\n    \"in_query\": False,  # Or set to True to send as query param\n    \"query_param\": \"api_key\"  # Query parameter name if in_query=True\n}\n</code></pre>"},{"location":"quick_reference/#bearer-token","title":"Bearer Token","text":"<pre><code>auth={\n    \"type\": \"bearer\",\n    \"token\": \"your-token\"  # Better: \"${TOKEN_ENV_VAR}\"\n}\n</code></pre>"},{"location":"quick_reference/#basic-auth","title":"Basic Auth","text":"<pre><code>auth={\n    \"type\": \"basic\",\n    \"username\": \"your-username\",  # Better: \"${USERNAME_ENV_VAR}\"\n    \"password\": \"your-password\"   # Better: \"${PASSWORD_ENV_VAR}\"\n}\n</code></pre>"},{"location":"quick_reference/#oauth2-client-credentials","title":"OAuth2 Client Credentials","text":"<pre><code>auth={\n    \"type\": \"oauth2_client_credentials\",  # Note the full type name\n    \"client_id\": \"${CLIENT_ID}\",\n    \"client_secret\": \"${CLIENT_SECRET}\",\n    \"token_url\": \"https://auth.example.com/token\",\n    \"scope\": \"read write\"  # Optional\n}\n</code></pre>"},{"location":"quick_reference/#common-transformers","title":"Common Transformers","text":"<pre><code># Register a custom transformer\ndef format_phone(value, **kwargs):\n    if not value:\n        return \"\"\n    digits = ''.join(c for c in value if c.isdigit())\n    if len(digits) == 10:\n        return f\"({digits[0:3]}) {digits[3:6]}-{digits[6:10]}\"\n    return value\n\nlinker.mapper.register_transformer(\"format_phone\", format_phone)\n\n# Use in mapping\nlinker.add_mapping(\n    source=\"get_users\",\n    target=\"create_contacts\",\n    fields=[\n        # Built-in transformers\n        {\"source\": \"name\", \"target\": \"name\", \"transform\": \"lowercase\"},\n        {\"source\": \"created_at\", \"target\": \"created\", \"transform\": \"iso_to_timestamp\"},\n        # Custom transformer\n        {\"source\": \"phone\", \"target\": \"phoneNumber\", \"transform\": \"format_phone\"}\n    ]\n)\n</code></pre>"},{"location":"quick_reference/#pagination","title":"Pagination","text":"<pre><code>linker.add_source(\n    type=\"rest\",\n    base_url=\"https://api.example.com\",\n    endpoints={\n        \"get_users\": {\n            \"path\": \"/users\",\n            \"method\": \"GET\",\n            \"pagination\": {\n                \"data_path\": \"data\",\n                \"next_page_path\": \"meta.next_page\",\n                \"page_param\": \"page\"\n            }\n        }\n    }\n)\n</code></pre>"},{"location":"quick_reference/#sse-streaming","title":"SSE Streaming","text":"<pre><code>from apilinker import SSEConnector\n\nconnector = SSEConnector(\n    base_url=\"https://events.example.com\",\n    endpoints={\n        \"feed\": {\n            \"path\": \"/stream\",\n            \"method\": \"GET\",\n            \"sse\": {\n                \"reconnect\": True,\n                \"reconnect_delay\": 1.0,\n                \"max_reconnect_attempts\": 10,\n                \"chunk_size\": 50,\n                \"backpressure_buffer_size\": 500,\n                \"drop_policy\": \"block\",  # or \"drop_oldest\"\n            },\n        }\n    },\n)\n\n# Event-by-event stream consumption\nfor event in connector.stream_events(endpoint_name=\"feed\", max_events=100):\n    print(event[\"event\"], event[\"data\"])\n\n# Chunked processing with explicit backpressure strategy\nsummary = connector.consume_events(\n    endpoint_name=\"feed\",\n    chunk_size=25,\n    processor=lambda chunk: [item[\"data\"] for item in chunk],\n)\nprint(summary[\"processed_events\"], summary[\"dropped_events\"])\n</code></pre>"},{"location":"quick_reference/#scheduling","title":"Scheduling","text":"<pre><code># Configure schedule by type\n\n# Option 1: Interval-based schedule (runs every X time units)\nlinker.add_schedule(\n    type=\"interval\",\n    minutes=60  # Run every 60 minutes\n    # Can also use: seconds=30, hours=2, days=1\n)\n\n# Option 2: Cron-based schedule (runs according to cron expression)\nlinker.add_schedule(\n    type=\"cron\",\n    expression=\"0 2 * * *\"  # Run at 2 AM daily\n)\n\n# Option 3: One-time schedule (runs once at a specific time)\nfrom datetime import datetime, timedelta\n\nlinker.add_schedule(\n    type=\"once\",\n    datetime=datetime.now() + timedelta(hours=1)  # Run in 1 hour\n)\n\n# Start the scheduler in a background thread\nlinker.start_scheduled_sync()\n\n# To stop the scheduler\n# linker.stop_scheduled_sync()\n</code></pre>"},{"location":"quick_reference/#error-handling","title":"Error Handling","text":"<pre><code># ApiLinker's sync method returns a SyncResult object with error information\nresult = linker.sync()\nif not result.success:\n    print(f\"Sync failed with {len(result.errors)} errors:\")\n    for error in result.errors:\n        print(f\" - {error}\")\n\n# Implementing custom error handling with try/except\ntry:\n    result = linker.sync()\n    print(f\"Synced {result.count} records\")\nexcept Exception as e:\n    print(f\"Error during sync: {e}\")\n    # Log the error, notify admins, etc.\n\n# The ApiConnector has built-in retry logic for transient failures\n# Configure when initializing the source/target:\nlinker.add_source(\n    # other parameters...\n    retry_count=3,    # Number of retries\n    retry_delay=1     # Seconds between retries\n)\n</code></pre>"},{"location":"quick_reference/#provenance-audit","title":"Provenance &amp; Audit","text":"<pre><code>provenance:\n  output_dir: runs/   # Sidecar JSON per run written here\n  jsonl_log: logs/apilinker_runs.jsonl  # Append-only JSON lines log\n\nidempotency:\n  enabled: true\n  salt: \"my-integration-v1\"  # Optional salt for key stability across versions\n</code></pre> <ul> <li>Each sync records config hash, git SHA (if available), timestamps, endpoints, and error/rate-limit events into the JSONL log and/or sidecar JSON.</li> <li>When idempotency is enabled, repeated runs skip items already sent (per-endpoint) based on a stable key derived from the payload.</li> </ul>"},{"location":"quick_reference/#config-file-yaml","title":"Config File (YAML)","text":"<pre><code>source:\n  type: rest\n  base_url: https://api.example.com/v1\n  auth:\n    type: bearer\n    token: ${TOKEN_ENV_VAR}\n  endpoints:\n    get_items:\n      path: /items\n      method: GET\n      params:\n        limit: 100\n      response_schema:\n        type: object\n        properties:\n          items:\n            type: array\n            items:\n              type: object\n              properties:\n                id: { type: string }\n                name: { type: string }\n\ntarget:\n  type: rest\n  base_url: https://api.target.com/v2\n  auth:\n    type: api_key\n    header: X-API-Key\n    key: ${API_KEY_ENV_VAR}\n  endpoints:\n    create_item:\n      path: /items\n      method: POST\n      request_schema:\n        type: object\n        properties:\n          external_id: { type: string }\n          title: { type: string }\n        required: [external_id, title]\n\nmapping:\n  - source: get_items\n    target: create_item\n    fields:\n      - source: id\n        target: external_id\n      - source: name\n        target: title\n      - source: created_at\n        target: meta.created\n        transform: iso_to_timestamp\n\nschedule:\n  type: interval\n  minutes: 60\nvalidation:\n  strict_mode: true\n</code></pre> <p>Load from config file:</p> <pre><code>linker = ApiLinker(config_path=\"config.yaml\")\n</code></pre>"},{"location":"quick_reference/#state-management","title":"State Management","text":"<pre><code>state:\n  type: sqlite            # or 'file'\n  path: .apilinker/state.db\n  default_last_sync: \"2024-01-01T00:00:00Z\"\n</code></pre> <ul> <li>Show current state: <pre><code>apilinker state --config config.yaml --action show\n</code></pre></li> <li>Reset state: <pre><code>apilinker state --config config.yaml --action reset\n</code></pre></li> </ul>"},{"location":"quickstart/","title":"Quick Start Guide","text":"<p>This guide will help you quickly set up and run your first API integration with ApiLinker.</p>"},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or higher installed</li> <li>ApiLinker installed (<code>pip install apilinker</code>)</li> <li>Access to source and target APIs with appropriate credentials</li> </ul>"},{"location":"quickstart/#step-1-create-a-configuration-file","title":"Step 1: Create a Configuration File","text":"<p>First, generate a template configuration file:</p> <pre><code>apilinker init --output my_config.yaml\n</code></pre> <p>This creates a starter YAML configuration file that you can customize:</p> <pre><code># my_config.yaml\nsource:\n  type: rest\n  base_url: https://api.example.com/v1\n  auth:\n    type: bearer\n    token: ${SOURCE_API_TOKEN}  # Will be read from environment variable\n  endpoints:\n    list_items:\n      path: /items\n      method: GET\n      params:\n        updated_since: \"{{last_sync}}\"\n\ntarget:\n  type: rest\n  base_url: https://api.destination.com/v2\n  auth:\n    type: api_key\n    header: X-API-Key\n    key: ${TARGET_API_KEY}  # Will be read from environment variable\n  endpoints:\n    create_item:\n      path: /items\n      method: POST\n\nmapping:\n  - source: list_items\n    target: create_item\n    fields:\n      - source: id\n        target: external_id\n      - source: name\n        target: title\n      - source: description\n        target: body.content\n      - source: created_at\n        target: metadata.created\n        transform: iso_to_timestamp\n</code></pre>"},{"location":"quickstart/#step-2-set-up-environment-variables","title":"Step 2: Set Up Environment Variables","text":"<p>Set your API credentials as environment variables:</p> <pre><code># Linux/macOS\nexport SOURCE_API_TOKEN=your_source_api_token\nexport TARGET_API_KEY=your_target_api_key\n\n# Windows (Command Prompt)\nset SOURCE_API_TOKEN=your_source_api_token\nset TARGET_API_KEY=your_target_api_key\n\n# Windows (PowerShell)\n$env:SOURCE_API_TOKEN=\"your_source_api_token\"\n$env:TARGET_API_KEY=\"your_target_api_key\"\n</code></pre>"},{"location":"quickstart/#step-3-validate-your-configuration","title":"Step 3: Validate Your Configuration","text":"<p>Check that your configuration is valid:</p> <pre><code>apilinker validate --config my_config.yaml\n</code></pre>"},{"location":"quickstart/#step-4-run-a-test-sync","title":"Step 4: Run a Test Sync","text":"<p>Perform a dry run to see what would happen without making changes:</p> <pre><code>apilinker sync --config my_config.yaml --dry-run\n</code></pre> <p>This shows details about the source and target APIs and the field mappings that will be used.</p>"},{"location":"quickstart/#step-5-perform-the-sync","title":"Step 5: Perform the Sync","text":"<p>Now, run the actual sync operation:</p> <pre><code>apilinker sync --config my_config.yaml\n</code></pre> <p>You'll see output showing the number of items synced and any errors encountered.</p>"},{"location":"quickstart/#step-6-schedule-recurring-syncs-optional","title":"Step 6: Schedule Recurring Syncs (Optional)","text":"<p>To run the sync on a schedule:</p> <pre><code>apilinker run --config my_config.yaml\n</code></pre> <p>This starts a process that runs the sync based on the schedule configuration (which defaults to hourly if not specified).</p>"},{"location":"quickstart/#using-apilinker-as-a-python-library","title":"Using ApiLinker as a Python Library","text":"<p>You can also use ApiLinker directly in your Python code:</p> <pre><code>from apilinker import ApiLinker\n\n# Initialize with config file\nlinker = ApiLinker(config_path=\"my_config.yaml\")\n\n# Run the sync\nresult = linker.sync()\nprint(f\"Synced {result.count} items\")\n\n# Check for errors\nif not result.success:\n    for error in result.errors:\n        print(f\"Error: {error}\")\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn how to create custom data transformations</li> <li>Configure advanced authentication methods </li> <li>Set up complex field mappings with conditions and filtering</li> <li>Explore pagination handling for large datasets</li> </ul> <p>For complete details on all configuration options and features, see the Configuration Guide.</p>"},{"location":"research_index/","title":"Research Documentation Index","text":"<p>Welcome to ApiLinker's research-focused documentation. This section is specifically designed for researchers, scientists, and academics who want to leverage API integration for their research workflows.</p>"},{"location":"research_index/#quick-start-for-researchers","title":"\ud83e\uddec Quick Start for Researchers","text":"<p>New to ApiLinker? Start here:</p> <pre><code># Install ApiLinker (includes all 8 research connectors)\npip install apilinker\n\n# Basic literature search across databases\nfrom apilinker import NCBIConnector, ArXivConnector\n\nncbi = NCBIConnector(email=\"researcher@university.edu\")\narxiv = ArXivConnector()\n\n# Search biomedical literature\npapers = ncbi.search_pubmed(\"CRISPR gene editing\", max_results=50)\nprint(f\"PubMed papers: {len(papers.get('esearchresult', {}).get('idlist', []))}\")\n\n# Search computer science preprints\nai_papers = arxiv.search_papers(\"machine learning\", max_results=100) \nprint(f\"arXiv papers: {len(ai_papers)}\")\n</code></pre> <p>Next Steps: 1. Research Workflows Guide - Comprehensive domain-specific workflows 2. Comprehensive Examples - All 8 connectors in action 3. Installation Guide - Setup with API keys</p>"},{"location":"research_index/#comprehensive-guides","title":"\ud83d\udcda Comprehensive Guides","text":""},{"location":"research_index/#core-research-documentation","title":"Core Research Documentation","text":"<ul> <li>Research Workflows - Complete guide with automated monitoring, ethical usage, and reproducible research</li> <li>Installation Guide - Setup all 8 research connectors with API keys</li> <li>Configuration Guide - Research connector configuration examples</li> </ul>"},{"location":"research_index/#domain-specific-guides","title":"Domain-Specific Guides","text":"<ul> <li>Bioinformatics &amp; Genomics - NCBI, GenBank, PubMed integration</li> <li>Computer Science &amp; AI - arXiv analysis, trend tracking, collaboration networks</li> <li>Climate Science - Environmental data integration</li> <li>Social Sciences - Public health research, policy analysis</li> <li>Physics &amp; Materials - Materials discovery, computational physics</li> <li>Psychology &amp; Neuroscience - Multi-modal research synthesis</li> </ul>"},{"location":"research_index/#technical-resources","title":"\ud83d\udee0\ufe0f Technical Resources","text":""},{"location":"research_index/#api-connectors","title":"API Connectors","text":""},{"location":"research_index/#scientific-literature-citation-data","title":"\ud83d\udd2c Scientific Literature &amp; Citation Data","text":"<ul> <li>NCBI Connector - PubMed, GenBank, ClinVar</li> <li>arXiv Connector - Academic preprint repository</li> <li>CrossRef Connector - Citation data and DOI resolution</li> <li>Semantic Scholar Connector - AI-powered academic search</li> </ul>"},{"location":"research_index/#chemical-biological-data","title":"\ud83e\uddea Chemical &amp; Biological Data","text":"<ul> <li>PubChem Connector - Chemical compounds and bioassays</li> <li>ORCID Connector - Researcher profiles and credentials</li> </ul>"},{"location":"research_index/#code-implementation-research","title":"\ud83d\udcbb Code &amp; Implementation Research","text":"<ul> <li>GitHub Connector - Code repositories and collaboration analysis</li> <li>NASA Connector - Earth science and climate data</li> </ul>"},{"location":"research_index/#custom-development","title":"\ud83d\udd27 Custom Development","text":"<ul> <li>Creating Custom Connectors - Build your own research connectors</li> </ul>"},{"location":"research_index/#research-patterns","title":"Research Patterns","text":"<ul> <li>Cross-Database Literature Search - Combine PubMed and arXiv</li> <li>Longitudinal Research Monitoring - Track research trends over time</li> <li>Collaboration Network Analysis - Map research partnerships</li> <li>Technology Transfer Tracking - Monitor how innovations spread</li> </ul>"},{"location":"research_index/#use-case-examples","title":"\ud83d\udcca Use Case Examples","text":""},{"location":"research_index/#quick-research-tasks-5-10-minutes","title":"Quick Research Tasks (5-10 minutes)","text":"<pre><code># Multi-connector research across 8 databases\nfrom apilinker import (\n    NCBIConnector, ArXivConnector, SemanticScholarConnector,\n    PubChemConnector, GitHubConnector, ORCIDConnector\n)\n\n# Initialize connectors\nncbi = NCBIConnector(email=\"researcher@university.edu\")\narxiv = ArXivConnector()\nsemantic = SemanticScholarConnector()\npubchem = PubChemConnector()\ngithub = GitHubConnector()\n\ntopic = \"machine learning drug discovery\"\n\n# Multi-platform search\npubmed_papers = ncbi.search_pubmed(topic, max_results=20)\narxiv_papers = arxiv.search_papers(topic, max_results=20)\nai_papers = semantic.search_papers(topic, max_results=20)\ncompounds = pubchem.search_compounds(\"machine learning\")\ncode_repos = github.search_repositories(topic, max_results=10)\n\ntotal_resources = (\n    len(pubmed_papers.get('esearchresult', {}).get('idlist', [])) +\n    len(arxiv_papers) + \n    len(ai_papers.get('data', [])) +\n    len(code_repos.get('items', []))\n)\n\nprint(f\"Found {total_resources} resources across multiple platforms\")\n</code></pre>"},{"location":"research_index/#comprehensive-research-workflows-30-minutes","title":"Comprehensive Research Workflows (30+ minutes)","text":"<ul> <li>Systematic Literature Reviews - Automated, reproducible reviews</li> <li>Research Gap Analysis - Identify under-explored intersections</li> <li>Grant Research - Monitor funding opportunities and trends</li> <li>Collaboration Discovery - Find potential research partners</li> </ul>"},{"location":"research_index/#research-domains","title":"\ud83c\udfaf Research Domains","text":""},{"location":"research_index/#life-sciences","title":"Life Sciences","text":"<ul> <li>Gene Function Research - Combine sequence and literature data</li> <li>Drug Discovery - Track computational approaches in pharma</li> <li>Protein Structure - Integrate experimental and computational studies</li> <li>Disease Research - Cross-reference genetics and clinical studies</li> </ul>"},{"location":"research_index/#physical-sciences","title":"Physical Sciences","text":"<ul> <li>Materials Discovery - Computational predictions + experimental validation</li> <li>Climate Science - Environmental monitoring and policy research</li> <li>Physics Research - Theoretical papers + experimental validation</li> </ul>"},{"location":"research_index/#social-sciences","title":"Social Sciences","text":"<ul> <li>Public Health - Epidemiology + social determinants</li> <li>Education Research - Technology adoption and learning outcomes</li> <li>Policy Analysis - Research impact on policy decisions</li> </ul>"},{"location":"research_index/#computer-science","title":"Computer Science","text":"<ul> <li>AI Research Trends - Track emerging techniques and applications</li> <li>Technology Transfer - How academic research enters industry</li> <li>Open Source Impact - Research influence on software development</li> </ul>"},{"location":"research_index/#research-best-practices","title":"\ud83d\udd2c Research Best Practices","text":""},{"location":"research_index/#ethical-research","title":"Ethical Research","text":"<pre><code>from apilinker import NCBIConnector\nimport time\n\n# Always use institutional email and respectful API usage (back off on 429)\nclass EthicalResearcher:\n    def __init__(self, email, rate_limit=1.0):\n        self.ncbi = NCBIConnector(email=email)\n        self.rate_limit = rate_limit\n\n    def respectful_search(self, query, max_results=50):\n        result = self.ncbi.search_pubmed(query, max_results=max_results)\n        time.sleep(self.backoff_seconds)  # Respectful backoff\n        return result\n</code></pre>"},{"location":"research_index/#reproducible-research","title":"Reproducible Research","text":"<ul> <li>Configuration Management - Version your research workflows</li> <li>Data Provenance - Track data sources and transformations  </li> <li>Collaborative Workflows - Share configurations with team</li> <li>Results Archiving - Store and version research outputs</li> </ul>"},{"location":"research_index/#research-quality","title":"Research Quality","text":"<ul> <li>Cross-Validation - Verify findings across multiple databases</li> <li>Longitudinal Analysis - Track changes over time</li> <li>Statistical Rigor - Proper sampling and analysis methods</li> <li>Peer Review Integration - Collaborative research workflows</li> </ul>"},{"location":"research_index/#advanced-features","title":"\ud83d\ude80 Advanced Features","text":""},{"location":"research_index/#automation-scheduling","title":"Automation &amp; Scheduling","text":"<ul> <li>Daily Research Updates - Monitor your field automatically</li> <li>Grant Deadline Tracking - Never miss funding opportunities  </li> <li>Citation Alerts - Track when your work is cited</li> <li>Conference Monitoring - Stay updated on relevant conferences</li> </ul>"},{"location":"research_index/#integration-capabilities","title":"Integration Capabilities","text":"<ul> <li>Reference Managers - Export to Zotero, Mendeley, EndNote</li> <li>Statistical Software - Integration with R, Python pandas</li> <li>Visualization Tools - Connect to matplotlib, plotly, D3.js</li> <li>Collaboration Platforms - Slack, Teams, Discord notifications</li> </ul>"},{"location":"research_index/#custom-research-workflows","title":"Custom Research Workflows","text":"<ul> <li>Plugin Development - Create domain-specific connectors</li> <li>Custom Transformations - Specialized data processing</li> <li>Authentication Systems - Institutional access integration</li> <li>Data Export Formats - CSV, JSON, XML, RDF, BibTeX</li> </ul>"},{"location":"research_index/#success-stories","title":"\ud83c\udfc6 Success Stories","text":""},{"location":"research_index/#example-use-cases","title":"Example Use Cases","text":"<ul> <li>Cross-Domain Literature Analysis - Combining multiple research databases</li> <li>Research Trend Prediction - Analyzing patterns across literature sources</li> <li>Collaboration Network Study - Mapping researcher connections</li> </ul>"},{"location":"research_index/#potential-applications","title":"Potential Applications","text":"<ul> <li>Academic Institutions - Automated research workflows</li> <li>Research Organizations - Large-scale literature analysis  </li> <li>Medical Research - Clinical research integration</li> <li>Scientific Computing - Reproducible research pipelines</li> <li>Environmental Research - Climate data integration</li> <li>Pharmaceutical Research - Drug discovery data mining</li> </ul>"},{"location":"research_index/#research-community","title":"\ud83d\udcde Research Community","text":""},{"location":"research_index/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Discussions - Research-specific questions</li> <li>Research Slack Channel - Real-time collaboration</li> <li>Office Hours - Weekly research support</li> </ul>"},{"location":"research_index/#contributing-to-research-features","title":"Contributing to Research Features","text":"<ul> <li>Research Roadmap - Planned research features</li> <li>Connector Requests - Request new scientific APIs</li> <li>Example Contributions - Share your research workflows</li> </ul>"},{"location":"research_index/#research-collaboration","title":"Research Collaboration","text":"<ul> <li>Research Working Groups - Domain-specific collaboration</li> <li>Conference Presentations - Presenting ApiLinker at academic conferences</li> <li>Workshop Materials - Teaching materials for research methods courses</li> </ul>"},{"location":"research_index/#academic-citation","title":"\ud83c\udf93 Academic Citation","text":"<p>If you use ApiLinker in your research, please cite:</p> <pre><code>@software{apilinker2024,\n  title={ApiLinker: A Universal Bridge for Scientific API Integration},\n  author={Your Name},\n  year={2024},\n  url={https://github.com/kkartas/apilinker},\n  version={0.4.0}\n}\n</code></pre> <p>This documentation is continuously updated based on researcher feedback and new scientific use cases. Contribute your research examples to help other researchers!</p>"},{"location":"research_workflows/","title":"Research Workflows with ApiLinker","text":"<p>This guide demonstrates how researchers can use ApiLinker to create automated workflows for data collection, literature reviews, and interdisciplinary research across scientific domains.</p>"},{"location":"research_workflows/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Quick Start for Researchers</li> <li>Bioinformatics Workflows</li> <li>Computer Science &amp; AI Research</li> <li>Interdisciplinary Research</li> <li>Literature Review Automation</li> <li>Data Collection Pipelines</li> <li>Best Practices for Research</li> </ul>"},{"location":"research_workflows/#quick-start-for-researchers","title":"Quick Start for Researchers","text":""},{"location":"research_workflows/#installation-for-research-use","title":"Installation for Research Use","text":"<pre><code># Install ApiLinker with all research features\npip install apilinker\n\n# Verify scientific connectors are available\npython -c \"from apilinker import NCBIConnector, ArXivConnector; print('\u2705 Ready for research!')\"\n</code></pre>"},{"location":"research_workflows/#your-first-research-workflow","title":"Your First Research Workflow","text":"<pre><code>from apilinker import NCBIConnector, ArXivConnector\n\n# Set up research APIs (replace with your institutional email)\nncbi = NCBIConnector(email=\"researcher@university.edu\")\narxiv = ArXivConnector()\n\n# Search for papers on a research topic\ntopic = \"machine learning protein folding\"\n\n# Get biomedical perspective\npubmed_papers = ncbi.search_pubmed(topic, max_results=20)\nprint(f\"Found {len(pubmed_papers.get('esearchresult', {}).get('idlist', []))} PubMed papers\")\n\n# Get computational perspective  \ncs_papers = arxiv.search_papers(topic, max_results=20)\nprint(f\"Found {len(cs_papers)} arXiv papers\")\n\n# This simple workflow gives you interdisciplinary insights!\n</code></pre>"},{"location":"research_workflows/#bioinformatics-workflows","title":"Bioinformatics Workflows","text":""},{"location":"research_workflows/#1-gene-function-literature-review","title":"1. Gene Function Literature Review","text":"<p>Automatically collect and analyze literature about specific genes:</p> <pre><code>from apilinker import NCBIConnector, ApiLinker\nimport pandas as pd\n\ndef gene_literature_pipeline(gene_name, output_file=\"gene_review.csv\"):\n    \"\"\"\n    Comprehensive literature review for a specific gene.\n    \"\"\"\n    ncbi = NCBIConnector(email=\"bioinformatician@university.edu\")\n\n    # Search for papers about the gene\n    search_results = ncbi.search_pubmed(\n        query=f\"{gene_name} AND (function OR pathway OR disease)\",\n        max_results=100,\n        sort=\"date\"\n    )\n\n    pubmed_ids = search_results.get('esearchresult', {}).get('idlist', [])\n\n    if not pubmed_ids:\n        print(f\"No papers found for {gene_name}\")\n        return\n\n    # Get detailed article information\n    summaries = ncbi.get_article_summaries(pubmed_ids)\n\n    # Extract key information\n    papers_data = []\n    for paper_id in pubmed_ids:\n        paper_info = summaries.get('result', {}).get(paper_id, {})\n\n        papers_data.append({\n            'pubmed_id': paper_id,\n            'title': paper_info.get('title', ''),\n            'authors': '; '.join([author.get('name', '') for author in paper_info.get('authors', [])]),\n            'journal': paper_info.get('source', ''),\n            'pub_date': paper_info.get('pubdate', ''),\n            'gene': gene_name\n        })\n\n    # Save to CSV for further analysis\n    df = pd.DataFrame(papers_data)\n    df.to_csv(output_file, index=False)\n\n    print(f\"\ud83d\udcca Saved {len(papers_data)} papers to {output_file}\")\n    print(f\"\ud83d\udcc5 Date range: {df['pub_date'].min()} - {df['pub_date'].max()}\")\n\n    return df\n\n# Example usage\ngene_data = gene_literature_pipeline(\"BRCA1\")\n</code></pre>"},{"location":"research_workflows/#2-sequence-data-collection","title":"2. Sequence Data Collection","text":"<p>Collect genetic sequences and associated metadata:</p> <pre><code>from apilinker import NCBIConnector\n\ndef collect_gene_sequences(gene_name, organism=\"Homo sapiens\", max_sequences=50):\n    \"\"\"\n    Collect genetic sequences and metadata for analysis.\n    \"\"\"\n    ncbi = NCBIConnector(email=\"genomics@university.edu\")\n\n    # Search for sequences\n    search_query = f\"{gene_name} AND {organism}[Organism]\"\n    search_results = ncbi.search_genbank(search_query, max_results=max_sequences)\n\n    sequence_ids = search_results.get('esearchresult', {}).get('idlist', [])\n\n    if sequence_ids:\n        # Get sequences in FASTA format\n        sequences = ncbi.get_sequences(sequence_ids[:10], format=\"fasta\")\n\n        # Save sequences to file\n        with open(f\"{gene_name}_sequences.fasta\", \"w\") as f:\n            f.write(sequences)\n\n        print(f\"\ud83d\udcca Collected {len(sequence_ids)} sequences for {gene_name}\")\n        print(f\"\ud83d\udcbe Saved first 10 sequences to {gene_name}_sequences.fasta\")\n\n    return sequence_ids\n\n# Example: Collect BRCA1 sequences\nbrca1_sequences = collect_gene_sequences(\"BRCA1\")\n</code></pre>"},{"location":"research_workflows/#3-disease-gene-association-research","title":"3. Disease-Gene Association Research","text":"<p>Cross-reference genetic variants with disease literature:</p> <pre><code>from apilinker import ApiLinker, NCBIConnector\n\ndef disease_gene_research(disease_name, associated_genes, max_papers_per_gene=20):\n    \"\"\"\n    Research disease-gene associations across literature.\n    \"\"\"\n    ncbi = NCBIConnector(email=\"medical_researcher@university.edu\")\n    linker = ApiLinker()\n\n    results = {}\n\n    for gene in associated_genes:\n        # Search for papers linking gene to disease\n        query = f\"{gene} AND {disease_name} AND (mutation OR variant OR polymorphism)\"\n\n        papers = ncbi.search_pubmed(query, max_results=max_papers_per_gene)\n        paper_count = len(papers.get('esearchresult', {}).get('idlist', []))\n\n        results[gene] = {\n            'paper_count': paper_count,\n            'search_query': query,\n            'pubmed_ids': papers.get('esearchresult', {}).get('idlist', [])\n        }\n\n        print(f\"\ud83e\uddec {gene}: {paper_count} papers linking to {disease_name}\")\n\n    # Identify most studied gene-disease associations\n    sorted_genes = sorted(results.items(), key=lambda x: x[1]['paper_count'], reverse=True)\n\n    print(f\"\\n\ud83d\udcca Top gene associations with {disease_name}:\")\n    for gene, data in sorted_genes[:5]:\n        print(f\"   {gene}: {data['paper_count']} papers\")\n\n    return results\n\n# Example: Study genes associated with Alzheimer's disease\nalzheimer_genes = [\"APOE\", \"APP\", \"PSEN1\", \"PSEN2\", \"TREM2\"]\nalzheimer_research = disease_gene_research(\"Alzheimer disease\", alzheimer_genes)\n</code></pre>"},{"location":"research_workflows/#computer-science-ai-research","title":"Computer Science &amp; AI Research","text":""},{"location":"research_workflows/#1-track-emerging-ai-techniques","title":"1. Track Emerging AI Techniques","text":"<p>Monitor arXiv for new developments in specific AI areas:</p> <pre><code>from apilinker import ArXivConnector\nfrom datetime import datetime, timedelta\n\ndef track_ai_developments(research_areas, days_back=30, max_papers_per_area=25):\n    \"\"\"\n    Track recent developments in AI research areas.\n    \"\"\"\n    arxiv = ArXivConnector()\n\n    results = {}\n\n    for area in research_areas:\n        # Search for recent papers\n        papers = arxiv.search_papers(\n            query=area,\n            max_results=max_papers_per_area,\n            sort_by=\"submittedDate\",\n            sort_order=\"descending\"\n        )\n\n        # Filter by date\n        recent_papers = []\n        cutoff_date = datetime.now() - timedelta(days=days_back)\n\n        for paper in papers:\n            if paper.get('published_date'):\n                pub_date = datetime.fromisoformat(paper['published_date'].replace('Z', '+00:00'))\n                if pub_date &gt;= cutoff_date:\n                    recent_papers.append(paper)\n\n        results[area] = recent_papers\n        print(f\"\ud83e\udd16 {area}: {len(recent_papers)} recent papers\")\n\n    # Analyze trends\n    print(f\"\\n\ud83d\udcc8 Research Activity (last {days_back} days):\")\n    for area, papers in sorted(results.items(), key=lambda x: len(x[1]), reverse=True):\n        if papers:\n            print(f\"   {area}: {len(papers)} papers\")\n            # Show most recent paper\n            latest = papers[0]\n            print(f\"      Latest: '{latest['title'][:60]}...'\")\n\n    return results\n\n# Example: Track hot AI research areas\nai_areas = [\n    \"transformer architecture\",\n    \"diffusion models\", \n    \"large language models\",\n    \"reinforcement learning\",\n    \"computer vision\",\n    \"graph neural networks\"\n]\n\nai_trends = track_ai_developments(ai_areas, days_back=14)\n</code></pre>"},{"location":"research_workflows/#2-researcher-collaboration-analysis","title":"2. Researcher Collaboration Analysis","text":"<p>Build collaboration networks from arXiv data:</p> <pre><code>from apilinker import ArXivConnector\nfrom collections import defaultdict, Counter\n\ndef analyze_research_collaborations(research_field, max_papers=100):\n    \"\"\"\n    Analyze collaboration patterns in a research field.\n    \"\"\"\n    arxiv = ArXivConnector()\n\n    # Get papers in the field\n    papers = arxiv.search_papers(research_field, max_results=max_papers)\n\n    # Build collaboration network\n    collaborations = defaultdict(list)\n    author_papers = defaultdict(int)\n    author_collaborators = defaultdict(set)\n\n    for paper in papers:\n        authors = paper.get('authors', [])\n\n        # Count papers per author\n        for author in authors:\n            author_papers[author] += 1\n\n        # Track collaborations\n        for i, author1 in enumerate(authors):\n            for author2 in authors[i+1:]:\n                collaborations[tuple(sorted([author1, author2]))].append(paper['title'])\n                author_collaborators[author1].add(author2)\n                author_collaborators[author2].add(author1)\n\n    # Find most prolific authors\n    top_authors = Counter(author_papers).most_common(10)\n\n    # Find strongest collaborations\n    top_collaborations = sorted(\n        [(pair, len(papers)) for pair, papers in collaborations.items()],\n        key=lambda x: x[1],\n        reverse=True\n    )[:10]\n\n    print(f\"\ud83d\udcca Collaboration Analysis for '{research_field}'\")\n    print(f\"\ud83d\udcc4 Analyzed {len(papers)} papers\")\n    print(f\"\ud83d\udc65 Found {len(author_papers)} unique authors\")\n\n    print(f\"\\n\ud83c\udfc6 Most Prolific Authors:\")\n    for author, count in top_authors:\n        print(f\"   {author}: {count} papers\")\n\n    print(f\"\\n\ud83e\udd1d Strongest Collaborations:\")\n    for (author1, author2), count in top_collaborations:\n        print(f\"   {author1} \u2194 {author2}: {count} joint papers\")\n\n    return {\n        'authors': dict(author_papers),\n        'collaborations': dict(collaborations),\n        'network': dict(author_collaborators)\n    }\n\n# Example: Analyze deep learning collaborations\ndl_network = analyze_research_collaborations(\"deep learning\", max_papers=200)\n</code></pre>"},{"location":"research_workflows/#interdisciplinary-research","title":"Interdisciplinary Research","text":""},{"location":"research_workflows/#1-cross-domain-knowledge-discovery","title":"1. Cross-Domain Knowledge Discovery","text":"<p>Find connections between different research domains:</p> <pre><code>from apilinker import NCBIConnector, ArXivConnector\n\ndef cross_domain_discovery(domain1_terms, domain2_terms, domain1_db=\"pubmed\", domain2_db=\"arxiv\"):\n    \"\"\"\n    Discover connections between research domains.\n    \"\"\"\n    ncbi = NCBIConnector(email=\"interdisciplinary@university.edu\")\n    arxiv = ArXivConnector()\n\n    results = {\n        'domain1_papers': {},\n        'domain2_papers': {},\n        'cross_domain_terms': []\n    }\n\n    # Search domain 1 (typically biomedical)\n    if domain1_db == \"pubmed\":\n        for term in domain1_terms:\n            papers = ncbi.search_pubmed(term, max_results=30)\n            paper_count = len(papers.get('esearchresult', {}).get('idlist', []))\n            results['domain1_papers'][term] = paper_count\n            print(f\"\ud83e\uddec PubMed - {term}: {paper_count} papers\")\n\n    # Search domain 2 (typically computer science)\n    if domain2_db == \"arxiv\":\n        for term in domain2_terms:\n            papers = arxiv.search_papers(term, max_results=30)\n            results['domain2_papers'][term] = len(papers)\n            print(f\"\ud83d\udcbb arXiv - {term}: {len(papers)} papers\")\n\n    # Look for cross-domain connections\n    print(f\"\\n\ud83d\udd17 Searching for cross-domain connections...\")\n\n    for bio_term in domain1_terms:\n        for cs_term in domain2_terms:\n            # Search PubMed for CS term\n            cs_in_bio = ncbi.search_pubmed(f\"{cs_term} AND computational\", max_results=10)\n            cs_bio_count = len(cs_in_bio.get('esearchresult', {}).get('idlist', []))\n\n            # Search arXiv for bio term\n            bio_in_cs = arxiv.search_papers(f\"{bio_term} bioinformatics\", max_results=10)\n            bio_cs_count = len(bio_in_cs)\n\n            if cs_bio_count &gt; 0 or bio_cs_count &gt; 0:\n                connection = {\n                    'bio_term': bio_term,\n                    'cs_term': cs_term,\n                    'cs_in_pubmed': cs_bio_count,\n                    'bio_in_arxiv': bio_cs_count,\n                    'total_overlap': cs_bio_count + bio_cs_count\n                }\n                results['cross_domain_terms'].append(connection)\n                print(f\"   \ud83c\udfaf {bio_term} \u2194 {cs_term}: {connection['total_overlap']} connections\")\n\n    # Sort by strongest connections\n    results['cross_domain_terms'].sort(key=lambda x: x['total_overlap'], reverse=True)\n\n    return results\n\n# Example: Find connections between genomics and machine learning\nbio_terms = [\"genomics\", \"proteomics\", \"gene expression\", \"protein folding\"]\ncs_terms = [\"machine learning\", \"neural networks\", \"deep learning\", \"artificial intelligence\"]\n\nconnections = cross_domain_discovery(bio_terms, cs_terms)\n</code></pre>"},{"location":"research_workflows/#2-technology-transfer-analysis","title":"2. Technology Transfer Analysis","text":"<p>Track how computational methods move between fields:</p> <pre><code>from apilinker import ArXivConnector, NCBIConnector\nfrom datetime import datetime\n\ndef technology_transfer_analysis(technology, source_field, target_fields):\n    \"\"\"\n    Analyze how technologies transfer between research fields.\n    \"\"\"\n    arxiv = ArXivConnector()\n    ncbi = NCBIConnector(email=\"tech_transfer@university.edu\")\n\n    results = {\n        'source_papers': [],\n        'target_adoption': {},\n        'timeline': {}\n    }\n\n    # Get papers from source field\n    print(f\"\ud83d\udd0d Searching for '{technology}' in {source_field}...\")\n    source_papers = arxiv.search_papers(\n        f\"{technology} {source_field}\",\n        max_results=50,\n        sort_by=\"submittedDate\",\n        sort_order=\"descending\"\n    )\n\n    results['source_papers'] = source_papers\n    print(f\"   Found {len(source_papers)} papers in source field\")\n\n    # Check adoption in target fields\n    for field in target_fields:\n        print(f\"\ud83c\udfaf Checking adoption in {field}...\")\n\n        # Search both arXiv and PubMed\n        arxiv_papers = arxiv.search_papers(f\"{technology} {field}\", max_results=30)\n        pubmed_papers = ncbi.search_pubmed(f\"{technology} AND {field}\", max_results=30)\n\n        pubmed_count = len(pubmed_papers.get('esearchresult', {}).get('idlist', []))\n\n        results['target_adoption'][field] = {\n            'arxiv_papers': len(arxiv_papers),\n            'pubmed_papers': pubmed_count,\n            'total': len(arxiv_papers) + pubmed_count\n        }\n\n        print(f\"   {field}: {len(arxiv_papers)} arXiv + {pubmed_count} PubMed = {len(arxiv_papers) + pubmed_count} total\")\n\n    # Analyze timeline if we have dates\n    years = {}\n    for paper in source_papers:\n        if paper.get('published_date'):\n            year = paper['published_date'][:4]\n            years[year] = years.get(year, 0) + 1\n\n    results['timeline'] = years\n\n    # Summary\n    print(f\"\\n\ud83d\udcca Technology Transfer Summary for '{technology}':\")\n    print(f\"   Source ({source_field}): {len(source_papers)} papers\")\n\n    sorted_targets = sorted(\n        results['target_adoption'].items(),\n        key=lambda x: x[1]['total'],\n        reverse=True\n    )\n\n    for field, counts in sorted_targets:\n        print(f\"   Target ({field}): {counts['total']} papers\")\n\n    return results\n\n# Example: Analyze how transformers moved from NLP to other fields\ntransformer_analysis = technology_transfer_analysis(\n    technology=\"transformer\",\n    source_field=\"natural language processing\",\n    target_fields=[\"computer vision\", \"bioinformatics\", \"drug discovery\", \"genomics\"]\n)\n</code></pre>"},{"location":"research_workflows/#literature-review-automation","title":"Literature Review Automation","text":""},{"location":"research_workflows/#1-systematic-literature-review","title":"1. Systematic Literature Review","text":"<p>Automate systematic literature reviews following research standards:</p> <pre><code>from apilinker import NCBIConnector, ArXivConnector\nimport pandas as pd\nfrom datetime import datetime\n\ndef systematic_literature_review(\n    research_question,\n    search_terms,\n    inclusion_criteria,\n    date_range=None,\n    max_papers_per_db=200\n):\n    \"\"\"\n    Conduct a systematic literature review across multiple databases.\n    \"\"\"\n    ncbi = NCBIConnector(email=\"systematic_review@university.edu\")\n    arxiv = ArXivConnector()\n\n    print(f\"\ud83d\udccb Systematic Literature Review\")\n    print(f\"Research Question: {research_question}\")\n    print(f\"Search Terms: {', '.join(search_terms)}\")\n\n    all_papers = []\n\n    # Search each database\n    for term in search_terms:\n        # Search PubMed\n        pubmed_results = ncbi.search_pubmed(term, max_results=max_papers_per_db)\n        pubmed_ids = pubmed_results.get('esearchresult', {}).get('idlist', [])\n\n        if pubmed_ids:\n            summaries = ncbi.get_article_summaries(pubmed_ids)\n\n            for paper_id in pubmed_ids:\n                paper_info = summaries.get('result', {}).get(paper_id, {})\n                all_papers.append({\n                    'id': f\"pubmed_{paper_id}\",\n                    'title': paper_info.get('title', ''),\n                    'authors': '; '.join([a.get('name', '') for a in paper_info.get('authors', [])]),\n                    'journal': paper_info.get('source', ''),\n                    'year': paper_info.get('pubdate', '')[:4] if paper_info.get('pubdate') else '',\n                    'database': 'PubMed',\n                    'search_term': term,\n                    'abstract': ''  # Would need additional API call\n                })\n\n        # Search arXiv\n        arxiv_results = arxiv.search_papers(term, max_results=max_papers_per_db)\n\n        for paper in arxiv_results:\n            all_papers.append({\n                'id': f\"arxiv_{paper.get('arxiv_id', '')}\",\n                'title': paper.get('title', ''),\n                'authors': '; '.join(paper.get('authors', [])),\n                'journal': 'arXiv',\n                'year': paper.get('published_date', '')[:4] if paper.get('published_date') else '',\n                'database': 'arXiv',\n                'search_term': term,\n                'abstract': paper.get('summary', '')\n            })\n\n    # Create DataFrame for analysis\n    df = pd.DataFrame(all_papers)\n\n    # Remove duplicates by title\n    df = df.drop_duplicates(subset=['title'], keep='first')\n\n    # Apply inclusion criteria (basic example)\n    if date_range:\n        start_year, end_year = date_range\n        df = df[df['year'].astype(str).between(str(start_year), str(end_year))]\n\n    # Save results\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filename = f\"systematic_review_{timestamp}.csv\"\n    df.to_csv(filename, index=False)\n\n    # Summary statistics\n    print(f\"\\n\ud83d\udcca Review Results:\")\n    print(f\"   Total papers found: {len(all_papers)}\")\n    print(f\"   After deduplication: {len(df)}\")\n    print(f\"   Database breakdown:\")\n    print(df['database'].value_counts().to_string())\n    print(f\"   Saved to: {filename}\")\n\n    return df\n\n# Example: Systematic review on AI in healthcare\nhealthcare_ai_review = systematic_literature_review(\n    research_question=\"How is artificial intelligence being applied in healthcare diagnosis?\",\n    search_terms=[\n        \"artificial intelligence healthcare diagnosis\",\n        \"machine learning medical diagnosis\", \n        \"deep learning radiology\",\n        \"AI clinical decision support\"\n    ],\n    inclusion_criteria=\"peer-reviewed, English, last 5 years\",\n    date_range=(2019, 2024),\n    max_papers_per_db=100\n)\n</code></pre>"},{"location":"research_workflows/#data-collection-pipelines","title":"Data Collection Pipelines","text":""},{"location":"research_workflows/#1-automated-research-monitoring","title":"1. Automated Research Monitoring","text":"<p>Set up automated monitoring for new research in your field:</p> <pre><code>from apilinker import ApiLinker, ArXivConnector, NCBIConnector\nimport schedule\nimport time\n\nclass ResearchMonitor:\n    \"\"\"\n    Automated research monitoring system.\n    \"\"\"\n\n    def __init__(self, email, research_topics, output_dir=\"research_updates\"):\n        self.ncbi = NCBIConnector(email=email)\n        self.arxiv = ArXivConnector()\n        self.topics = research_topics\n        self.output_dir = output_dir\n\n        # Create output directory\n        import os\n        os.makedirs(output_dir, exist_ok=True)\n\n    def daily_update(self):\n        \"\"\"\n        Daily research update scan.\n        \"\"\"\n        print(f\"\ud83d\udd0d Daily research update - {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n\n        updates = {}\n\n        for topic in self.topics:\n            # Get recent arXiv papers (last 2 days)\n            arxiv_papers = self.arxiv.search_recent_papers(\n                category=\"cs.AI\" if \"AI\" in topic else \"cs.LG\",\n                days_back=2,\n                max_results=10\n            )\n\n            # Filter by topic\n            relevant_papers = [\n                paper for paper in arxiv_papers\n                if topic.lower() in paper.get('title', '').lower() or\n                   topic.lower() in paper.get('summary', '').lower()\n            ]\n\n            updates[topic] = {\n                'arxiv_papers': relevant_papers,\n                'count': len(relevant_papers)\n            }\n\n            print(f\"   \ud83d\udcc4 {topic}: {len(relevant_papers)} new papers\")\n\n        # Save daily update\n        if any(update['count'] &gt; 0 for update in updates.values()):\n            self._save_update(updates)\n\n        return updates\n\n    def _save_update(self, updates):\n        \"\"\"Save updates to file.\"\"\"\n        timestamp = datetime.now().strftime(\"%Y%m%d\")\n        filename = f\"{self.output_dir}/update_{timestamp}.md\"\n\n        with open(filename, \"w\") as f:\n            f.write(f\"# Research Update - {datetime.now().strftime('%Y-%m-%d')}\\n\\n\")\n\n            for topic, data in updates.items():\n                if data['count'] &gt; 0:\n                    f.write(f\"## {topic} ({data['count']} papers)\\n\\n\")\n\n                    for paper in data['arxiv_papers']:\n                        f.write(f\"### {paper['title']}\\n\")\n                        f.write(f\"**Authors:** {', '.join(paper['authors'][:3])}\\n\\n\")\n                        f.write(f\"**Abstract:** {paper['summary'][:200]}...\\n\\n\")\n                        f.write(f\"**Link:** {paper['links'].get('abstract_url', '')}\\n\\n\")\n\n        print(f\"\ud83d\udcbe Saved update to {filename}\")\n\n# Example usage\ndef setup_research_monitoring():\n    \"\"\"\n    Set up automated research monitoring.\n    \"\"\"\n    monitor = ResearchMonitor(\n        email=\"researcher@university.edu\",\n        research_topics=[\n            \"large language models\",\n            \"protein folding\",\n            \"quantum computing\",\n            \"computer vision\"\n        ]\n    )\n\n    # Schedule daily updates\n    schedule.every().day.at(\"09:00\").do(monitor.daily_update)\n\n    print(\"\ud83d\ude80 Research monitoring started!\")\n    print(\"\ud83d\udcc5 Daily updates scheduled for 9:00 AM\")\n    print(\"Press Ctrl+C to stop\")\n\n    try:\n        while True:\n            schedule.run_pending()\n            time.sleep(60)  # Check every minute\n    except KeyboardInterrupt:\n        print(\"\\n\ud83d\udc4b Research monitoring stopped\")\n\n# To start monitoring (uncomment to run):\n# setup_research_monitoring()\n</code></pre>"},{"location":"research_workflows/#best-practices-for-research","title":"Best Practices for Research","text":""},{"location":"research_workflows/#1-reproducible-research-workflows","title":"1. Reproducible Research Workflows","text":"<pre><code>from apilinker import ApiLinker\nimport yaml\nimport hashlib\nimport json\n\ndef create_reproducible_workflow(config_path):\n    \"\"\"\n    Create a reproducible research workflow with full provenance tracking.\n    \"\"\"\n    # Load configuration\n    with open(config_path, 'r') as f:\n        config = yaml.safe_load(f)\n\n    # Create hash of configuration for reproducibility\n    config_hash = hashlib.md5(json.dumps(config, sort_keys=True).encode()).hexdigest()\n\n    # Set up ApiLinker\n    linker = ApiLinker(config_path=config_path)\n\n    # Add metadata tracking\n    workflow_metadata = {\n        'timestamp': datetime.now().isoformat(),\n        'config_hash': config_hash,\n        'apilinker_version': '0.4.0',\n        'workflow_id': config_hash[:8]\n    }\n\n    print(f\"\ud83d\udd2c Starting reproducible workflow\")\n    print(f\"   Workflow ID: {workflow_metadata['workflow_id']}\")\n    print(f\"   Config Hash: {config_hash}\")\n\n    return linker, workflow_metadata\n\n# Example configuration for reproducible research\nexample_config = {\n    'research_info': {\n        'title': 'AI Applications in Drug Discovery',\n        'investigator': 'Dr. Research Scientist',\n        'institution': 'University Research Lab',\n        'date': '2024-01-15'\n    },\n    'data_sources': {\n        'pubmed_query': 'artificial intelligence drug discovery',\n        'arxiv_category': 'cs.LG',\n        'date_range': '2020-2024',\n        'max_papers': 500\n    },\n    'analysis_parameters': {\n        'min_citations': 10,\n        'exclude_preprints': False,\n        'language': 'English'\n    }\n}\n</code></pre>"},{"location":"research_workflows/#2-ethical-research-data-collection","title":"2. Ethical Research Data Collection","text":"<pre><code>from apilinker import NCBIConnector, ArXivConnector\nimport time\n\nclass EthicalResearchCollector:\n    \"\"\"\n    Research data collector with ethical guidelines built-in.\n    \"\"\"\n\n    def __init__(self, email, rate_limit_delay=1.0):\n        self.ncbi = NCBIConnector(email=email)\n        self.arxiv = ArXivConnector()\n        self.rate_limit_delay = rate_limit_delay\n        self.request_count = 0\n\n    def respectful_search(self, query, max_results=100, database=\"both\"):\n        \"\"\"\n        Search with respectful API usage and backoff on 429.\n        \"\"\"\n        results = {}\n\n        if database in [\"both\", \"pubmed\"]:\n            print(f\"\ud83d\udd0d Searching PubMed: {query}\")\n            pubmed_results = self.ncbi.search_pubmed(query, max_results=max_results)\n            results['pubmed'] = pubmed_results\n\n            # Rate limiting\n            time.sleep(self.rate_limit_delay)\n            self.request_count += 1\n\n        if database in [\"both\", \"arxiv\"]:\n            print(f\"\ud83d\udd0d Searching arXiv: {query}\")\n            arxiv_results = self.arxiv.search_papers(query, max_results=max_results)\n            results['arxiv'] = arxiv_results\n\n            # Rate limiting\n            time.sleep(self.rate_limit_delay)\n            self.request_count += 1\n\n        print(f\"\ud83d\udcca API requests made: {self.request_count}\")\n\n        return results\n\n    def get_usage_summary(self):\n        \"\"\"\n        Report on API usage for transparency.\n        \"\"\"\n        return {\n            'total_requests': self.request_count,\n            'rate_limit_delay': self.rate_limit_delay,\n            'apis_used': ['NCBI E-utilities', 'arXiv API'],\n            'data_use_policy': 'Research and educational use only'\n        }\n\n# Example usage\nethical_collector = EthicalResearchCollector(\n    email=\"ethical_researcher@university.edu\",\n    rate_limit_delay=1.5  # Be extra respectful\n)\n</code></pre> <p>This documentation demonstrates ApiLinker's power for research workflows. For more examples, see the <code>examples/</code> directory and visit our documentation at apilinker.readthedocs.io.</p>"},{"location":"security/","title":"Security Considerations","text":"<p>APILinker provides security features to ensure safe handling of API credentials and multi-user access. This document outlines security best practices and describes the available security features.</p>"},{"location":"security/#security-features","title":"Security Features","text":""},{"location":"security/#secure-credential-storage","title":"Secure Credential Storage","text":"<p>APILinker provides optional encrypted storage for sensitive credentials (for development convenience; consider dedicated secret managers in production):</p> <pre><code># Configure secure credential storage\nlinker = ApiLinker(\n    security_config={\n        \"master_password\": \"your-strong-password\",  # Or use environment variable\n        \"credential_storage_path\": \"./credentials.enc\"\n    }\n)\n\n# Store credentials securely\nlinker.store_credential(\"github_api\", {\n    \"token\": \"ghp_1234567890abcdefghijklmnopqrstuvwxyz\",\n    \"expires_at\": 1735689600  # Optional expiry timestamp\n})\n\n# Retrieve credentials\ncred = linker.get_credential(\"github_api\")\nprint(f\"Token: {cred['token']}\")\n</code></pre> <p>Note: ApiLinker defaults to using HTTPS without custom request/response encryption. Optional request/response encryption helpers exist for advanced scenarios; evaluate carefully and prefer provider-managed security controls.</p>"},{"location":"security/#multi-user-access-control","title":"Multi-User Access Control","text":"<p>Set up role-based access control for multi-user environments:</p> <pre><code># Enable access control\nlinker = ApiLinker(\n    security_config={\n        \"enable_access_control\": True,\n        \"users\": [\n            {\"username\": \"admin1\", \"role\": \"admin\"},\n            {\"username\": \"operator1\", \"role\": \"operator\"},\n            {\"username\": \"viewer1\", \"role\": \"viewer\"}\n        ]\n    }\n)\n\n# Add users programmatically\nuser = linker.add_user(\"developer1\", \"developer\")\nprint(f\"API Key: {user['api_key']}\")\n\n# List users\nusers = linker.list_users()\n</code></pre>"},{"location":"security/#oauth-enhancements","title":"OAuth Enhancements","text":"<p>APILinker now supports additional OAuth flows:</p> <ul> <li>PKCE Flow: For mobile and single-page applications</li> <li>Device Flow: For devices with limited input capabilities</li> </ul>"},{"location":"security/#configuration-file-security","title":"Configuration File Security","text":"<p>When using configuration files, avoid including sensitive values directly. You have several options:</p> <ol> <li>Use environment variable references as shown above</li> <li>Keep separate configuration files for development and production</li> <li>Use APILinker's secure credential storage</li> <li>Store sensitive values in a secret management system</li> </ol>"},{"location":"security/#oauth-authentication-flows","title":"OAuth Authentication Flows","text":""},{"location":"security/#oauth-client-credentials-flow","title":"OAuth Client Credentials Flow","text":"<pre><code>auth:\n  type: oauth2_client_credentials\n  client_id: \"your-client-id\"\n  client_secret: \"${CLIENT_SECRET}\"  # From environment variable\n  token_url: \"https://auth.example.com/oauth/token\"\n  scope: \"read write\"  # Optional\n</code></pre>"},{"location":"security/#oauth-pkce-flow-for-public-clients","title":"OAuth PKCE Flow (for public clients)","text":"<p>For public clients that cannot securely store a client secret:</p> <pre><code>from apilinker.core.auth import OAuth2PKCE\n\n# Initialize auth config\npkce_config = auth_manager.configure_auth({\n    \"type\": \"oauth2_pkce\",\n    \"client_id\": \"your-client-id\",\n    \"redirect_uri\": \"http://localhost:8080/callback\",\n    \"authorization_url\": \"https://auth.example.com/oauth/authorize\",\n    \"token_url\": \"https://auth.example.com/oauth/token\",\n    \"scope\": \"read write\",  # Optional\n    \"storage_key\": \"my_oauth_pkce_creds\"  # For secure storage\n})\n\n# Get authorization URL for user to visit\nauth_url = auth_manager.get_pkce_authorization_url(pkce_config)\nprint(f\"Visit this URL to authorize: {auth_url}\")\n\n# After receiving the authorization code from the redirect\nauth_config = auth_manager.complete_pkce_flow(pkce_config, \"authorization_code_from_redirect\")\n\n# Later, refresh the token when needed\nauth_config = auth_manager.refresh_pkce_token(auth_config)\n</code></pre>"},{"location":"security/#oauth-device-flow-for-limited-input-devices","title":"OAuth Device Flow (for limited input devices)","text":"<p>For devices with limited input capabilities:</p> <pre><code>from apilinker.core.auth import OAuth2DeviceFlow\n\n# Initialize device flow\ndevice_config = auth_manager.configure_auth({\n    \"type\": \"oauth2_device_flow\",\n    \"client_id\": \"your-client-id\",\n    \"device_authorization_url\": \"https://auth.example.com/oauth/device/code\",\n    \"token_url\": \"https://auth.example.com/oauth/token\",\n    \"storage_key\": \"my_device_flow_creds\"  # For secure storage\n})\n\n# Start the device flow\ndevice_config = auth_manager.start_device_flow(device_config)\n\n# Show the user the verification code and URL\nprint(f\"Please visit {device_config.verification_uri} and enter code: {device_config.user_code}\")\n\n# Poll for completion\nimport time\nwhile True:\n    completed, updated_config = auth_manager.poll_device_flow(device_config)\n    if completed:\n        print(\"Authorization complete!\")\n        device_config = updated_config\n        break\n    time.sleep(device_config.interval)  # Respect polling interval\n</code></pre>"},{"location":"security/#oauth-token-refresh","title":"OAuth Token Refresh","text":"<p>When using OAuth, ApiLinker automatically handles token refreshing when tokens expire.</p>"},{"location":"security/#credential-management","title":"Credential Management","text":""},{"location":"security/#environment-variables","title":"Environment Variables","text":"<p>Store API keys and tokens as environment variables rather than hardcoding them in your configuration files:</p> <pre><code>auth:\n  type: api_key\n  header: X-API-Key\n  key: ${MY_API_KEY}\n</code></pre> <p>In your Python code, you can set environment variables before running your script:</p> <pre><code>import os\nos.environ[\"MY_API_KEY\"] = \"your_api_key_here\"  # Set this securely\n</code></pre>"},{"location":"security/#secure-storage","title":"Secure Storage","text":"<p>For production environments, consider using:</p> <ul> <li>Environment variables set at the system or container level</li> <li>Secret management services like HashiCorp Vault or AWS Secrets Manager</li> <li>Encrypted configuration files with restricted access</li> </ul>"},{"location":"security/#authentication-methods","title":"Authentication Methods","text":"<p>ApiLinker supports several authentication methods, each with its own security considerations:</p>"},{"location":"security/#api-key-authentication","title":"API Key Authentication","text":"<pre><code>auth:\n  type: api_key\n  header: X-API-Key  # Header name varies by API\n  key: ${API_KEY}\n</code></pre> <p>Security tips: - Rotate API keys regularly - Use keys with the minimum required permissions - Set IP restrictions on API keys when supported by the service</p>"},{"location":"security/#bearer-token-authentication","title":"Bearer Token Authentication","text":"<pre><code>auth:\n  type: bearer\n  token: ${BEARER_TOKEN}\n</code></pre> <p>Security tips: - Use short-lived tokens when possible - Implement token refresh logic for long-running processes - Store refresh tokens with additional security measures</p>"},{"location":"security/#basic-authentication","title":"Basic Authentication","text":"<pre><code>auth:\n  type: basic\n  username: ${API_USERNAME}\n  password: ${API_PASSWORD}\n</code></pre> <p>Security tips: - Only use over HTTPS connections - Use application-specific passwords when available - Avoid using your main account credentials</p>"},{"location":"security/#oauth-20","title":"OAuth 2.0","text":"<pre><code>auth:\n  type: oauth2\n  client_id: ${CLIENT_ID}\n  client_secret: ${CLIENT_SECRET}\n  token_url: \"https://api.example.com/oauth/token\"\n  scope: \"read write\"\n</code></pre> <p>Security tips: - Store client secrets securely - Request only the scopes you need - Implement proper token storage and refresh logic</p>"},{"location":"security/#network-security","title":"Network Security","text":""},{"location":"security/#request-timeouts","title":"Request Timeouts","text":"<p>Set appropriate timeouts to prevent hanging connections:</p> <pre><code>source:\n  timeout: 30  # Timeout in seconds\n</code></pre>"},{"location":"security/#data-protection","title":"Data Protection","text":""},{"location":"security/#https","title":"HTTPS","text":"<p>Always use HTTPS for API endpoints to ensure data is encrypted in transit.</p>"},{"location":"security/#audit-logging","title":"Audit Logging","text":"<p>Enable audit logging for security-relevant events:</p> <pre><code>logging:\n  level: INFO\n  security_audit: true\n  file: \"apilinker_audit.log\"\n</code></pre>"},{"location":"security/#data-handling","title":"Data Handling","text":""},{"location":"security/#sensitive-data-filtering","title":"Sensitive Data Filtering","text":"<p>When logging API responses, sensitive data can be filtered:</p> <pre><code>logging:\n  filter_fields:\n    - \"password\"\n    - \"token\"\n    - \"secret\"\n    - \"credit_card\"\n</code></pre>"},{"location":"security/#data-validation","title":"Data Validation","text":"<p>Always validate data before processing:</p> <pre><code>mapping:\n  - source: \"get_user\"\n    target: \"create_profile\"\n    validation:\n      required_fields: [\"id\", \"email\"]\n      email_fields: [\"email\"]\n</code></pre>"},{"location":"security/#best-practices-for-custom-plugins","title":"Best Practices for Custom Plugins","text":"<p>When developing custom plugins:</p> <ol> <li>Validate all inputs to prevent injection attacks</li> <li>Do not log sensitive information</li> <li>Handle exceptions properly to avoid information leakage</li> <li>Follow the principle of least privilege when accessing resources</li> </ol>"},{"location":"security/#access-control-for-multi-user-environments","title":"Access Control for Multi-User Environments","text":"<p>For environments where multiple users need access to APILinker, you can enable role-based access control:</p> <pre><code>security:\n  enable_access_control: true\n  users:\n    - username: \"admin1\"\n      role: \"admin\"\n      api_key: \"optional-predefined-api-key\"\n    - username: \"viewer1\"\n      role: \"viewer\"\n</code></pre>"},{"location":"security/#available-roles","title":"Available Roles","text":"<ul> <li><code>admin</code>: Full access to all operations</li> <li><code>operator</code>: Can run syncs and view results</li> <li><code>developer</code>: Can modify configurations but not run syncs</li> <li><code>viewer</code>: Can only view configurations and results</li> </ul>"},{"location":"security/#permission-management","title":"Permission Management","text":"<p>Each role has a predefined set of permissions:</p> Permission Admin Operator Developer Viewer view_config \u2705 \u2705 \u2705 \u2705 edit_config \u2705 \u274c \u2705 \u274c run_sync \u2705 \u2705 \u274c \u274c view_results \u2705 \u2705 \u2705 \u2705 manage_users \u2705 \u274c \u274c \u274c manage_credentials \u2705 \u274c \u274c \u274c view_logs \u2705 \u2705 \u2705 \u274c access_analytics \u2705 \u2705 \u274c \u274c"},{"location":"security/#logging","title":"Logging","text":"<p>Avoid logging sensitive information like API keys, tokens, or personal data. The ApiLinker logger is configured to avoid logging sensitive data by default.</p> <p>For enhanced security, consider using a separate log file with restricted permissions:</p> <pre><code>logging:\n  level: INFO\n  file: \"/secure/path/apilinker.log\"\n</code></pre>"},{"location":"security/#security-configuration-example","title":"Security Configuration Example","text":"<p>A complete security-focused configuration might look like:</p> <pre><code>source:\n  type: rest\n  base_url: \"https://api.example.com\"\n  auth:\n    type: oauth2\n    client_id: ${CLIENT_ID}\n    client_secret: ${CLIENT_SECRET}\n    token_url: \"https://api.example.com/oauth/token\"\n    scope: \"read\"\n  timeout: 30\n  retry:\n    max_attempts: 3\n    backoff_factor: 2\n\nlogging:\n  level: INFO\n  security_audit: true\n  filter_fields:\n    - \"password\"\n    - \"token\"\n    - \"secret\"\n</code></pre> <p>Remember that security is an ongoing process. Regularly review and update your security practices, and stay informed about security updates for ApiLinker and its dependencies.</p>"},{"location":"troubleshooting/","title":"Troubleshooting Guide","text":"<p>This guide helps you diagnose and resolve common issues with ApiLinker and explains how to use the robust error handling and recovery system.</p>"},{"location":"troubleshooting/#error-handling-recovery-system","title":"Error Handling &amp; Recovery System","text":"<p>APILinker includes a sophisticated error handling and recovery system to make your API integrations more resilient against common failures. The system includes:</p> <ol> <li>Circuit Breakers - Prevent cascading failures during service outages</li> <li>Dead Letter Queues (DLQ) - Store failed operations for later retry</li> <li>Configurable Recovery Strategies - Apply different strategies for different error types</li> <li>Error Analytics - Track error patterns and trends</li> </ol>"},{"location":"troubleshooting/#using-the-error-handling-system","title":"Using the Error Handling System","text":"<p>The error handling system is configured in your configuration file under the <code>error_handling</code> section:</p> <pre><code>error_handling:\n  # Configure circuit breakers\n  circuit_breakers:\n    source_customer_api:  # Name of the circuit breaker\n      failure_threshold: 5  # Number of failures before opening circuit\n      reset_timeout_seconds: 60  # Seconds to wait before trying again\n      half_open_max_calls: 1  # Max calls allowed in half-open state\n\n  # Configure recovery strategies by error category\n  recovery_strategies:\n    network:  # Error category\n      - exponential_backoff\n      - circuit_breaker\n    rate_limit:\n      - exponential_backoff\n    server:\n      - circuit_breaker\n      - exponential_backoff\n\n  # Configure Dead Letter Queue\n  dlq:\n    directory: \"./dlq\"  # Directory to store failed operations\n</code></pre>"},{"location":"troubleshooting/#diagnostic-decision-tree","title":"Diagnostic Decision Tree","text":"<p>Start here and follow the branches to diagnose your issue:</p> <ol> <li>Installation Issues</li> <li>Package Not Found</li> <li>Version Conflicts</li> <li> <p>ImportError</p> </li> <li> <p>Configuration Issues</p> </li> <li>Invalid Configuration</li> <li>Environment Variables Not Working</li> <li> <p>File Not Found</p> </li> <li> <p>API Connection Issues</p> </li> <li>Connection Failed</li> <li>Authentication Failed</li> <li>SSL/Certificate Errors</li> <li>Timeout Errors</li> <li> <p>Circuit Breaker Open</p> </li> <li> <p>Mapping Issues</p> </li> <li>Missing Fields</li> <li>Transformation Errors</li> <li> <p>Type Errors</p> </li> <li> <p>Runtime Issues</p> </li> <li>Scheduling Problems</li> <li>Memory Usage</li> <li>Performance Problems</li> <li>DLQ Processing Errors</li> </ol>"},{"location":"troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"troubleshooting/#package-not-found","title":"Package Not Found","text":"<p>Symptoms: <pre><code>ERROR: Could not find a version that satisfies the requirement apilinker\n</code></pre></p> <p>Solutions: 1. Verify your Python version (3.8+ required):    <pre><code>python --version\n</code></pre></p> <ol> <li> <p>Update pip:    <pre><code>pip install --upgrade pip\n</code></pre></p> </li> <li> <p>Check your internet connection and try again.</p> </li> <li> <p>If using a corporate network, check proxy settings:    <pre><code>pip install apilinker --proxy http://your-proxy:port\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#version-conflicts","title":"Version Conflicts","text":"<p>Symptoms: <pre><code>ERROR: Cannot install apilinker due to dependency conflicts\n</code></pre></p> <p>Solutions: 1. Create a clean virtual environment:    <pre><code>python -m venv apilinker_env\nsource apilinker_env/bin/activate  # On Windows: apilinker_env\\Scripts\\activate\npip install apilinker\n</code></pre></p> <ol> <li>Install with the <code>--no-dependencies</code> flag and handle dependencies manually:    <pre><code>pip install --no-dependencies apilinker\npip install httpx pydantic pyyaml typer\n</code></pre></li> </ol>"},{"location":"troubleshooting/#importerror","title":"ImportError","text":"<p>Symptoms: <pre><code>ImportError: No module named apilinker\n</code></pre></p> <p>Solutions: 1. Verify installation:    <pre><code>pip list | grep apilinker\n</code></pre></p> <ol> <li> <p>Check your Python environment:    <pre><code># On Windows\nwhere python\n\n# On Linux/Mac\nwhich python\n</code></pre></p> </li> <li> <p>Try reinstalling:    <pre><code>pip uninstall -y apilinker\npip install apilinker\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#configuration-issues","title":"Configuration Issues","text":""},{"location":"troubleshooting/#invalid-configuration","title":"Invalid Configuration","text":"<p>Symptoms: <pre><code>ConfigError: Invalid configuration at path 'source.auth'\n</code></pre></p> <p>Solutions: 1. Validate your YAML syntax using an online validator.</p> <ol> <li> <p>Check the specific error message for details about what's wrong.</p> </li> <li> <p>Compare with the examples in the documentation.</p> </li> <li> <p>Common issues:</p> </li> <li>Indentation errors</li> <li>Missing required fields</li> <li>Incorrect value types</li> </ol>"},{"location":"troubleshooting/#environment-variables","title":"Environment Variables","text":"<p>Symptoms: Environment variables are not being replaced in your configuration.</p> <p>Solutions: 1. Verify the environment variable is set:    <pre><code># Windows\necho %API_KEY%\n\n# Linux/Mac\necho $API_KEY\n</code></pre></p> <ol> <li> <p>Check the syntax in your configuration file:    <pre><code># Correct\nauth:\n  token: ${API_KEY}\n\n# Incorrect\nauth:\n  token: $API_KEY\n  token: \"{API_KEY}\"\n</code></pre></p> </li> <li> <p>Set the environment variable in your script:    <pre><code>import os\nos.environ[\"API_KEY\"] = \"your_api_key\"\nlinker = ApiLinker(config_path=\"config.yaml\")\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#file-not-found","title":"File Not Found","text":"<p>Symptoms: <pre><code>FileNotFoundError: No such file or directory: 'config.yaml'\n</code></pre></p> <p>Solutions: 1. Check the file path:    <pre><code>import os\nprint(os.getcwd())  # Current working directory\n</code></pre></p> <ol> <li>Use absolute paths:    <pre><code>import os\nconfig_path = os.path.join(os.path.dirname(__file__), \"config.yaml\")\nlinker = ApiLinker(config_path=config_path)\n</code></pre></li> </ol>"},{"location":"troubleshooting/#api-connection-issues","title":"API Connection Issues","text":""},{"location":"troubleshooting/#connection-failed","title":"Connection Failed","text":"<p>Symptoms: <pre><code>ConnectionError: Failed to establish connection to api.example.com\n</code></pre></p> <p>Solutions: 1. Verify your internet connection.</p> <ol> <li> <p>Check if the API domain is correct and accessible:    <pre><code>ping api.example.com\n</code></pre></p> </li> <li> <p>Try with a different network or disable firewall temporarily.</p> </li> <li> <p>Add timeout and retry settings:    <pre><code>linker.add_source(\n    # Other configuration...\n    timeout=30,  # Seconds\n    retry={\n        \"max_attempts\": 3,\n        \"delay_seconds\": 2\n    }\n)\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#authentication-failed","title":"Authentication Failed","text":"<p>Symptoms: <pre><code>AuthenticationError: API responded with status code 401 Unauthorized\n</code></pre></p> <p>Solutions: 1. Verify your credentials are correct.</p> <ol> <li> <p>Check if your token or API key has expired.</p> </li> <li> <p>Ensure you're using the correct authentication method.</p> </li> <li> <p>Examine the API documentation for specific auth requirements.</p> </li> <li> <p>Enable debug logging to see the actual request:    <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#sslcertificate-errors","title":"SSL/Certificate Errors","text":"<p>Symptoms: <pre><code>SSLError: SSL certificate verification failed\n</code></pre></p> <p>Solutions: 1. Update your CA certificates.</p> <ol> <li> <p>If necessary (and safe), disable SSL verification:    <pre><code>linker.add_source(\n    # Other configuration...\n    verify_ssl=False  # WARNING: Security risk in production\n)\n</code></pre></p> </li> <li> <p>Specify a custom CA bundle:    <pre><code>linker.add_source(\n    # Other configuration...\n    verify_ssl=\"/path/to/ca-bundle.crt\"\n)\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#timeout-errors","title":"Timeout Errors","text":"<p>Symptoms: <pre><code>TimeoutError: Request timed out after 30 seconds\n</code></pre></p> <p>Solutions: 1. Increase timeout duration:    <pre><code>linker.add_source(\n    # Other configuration...\n    timeout=60  # Seconds\n)\n</code></pre></p> <ol> <li> <p>Check if the API is experiencing high latency.</p> </li> <li> <p>Consider adding pagination for large data sets.</p> </li> </ol>"},{"location":"troubleshooting/#mapping-issues","title":"Mapping Issues","text":""},{"location":"troubleshooting/#missing-fields","title":"Missing Fields","text":"<p>Symptoms: <pre><code>KeyError: 'Field not found in source data: user_profile'\n</code></pre></p> <p>Solutions: 1. Print the actual response data to inspect the structure:    <pre><code>data = linker.fetch(\"get_users\")\nprint(data[0])  # Print first record\n</code></pre></p> <ol> <li> <p>Use dot notation for nested fields:    <pre><code>linker.add_mapping(\n    # Other configuration...\n    fields=[\n        {\"source\": \"user.profile.name\", \"target\": \"name\"}\n    ]\n)\n</code></pre></p> </li> <li> <p>Add conditional mapping:    <pre><code>linker.add_mapping(\n    # Other configuration...\n    fields=[\n        {\n            \"source\": \"profile.name\", \n            \"target\": \"name\",\n            \"condition\": {\n                \"field\": \"profile\",\n                \"operator\": \"exists\"\n            }\n        }\n    ]\n)\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#transformation-errors","title":"Transformation Errors","text":"<p>Symptoms: <pre><code>TransformError: Error transforming value: invalid date format\n</code></pre></p> <p>Solutions: 1. Check the input data format.</p> <ol> <li> <p>Add validation in your transformer:    <pre><code>def date_transformer(value, **kwargs):\n    if not value or not isinstance(value, str):\n        return None\n\n    try:\n        # Transformation logic\n        return transformed_value\n    except ValueError:\n        return kwargs.get(\"default\", None)\n</code></pre></p> </li> <li> <p>Test the transformer directly:    <pre><code>result = linker.mapper.transform(\"2023-01-01\", \"date_transformer\")\nprint(result)\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#type-errors","title":"Type Errors","text":"<p>Symptoms: <pre><code>TypeError: Cannot process input of type: dict\n</code></pre></p> <p>Solutions: 1. Add type checking:    <pre><code>def my_transformer(value, **kwargs):\n    if isinstance(value, dict):\n        return json.dumps(value)\n    elif isinstance(value, (int, float)):\n        return str(value)\n    return value\n</code></pre></p> <ol> <li>Use a pre-processor:    <pre><code>def pre_process(data):\n    for item in data:\n        if \"amount\" in item and item[\"amount\"] is not None:\n            item[\"amount\"] = float(item[\"amount\"])\n    return data\n\nlinker.add_source_processor(\"get_data\", pre_process)\n</code></pre></li> </ol>"},{"location":"troubleshooting/#api-connection-issues_1","title":"API Connection Issues","text":""},{"location":"troubleshooting/#connection-failed_1","title":"Connection Failed","text":"<p>Symptoms: <pre><code>ConnectionError: Failed to establish connection to api.example.com\n</code></pre></p> <p>Solutions: 1. Check your internet connection 2. Verify the API is online using a tool like cURL or Postman 3. Check if the API domain resolves correctly:    <pre><code>ping api.example.com\n</code></pre> 4. Check for firewall or proxy issues in your environment</p>"},{"location":"troubleshooting/#authentication-failed_1","title":"Authentication Failed","text":"<p>Symptoms: <pre><code>APILinkerError: [AUTHENTICATION] Failed to fetch data: 401 Unauthorized\n</code></pre></p> <p>Solutions: 1. Verify your credentials are correct 2. Check if the token has expired 3. Ensure you're using the correct authentication method for the API 4. Check if your API key has the necessary permissions</p>"},{"location":"troubleshooting/#sslcertificate-errors_1","title":"SSL/Certificate Errors","text":"<p>Symptoms: <pre><code>SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed\n</code></pre></p> <p>Solutions: 1. Update your CA certificates 2. If working in a development environment, you can disable verification (not recommended for production):    <pre><code>connector.client.verify = False\n</code></pre> 3. Provide the path to your custom certificate:    <pre><code>connector.client.verify = '/path/to/cert.pem'\n</code></pre></p>"},{"location":"troubleshooting/#timeout-errors_1","title":"Timeout Errors","text":"<p>Symptoms: <pre><code>APILinkerError: [TIMEOUT] Failed to fetch data: Request timed out\n</code></pre></p> <p>Solutions: 1. Increase the timeout in your configuration:    <pre><code>source:\n  timeout: 60  # Seconds\n</code></pre> 2. Check if the API endpoint is slow or under heavy load 3. Consider adding exponential backoff retry strategy:    <pre><code>error_handling:\n  recovery_strategies:\n    timeout:\n      - exponential_backoff\n</code></pre></p>"},{"location":"troubleshooting/#circuit-breaker-open","title":"Circuit Breaker Open","text":"<p>Symptoms: <pre><code>APILinkerError: Circuit breaker 'source_customer_api' is open\n</code></pre></p> <p>Solutions: 1. Wait for the circuit breaker to reset (typically 60 seconds by default) 2. Check the health of the API service that's failing 3. Adjust your circuit breaker configuration if needed:    <pre><code>error_handling:\n  circuit_breakers:\n    source_customer_api:\n      failure_threshold: 10  # More permissive\n      reset_timeout_seconds: 30  # Quicker reset\n</code></pre> 4. Use error analytics to diagnose recurring problems:    <pre><code>error_stats = linker.get_error_analytics()\nprint(error_stats)\n</code></pre></p>"},{"location":"troubleshooting/#runtime-issues","title":"Runtime Issues","text":""},{"location":"troubleshooting/#scheduling-problems","title":"Scheduling Problems","text":"<p>Symptoms: Scheduled syncs not running at expected times.</p> <p>Solutions: 1. Check your system time and timezone.</p> <ol> <li> <p>Verify the cron expression format.</p> </li> <li> <p>Ensure your script is kept running:    <pre><code># Add this at the end of your script\ntry:\n    # Keep the process alive\n    while True:\n        time.sleep(60)\nexcept KeyboardInterrupt:\n    print(\"Stopping scheduled syncs\")\n    linker.stop_scheduled_sync()\n</code></pre></p> </li> <li> <p>Use a dedicated task scheduler like systemd, cron, or Windows Task Scheduler.</p> </li> </ol>"},{"location":"troubleshooting/#memory-usage","title":"Memory Usage","text":"<p>Symptoms: High memory usage or <code>MemoryError</code> exceptions.</p> <p>Solutions: 1. Process data in batches:    <pre><code>linker.add_mapping(\n    # Other configuration...\n    batch_size=100  # Process 100 records at a time\n)\n</code></pre></p> <ol> <li> <p>Use pagination with limits:    <pre><code>linker.add_source(\n    # Other configuration...\n    endpoints={\n        \"get_data\": {\n            # Other configuration...\n            \"pagination\": {\n                \"limit\": 200,  # Get 200 records per page\n                \"page_param\": \"page\"\n            }\n        }\n    }\n)\n</code></pre></p> </li> <li> <p>Implement a custom stream processor for very large datasets.</p> </li> </ol>"},{"location":"troubleshooting/#performance-problems","title":"Performance Problems","text":"<p>Symptoms: - Syncs take longer than expected - High memory usage - Slow response times</p> <p>Solutions: 1. Use batch processing for large datasets 2. Add appropriate indexes to your database 3. Use pagination for large API responses 4. Profile your transformers to identify bottlenecks 5. Consider adding caching for frequently accessed data</p>"},{"location":"troubleshooting/#dlq-processing-errors","title":"DLQ Processing Errors","text":"<p>Symptoms: <pre><code>Failed to retry DLQ item: item_id\n</code></pre></p> <p>Solutions: 1. Check the DLQ item's payload and error details:    <pre><code>items = linker.dlq.get_items(limit=10)\nprint(items[0])  # Examine the first item\n</code></pre></p> <ol> <li> <p>Process specific types of failed operations:    <pre><code>results = linker.process_dlq(operation_type=\"source_customer_api\")\nprint(f\"Processed {results['total_processed']} items, {results['successful']} succeeded\")\n</code></pre></p> </li> <li> <p>Manually fix issues and retry:    <pre><code># For specific item\nlinker.dlq.retry_item(\"item_id\", my_operation_function)\n</code></pre></p> </li> <li> <p>Check the error category distribution to identify patterns:    <pre><code>analytics = linker.get_error_analytics()\nprint(analytics[\"error_counts_by_category\"])\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#error-handling-system-details","title":"Error Handling System Details","text":""},{"location":"troubleshooting/#circuit-breaker-pattern","title":"Circuit Breaker Pattern","text":"<p>The circuit breaker pattern prevents cascading failures by temporarily stopping calls to failing services. It has three states:</p> <ul> <li>CLOSED - Normal operation, requests pass through</li> <li>OPEN - Service is failing, requests fail fast without calling the service</li> <li>HALF-OPEN - Testing if service has recovered with limited requests</li> </ul> <p>Configuration options: <pre><code>circuit_breakers:\n  name_of_breaker:\n    failure_threshold: 5      # Failures before opening\n    reset_timeout_seconds: 60 # Time before half-open\n    half_open_max_calls: 1    # Test calls allowed\n</code></pre></p>"},{"location":"troubleshooting/#recovery-strategies","title":"Recovery Strategies","text":"<p>APILinker supports these recovery strategies:</p> <ul> <li>RETRY - Simple retry without delay</li> <li>EXPONENTIAL_BACKOFF - Retry with increasing delays</li> <li>CIRCUIT_BREAKER - Use circuit breaker pattern</li> <li>FALLBACK - Use default data instead</li> <li>SKIP - Skip the operation</li> <li>FAIL_FAST - Fail immediately</li> </ul> <p>Configure by error category: <pre><code>recovery_strategies:\n  network:                  # Error category\n    - exponential_backoff   # First strategy\n    - circuit_breaker       # Second strategy\n</code></pre></p> <p>Available error categories: - NETWORK - Network connectivity issues - AUTHENTICATION - Auth failures - VALIDATION - Invalid data - TIMEOUT - Request timeouts - RATE_LIMIT - API returned 429 (rate limited): apply exponential backoff and retry - SERVER - Server errors (5xx) - CLIENT - Client errors (4xx) - MAPPING - Data mapping errors - PLUGIN - Plugin errors - UNKNOWN - Uncategorized errors</p>"},{"location":"troubleshooting/#dead-letter-queue-dlq","title":"Dead Letter Queue (DLQ)","text":"<p>The DLQ stores failed operations for later analysis and retry. Each entry contains: - Error details (category, message, status code) - Original payload that caused the failure - Timestamp and operation context - Correlation ID for tracing</p> <p>Access DLQ data: <pre><code># Get failed operations\nitems = linker.dlq.get_items(error_category=ErrorCategory.RATE_LIMIT)\n\n# Retry operations\nlinker.process_dlq(operation_type=\"source_customers\", limit=10)\n</code></pre></p>"},{"location":"troubleshooting/#error-analytics","title":"Error Analytics","text":"<p>The error analytics system tracks: - Error counts by category - Error rates over time - Top error types</p> <p>Access analytics: <pre><code>analytics = linker.get_error_analytics()\nprint(f\"Error rate: {analytics['recent_error_rate']} errors/minute\")\nprint(f\"Top errors: {analytics['top_errors']}\")\n</code></pre></p> <p>Symptoms: Syncs taking too long to complete.</p> <p>Solutions: 1. Enable performance logging:    <pre><code>linker = ApiLinker(performance_logging=True)\n</code></pre></p> <ol> <li>Optimize transformers:</li> <li>Avoid unnecessary operations</li> <li>Cache repeated calculations</li> <li> <p>Use built-in functions where possible</p> </li> <li> <p>Use concurrent requests when appropriate:    <pre><code>linker.add_source(\n    # Other configuration...\n    concurrency=5  # Up to 5 concurrent requests\n)\n</code></pre></p> </li> <li> <p>Implement partial syncs with filters:    <pre><code>linker.add_source(\n    # Other configuration...\n    endpoints={\n        \"get_data\": {\n            # Other configuration...\n            \"params\": {\n                \"updated_since\": \"{{last_sync}}\"  # Only get recently changed data\n            }\n        }\n    }\n)\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#debugging-techniques","title":"Debugging Techniques","text":""},{"location":"troubleshooting/#enable-debug-logging","title":"Enable Debug Logging","text":"<pre><code>import logging\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n</code></pre>"},{"location":"troubleshooting/#use-dry-run-mode","title":"Use Dry Run Mode","text":"<pre><code># Test the sync without making changes\nresult = linker.sync(dry_run=True)\nprint(f\"Would sync {result.count} records\")\nprint(f\"Preview: {result.preview[:3]}\")  # First 3 records\n</code></pre>"},{"location":"troubleshooting/#inspect-api-requests","title":"Inspect API Requests","text":"<pre><code># Install HTTP debugging tool\n# pip install httpx-debug\n\nimport httpx_debug\nhttpx_debug.install()  # Shows all HTTP requests and responses\n</code></pre>"},{"location":"troubleshooting/#interactive-debugging","title":"Interactive Debugging","text":"<pre><code># Add this where you want to inspect\nimport pdb; pdb.set_trace()\n# or\nbreakpoint()  # Python 3.7+\n</code></pre> <p>If you're still experiencing issues after trying these solutions, please open an issue on GitHub with detailed information about your problem.</p>"},{"location":"developer-guide/architecture/","title":"System Architecture","text":""},{"location":"developer-guide/architecture/#high-level-design","title":"High-Level Design","text":"<p>ApiLinker follows a modular, plugin-based architecture with clear separation of concerns:</p> <pre><code>graph TD\n    CLI[CLI Layer] --&gt; Orch[Main Orchestrator]\n    Orch --&gt; Source[Source Connectors]\n    Orch --&gt; Target[Target Connectors]\n    Orch --&gt; Mapper[Field Mapper]\n    Orch --&gt; Sched[Scheduler]\n\n    Source --&gt; Plugin[Plugin System]\n    Target --&gt; Plugin\n    Mapper --&gt; Transform[Transform Pipeline]\n    Sched --&gt; Error[Error Handling]\n</code></pre>"},{"location":"developer-guide/architecture/#core-design-patterns","title":"Core Design Patterns","text":"<p>Real-time event ingestion is supported via webhooks, message queues, and Server-Sent Events (SSE).</p> <ol> <li>Strategy Pattern: Different connectors, authenticators, and transformers</li> <li>Factory Pattern: Plugin instantiation and connector creation</li> <li>Observer Pattern: Event handling and monitoring</li> <li>Circuit Breaker Pattern: Error handling and recovery</li> <li>Template Method Pattern: Base classes with customizable steps</li> </ol>"},{"location":"developer-guide/architecture/#package-structure","title":"Package Structure","text":"<pre><code>apilinker/\n\u251c\u2500\u2500 __init__.py                 # Main exports and version\n\u251c\u2500\u2500 api_linker.py              # Main orchestrator class\n\u251c\u2500\u2500 cli.py                     # Command-line interface\n\u251c\u2500\u2500 core/                      # Core system components\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 connector.py           # Base connector classes\n\u2502   \u251c\u2500\u2500 mapper.py              # Field mapping and transformations\n\u2502   \u251c\u2500\u2500 scheduler.py           # Scheduling and automation\n\u2502   \u251c\u2500\u2500 auth.py                # Authentication system\n\u2502   \u251c\u2500\u2500 error_handling.py      # Error handling and recovery\n\u2502   \u251c\u2500\u2500 logger.py              # Logging system\n\u2502   \u251c\u2500\u2500 plugins.py             # Plugin base classes\n\u2502   \u251c\u2500\u2500 security.py            # Security features\n\u2502   \u2514\u2500\u2500 security_integration.py # Security integration layer\n\u251c\u2500\u2500 connectors/                # Connector implementations\n\u2502   \u251c\u2500\u2500 scientific/            # Research/scientific APIs\n\u2502   \u2514\u2500\u2500 general/               # General purpose APIs\n\u251c\u2500\u2500 plugins/                   # Built-in plugins\n\u2514\u2500\u2500 examples/                  # Usage examples\n</code></pre>"},{"location":"developer-guide/benchmarks/","title":"Performance Benchmarks","text":"<p>This page describes the performance characteristics and benchmarking methodology for ApiLinker.</p>"},{"location":"developer-guide/benchmarks/#overview","title":"Overview","text":"<p>ApiLinker has been rigorously benchmarked to ensure it meets the requirements of high-throughput scientific workflows. All benchmarks are reproducible and can be run locally.</p>"},{"location":"developer-guide/benchmarks/#quick-results","title":"Quick Results","text":"Scenario Throughput (nominal) Throughput (under faults) Success Rate Bibliographic Enrichment 45.3 \u00b1 3.2 rps 12.1 \u00b1 2.1 rps 99.7% PubMed Sampling 32.8 \u00b1 2.5 rps 8.3 \u00b1 1.8 rps 98.9% Issue Migration 18.4 \u00b1 1.9 rps 14.2 \u00b1 2.3 rps 96.1% <p>Reference hardware: Intel i7-9750H, 16GB RAM, 100 Mbps network</p>"},{"location":"developer-guide/benchmarks/#benchmark-scenarios","title":"Benchmark Scenarios","text":""},{"location":"developer-guide/benchmarks/#1-bibliographic-enrichment-crossref-semantic-scholar","title":"1. Bibliographic Enrichment (CrossRef \u2192 Semantic Scholar)","text":"<p>Objective: Validate schema-conformant mapping and throughput for academic metadata.</p> <p>Metrics: - Throughput (records/second) - Schema validation pass rate (%) - Field mapping accuracy (%) - Latency distribution (p50, p95, p99)</p> <p>Expected Results: - Throughput: 40-50 records/sec - Schema conformance: &gt;99.5% - Latency p95: &lt;500ms</p>"},{"location":"developer-guide/benchmarks/#2-literature-sampling-ncbipubmed-csv","title":"2. Literature Sampling (NCBI/PubMed \u2192 CSV)","text":"<p>Objective: Demonstrate stable pagination and reproducible exports.</p> <p>Metrics: - Total records retrieved - Pagination consistency - Export completeness - Memory usage</p> <p>Expected Results: - Throughput: 30-40 records/sec - Pagination: 100% consistent - Deterministic record count</p>"},{"location":"developer-guide/benchmarks/#3-issue-migration-github-gitlab","title":"3. Issue Migration (GitHub \u2192 GitLab)","text":"<p>Objective: Preserve data invariants under field transformations.</p> <p>Metrics: - Invariant preservation rate (%) - Label transformation accuracy (%) - State mapping correctness (%)</p> <p>Expected Results: - ID preservation: 100% - Label accuracy: 100% - Throughput: 15-25 records/sec</p>"},{"location":"developer-guide/benchmarks/#running-benchmarks","title":"Running Benchmarks","text":""},{"location":"developer-guide/benchmarks/#prerequisites","title":"Prerequisites","text":"<pre><code>pip install apilinker matplotlib pytest-benchmark httpx\n</code></pre>"},{"location":"developer-guide/benchmarks/#run-all-benchmarks","title":"Run All Benchmarks","text":"<pre><code>cd benchmarks\npython run_benchmarks.py\n</code></pre> <p>Output: Results will be saved to <code>benchmarks/results/</code>: - <code>results.json</code>: Machine-readable stats - <code>README.md</code>: Human-readable summary - <code>mean_latency_ms.png</code>: Latency distribution chart - <code>throughput_rps.png</code>: Throughput comparison chart</p>"},{"location":"developer-guide/benchmarks/#run-individual-scenarios","title":"Run Individual Scenarios","text":"<pre><code>from benchmarks.scenarios import BenchmarkScenarios\n\nscenarios = BenchmarkScenarios()\n\n# Transformation throughput\nresults = scenarios.run_transformation_benchmark(num_records=1000)\nprint(f\"Throughput: {results['throughput']:.2f} records/sec\")\n\n# HTTP retry behavior\nresults = scenarios.run_http_retry_benchmark(failure_rate=0.1)\nprint(f\"Success rate: {results['success_rate']:.1%}\")\n</code></pre>"},{"location":"developer-guide/benchmarks/#fault-injection-testing","title":"Fault Injection Testing","text":"<p>ApiLinker includes fault injection to validate resilience:</p> <ul> <li>Simulated 429 (rate limit): Triggers exponential backoff</li> <li>Simulated 5xx errors: Tests retry logic and circuit breakers</li> <li>Network timeouts: Validates DLQ (Dead Letter Queue) functionality</li> </ul>"},{"location":"developer-guide/benchmarks/#mock-server-for-reproducibility","title":"Mock Server for Reproducibility","text":"<p>For deterministic testing without external API dependencies:</p> <pre><code># Terminal 1: Start mock server\npython benchmarks/mock_server.py\n\n# Terminal 2: Run benchmarks\nexport BENCHMARK_USE_MOCK=true\npython run_benchmarks.py\n</code></pre> <p>The mock server simulates: - Pagination (offset and cursor-based) - Rate limiting - Transient failures - Authentication schemes</p>"},{"location":"developer-guide/benchmarks/#interpreting-results","title":"Interpreting Results","text":"<p>Results are output in JSON format with the following structure:</p> <pre><code>{\n  \"scenarios\": [\n    {\n      \"name\": \"bibliographic_enrichment\",\n      \"nominal\": {\n        \"throughput_rps\": 45.3,\n        \"latency_p95_ms\": 420,\n        \"success_rate\": 0.997\n      },\n      \"fault_injected\": {\n        \"throughput_rps\": 12.1,\n        \"success_rate\": 0.947,\n        \"circuit_breaker_activations\": 2\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"developer-guide/benchmarks/#performance-notes","title":"Performance Notes","text":"<p>Factors affecting throughput: - API provider rate limits - Network latency - Local CPU/memory resources - Payload size</p> <p>Expected variance: \u00b110% due to network variability (\u00b15% with mock server).</p>"},{"location":"developer-guide/benchmarks/#full-reproduction-guide","title":"Full Reproduction Guide","text":"<p>For complete reproduction instructions, see: benchmarks/REPRODUCTION.md</p>"},{"location":"developer-guide/benchmarks/#citation","title":"Citation","text":"<p>If you use these benchmarks in your research, please cite ApiLinker (see How to Cite).</p>"},{"location":"developer-guide/contributing/","title":"Development Guide","text":""},{"location":"developer-guide/contributing/#code-quality","title":"Code Quality","text":"<p>This project maintains code quality through automated testing and linting tools.</p>"},{"location":"developer-guide/contributing/#running-tests","title":"Running Tests","text":"<p>Run the full test suite: <pre><code>pytest\n</code></pre></p> <p>Run tests with coverage: <pre><code>pytest --cov=apilinker --cov-report=html\n</code></pre></p>"},{"location":"developer-guide/contributing/#code-formatting-and-linting","title":"Code Formatting and Linting","text":"<p>Check code style: <pre><code>flake8 apilinker\nmypy apilinker\nblack apilinker --check\n</code></pre></p> <p>Auto-format code: <pre><code>black apilinker\n</code></pre></p>"},{"location":"developer-guide/contributing/#best-practices","title":"Best Practices","text":"<ol> <li>Write tests for new features - Maintain or improve code coverage</li> <li>Run tests before committing - Ensure changes don't break existing functionality</li> <li>Follow PEP 8 style guidelines - Use black for consistent formatting</li> <li>Add type hints - Help with code maintainability and IDE support</li> </ol>"},{"location":"developer-guide/core-components/","title":"Core Components","text":""},{"location":"developer-guide/core-components/#apilinker-class","title":"ApiLinker Class","text":"<p>The <code>ApiLinker</code> class is the main orchestrator that coordinates all system components.</p> <pre><code>class ApiLinker:\n    \"\"\"\n    Main orchestrator class for API integration workflows.\n    \"\"\"\n    def sync(self) -&gt; SyncResult:\n        \"\"\"Execute synchronization workflow.\"\"\"\n</code></pre> <p>It manages: - Source and target connectors - Field mapping and transformations - Error recovery and retry logic - Scheduling capabilities - Security and monitoring integration</p>"},{"location":"developer-guide/core-components/#apiconnector-base-class","title":"ApiConnector Base Class","text":"<p>The <code>ApiConnector</code> is the foundation for all API connectors.</p> <ul> <li>Consistent Interface: Uniform methods for all API types.</li> <li>Error Handling: Built-in retry logic and circuit breakers.</li> <li>Rate Limiting: Respectful API usage with configurable limits.</li> <li>SSE Streaming: <code>stream_sse(...)</code> and <code>consume_sse(...)</code> for real-time event streams.</li> </ul>"},{"location":"developer-guide/core-components/#fieldmapper-class","title":"FieldMapper Class","text":"<p>The <code>FieldMapper</code> handles advanced data transformation.</p> <ul> <li>Nested Mapping: Support for dot notation (e.g., <code>user.profile.name</code>).</li> <li>Transformers: Built-in and custom transformation functions.</li> <li>Conditional Logic: Apply mappings based on data values.</li> </ul>"},{"location":"developer-guide/core-components/#scheduler-class","title":"Scheduler Class","text":"<p>The <code>Scheduler</code> provides sophisticated automation.</p> <ul> <li>Intervals: Run jobs every X minutes/hours.</li> <li>Cron: Support for complex cron expressions.</li> <li>Persistence: Job state tracking and recovery.</li> </ul>"},{"location":"developer-guide/plugins/","title":"Plugin System Architecture","text":"<p>ApiLinker features an extensible plugin architecture allowing users to add custom functionality without modifying the core codebase.</p>"},{"location":"developer-guide/plugins/#plugin-types","title":"Plugin Types","text":"<ul> <li>ConnectorPlugin: Custom API connectors.</li> <li>TransformerPlugin: Data transformation functions.</li> <li>AuthPlugin: Authentication methods.</li> <li>SchedulerPlugin: Custom scheduling triggers.</li> <li>MonitoringPlugin: Metrics and monitoring.</li> </ul>"},{"location":"developer-guide/plugins/#creating-a-plugin","title":"Creating a Plugin","text":"<p>Plugins inherit from <code>PluginBase</code> and must implement <code>initialize</code> and <code>cleanup</code> methods.</p>"},{"location":"developer-guide/plugins/#example-custom-connector","title":"Example: Custom Connector","text":"<pre><code>from apilinker.core.plugins import ConnectorPlugin\nfrom apilinker.core.connector import ApiConnector\n\nclass CustomAPIConnector(ConnectorPlugin, ApiConnector):\n    plugin_type = \"connector\"\n    plugin_name = \"custom_api\"\n\n    def fetch_data(self, endpoint_name: str, **kwargs):\n        # Implementation\n        pass\n</code></pre>"},{"location":"developer-guide/plugins/#plugin-discovery","title":"Plugin Discovery","text":"<p>The <code>PluginManager</code> automatically discovers plugins in: 1. The <code>apilinker.plugins</code> namespace. 2. Directories specified in configuration. 3. Registered entry points.</p>"},{"location":"developer-guide/release-process/","title":"Release Process","text":"<p>This guide covers version management and releasing new versions of APILinker.</p>"},{"location":"developer-guide/release-process/#version-bumping","title":"Version Bumping","text":"<p>APILinker uses <code>bump-my-version</code> for version management.</p>"},{"location":"developer-guide/release-process/#bump-patch-version-053-054","title":"Bump Patch Version (0.5.3 \u2192 0.5.4)","text":"<pre><code>bump-my-version bump patch\n</code></pre>"},{"location":"developer-guide/release-process/#bump-minor-version-053-060","title":"Bump Minor Version (0.5.3 \u2192 0.6.0)","text":"<pre><code>bump-my-version bump minor\n</code></pre>"},{"location":"developer-guide/release-process/#bump-major-version-053-100","title":"Bump Major Version (0.5.3 \u2192 1.0.0)","text":"<pre><code>bump-my-version bump major\n</code></pre>"},{"location":"developer-guide/release-process/#release-workflow","title":"Release Workflow","text":"<ol> <li>Update CHANGELOG.md with new version details</li> <li>Bump version: <code>bump-my-version bump &lt;patch|minor|major&gt;</code></li> <li>Commit changes: <code>git commit -am \"Bump version to X.Y.Z\"</code></li> <li>Create tag: <code>git tag vX.Y.Z</code></li> <li>Push: <code>git push &amp;&amp; git push --tags</code></li> <li>GitHub Actions will automatically publish to PyPI</li> </ol>"},{"location":"developer-guide/release-process/#configuration","title":"Configuration","text":"<p>Version bumping is configured in <code>.bumpversion.toml</code>.</p>"},{"location":"developer-guide/testing/","title":"Testing &amp; Validation","text":"<p>ApiLinker follows rigorous testing practices to ensure reliability and correctness.</p>"},{"location":"developer-guide/testing/#test-suite-overview","title":"Test Suite Overview","text":"<p>The test suite covers:</p> <ul> <li>Unit tests: Core functionality (mapping, transformers, authentication)</li> <li>Integration tests: End-to-end API workflows</li> <li>Connector tests: Research connector validation</li> <li>Error handling tests: Retry logic, circuit breakers, DLQ</li> </ul>"},{"location":"developer-guide/testing/#running-tests","title":"Running Tests","text":""},{"location":"developer-guide/testing/#run-all-tests","title":"Run All Tests","text":"<pre><code>pytest\n</code></pre>"},{"location":"developer-guide/testing/#run-with-coverage","title":"Run with Coverage","text":"<pre><code>pytest --cov=apilinker --cov-report=html\n</code></pre> <p>View coverage report at <code>htmlcov/index.html</code>.</p>"},{"location":"developer-guide/testing/#run-specific-test-categories","title":"Run Specific Test Categories","text":"<pre><code># Unit tests only\npytest tests/unit/\n\n# Integration tests\npytest tests/integration/\n\n# Connector tests\npytest tests/connectors/\n</code></pre>"},{"location":"developer-guide/testing/#test-coverage","title":"Test Coverage","text":"<p>Current test coverage: &gt;80%</p> <p>Coverage is tracked automatically in CI/CD. View the latest coverage report on GitHub Actions.</p>"},{"location":"developer-guide/testing/#continuous-integration","title":"Continuous Integration","text":"<p>ApiLinker uses GitHub Actions for automated testing on every commit:</p> <ul> <li>Multi-platform: Tests run on Ubuntu, Windows, and macOS</li> <li>Multi-version: Python 3.9, 3.10, 3.11, 3.12</li> <li>Quality checks: Linting (flake8, black), type checking (mypy)</li> <li>Coverage enforcement: Minimum 80% code coverage required</li> </ul> <p>View CI configuration: <code>.github/workflows/ci.yml</code></p>"},{"location":"developer-guide/testing/#testing-methodology","title":"Testing Methodology","text":""},{"location":"developer-guide/testing/#unit-tests","title":"Unit Tests","text":"<p>Unit tests validate individual components in isolation:</p> <pre><code>def test_field_mapper_basic():\n    mapper = FieldMapper()\n    mapper.add_mapping(\"source_field\", \"target_field\")\n\n    result = mapper.transform({\"source_field\": \"value\"})\n    assert result == {\"target_field\": \"value\"}\n</code></pre>"},{"location":"developer-guide/testing/#integration-tests","title":"Integration Tests","text":"<p>Integration tests validate complete workflows:</p> <pre><code>def test_github_to_gitlab_sync():\n    linker = ApiLinker()\n    linker.add_source(type=\"rest\", base_url=\"...\")\n    linker.add_target(type=\"rest\", base_url=\"...\")\n\n    result = linker.sync(dry_run=True)\n    assert result.success == True\n</code></pre>"},{"location":"developer-guide/testing/#mock-testing","title":"Mock Testing","text":"<p>External APIs are mocked for deterministic testing:</p> <pre><code>@pytest.fixture\ndef mock_api(requests_mock):\n    requests_mock.get(\n        \"https://api.example.com/users\",\n        json=[{\"id\": 1, \"name\": \"Alice\"}]\n    )\n</code></pre>"},{"location":"developer-guide/testing/#validation-framework","title":"Validation Framework","text":""},{"location":"developer-guide/testing/#schema-validation","title":"Schema Validation","text":"<p>APIs can enforce JSON Schema validation:</p> <pre><code>target:\n  endpoints:\n    create_item:\n      request_schema:\n        type: object\n        properties:\n          id: { type: string }\n        required: [id]\n\nvalidation:\n  strict_mode: true  # Fail if schema validation fails\n</code></pre>"},{"location":"developer-guide/testing/#data-integrity-tests","title":"Data Integrity Tests","text":"<p>Benchmarks include invariant preservation tests:</p> <ul> <li>ID preservation: Ensures unique identifiers are maintained</li> <li>Type consistency: Validates data type transformations</li> <li>Referential integrity: Checks foreign key relationships</li> </ul>"},{"location":"developer-guide/testing/#performance-testing","title":"Performance Testing","text":"<p>See Benchmarks for performance validation.</p>"},{"location":"developer-guide/testing/#reproducibility","title":"Reproducibility","text":"<p>All tests are designed for reproducibility:</p> <ul> <li>Fixed random seeds for deterministic behavior</li> <li>Mock servers for consistent external dependencies</li> <li>Version pinning in test environments</li> </ul>"},{"location":"developer-guide/testing/#contributing-tests","title":"Contributing Tests","text":"<p>When contributing code, please:</p> <ol> <li>Add unit tests for new features</li> <li>Ensure coverage remains &gt;80%</li> <li>Run full test suite before submitting PR</li> <li>Document test scenarios</li> </ol> <p>See Contributing Guide for details.</p>"},{"location":"examples/","title":"ApiLinker Examples","text":"<p>This section provides practical examples of ApiLinker usage for common integration scenarios.</p>"},{"location":"examples/#basic-examples","title":"Basic Examples","text":""},{"location":"examples/#simple-data-fetch","title":"Simple Data Fetch","text":"<p>Fetch data from a REST API:</p> <pre><code>from apilinker import ApiLinker\n\n# Initialize ApiLinker\nlinker = ApiLinker()\n\n# Configure source API\nlinker.add_source(\n    type=\"rest\",\n    base_url=\"https://jsonplaceholder.typicode.com\",  # Free testing API\n    endpoints={\n        \"get_posts\": {\n            \"path\": \"/posts\",\n            \"method\": \"GET\",\n            \"params\": {\"_limit\": 5}\n        }\n    }\n)\n\n# Fetch data\nposts = linker.fetch(\"get_posts\")\n\n# Process data\nfor post in posts:\n    print(f\"Post #{post['id']}: {post['title']}\")\n</code></pre>"},{"location":"examples/#basic-sync","title":"Basic Sync","text":"<p>Sync data between two endpoints:</p> <pre><code>from apilinker import ApiLinker\n\n# Initialize ApiLinker\nlinker = ApiLinker()\n\n# Configure source API\nlinker.add_source(\n    type=\"rest\",\n    base_url=\"https://source-api.example.com\",\n    auth={\n        \"type\": \"bearer\",\n        \"token\": \"${SOURCE_API_TOKEN}\"\n    },\n    endpoints={\n        \"get_products\": {\n            \"path\": \"/products\",\n            \"method\": \"GET\",\n            \"params\": {\"updated_since\": \"2023-01-01\"}\n        }\n    }\n)\n\n# Configure target API\nlinker.add_target(\n    type=\"rest\",\n    base_url=\"https://target-api.example.com\",\n    auth={\n        \"type\": \"api_key\",\n        \"header\": \"X-API-Key\",\n        \"key\": \"${TARGET_API_KEY}\"\n    },\n    endpoints={\n        \"create_product\": {\n            \"path\": \"/products\",\n            \"method\": \"POST\"\n        }\n    }\n)\n\n# Define mapping\nlinker.add_mapping(\n    source=\"get_products\",\n    target=\"create_product\",\n    fields=[\n        {\"source\": \"id\", \"target\": \"external_id\"},\n        {\"source\": \"name\", \"target\": \"title\"},\n        {\"source\": \"description\", \"target\": \"description\"},\n        {\"source\": \"price\", \"target\": \"price\"},\n        {\"source\": \"category\", \"target\": \"category\", \"transform\": \"lowercase\"}\n    ]\n)\n\n# Run the sync\nresult = linker.sync()\nprint(f\"Synced {result.count} products\")\n</code></pre>"},{"location":"examples/#intermediate-examples","title":"Intermediate Examples","text":""},{"location":"examples/#scheduled-sync-with-error-handling","title":"Scheduled Sync with Error Handling","text":"<p>Set up a scheduled sync with robust error handling:</p> <pre><code>from apilinker import ApiLinker\nimport logging\nimport time\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    filename='sync.log'\n)\nlogger = logging.getLogger('api_sync')\n\n# Initialize ApiLinker\nlinker = ApiLinker()\n\n# Configure APIs and mapping\n# ... (configuration as in previous examples)\n\n# Define error handler\ndef handle_sync_error(error, context):\n    logger.error(f\"Sync failed: {error}\")\n    logger.info(f\"Context: {context}\")\n\n    # Send notification (example)\n    send_notification(f\"API Sync failed: {error}\")\n\n    # Return True to retry, False to abort\n    return True\n\n# Add schedule with error handling\nlinker.add_schedule(\n    interval_minutes=60,\n    error_handler=handle_sync_error,\n    max_retries=3,\n    retry_delay_seconds=300\n)\n\n# Start scheduled sync\ntry:\n    logger.info(\"Starting scheduled sync\")\n    linker.start_scheduled_sync()\n\n    # Keep the script running\n    while True:\n        time.sleep(60)\nexcept KeyboardInterrupt:\n    logger.info(\"Scheduled sync stopped by user\")\n</code></pre>"},{"location":"examples/#pagination-handling","title":"Pagination Handling","text":"<p>Handle paginated API responses:</p> <pre><code>from apilinker import ApiLinker\n\n# Initialize ApiLinker\nlinker = ApiLinker()\n\n# Configure source API with pagination\nlinker.add_source(\n    type=\"rest\",\n    base_url=\"https://api.example.com\",\n    endpoints={\n        \"get_orders\": {\n            \"path\": \"/orders\",\n            \"method\": \"GET\",\n            \"pagination\": {\n                \"data_path\": \"data\",\n                \"next_page_path\": \"meta.next_page\",\n                \"page_param\": \"page\",\n                \"limit_param\": \"limit\",\n                \"limit\": 100\n            }\n        }\n    }\n)\n\n# Fetch all pages automatically\nall_orders = linker.fetch(\"get_orders\")\nprint(f\"Retrieved {len(all_orders)} orders across multiple pages\")\n\n# Configure target and mapping\n# ...\n\n# Sync with automatic pagination\nresult = linker.sync()\nprint(f\"Synced {result.count} orders\")\n</code></pre>"},{"location":"examples/#advanced-examples","title":"Advanced Examples","text":""},{"location":"examples/#custom-data-transformations","title":"Custom Data Transformations","text":"<p>Apply complex transformations to your data:</p> <pre><code>from apilinker import ApiLinker\nimport datetime\n\n# Initialize ApiLinker\nlinker = ApiLinker()\n\n# Configure APIs\n# ...\n\n# Define custom transformers\ndef format_date(value, **kwargs):\n    \"\"\"Convert date string to desired format.\"\"\"\n    if not value:\n        return None\n\n    format_in = kwargs.get(\"format_in\", \"%Y-%m-%d\")\n    format_out = kwargs.get(\"format_out\", \"%d/%m/%Y\")\n\n    try:\n        date_obj = datetime.datetime.strptime(value, format_in)\n        return date_obj.strftime(format_out)\n    except ValueError:\n        return value\n\ndef combine_fields(value, **kwargs):\n    \"\"\"Combine multiple fields into one.\"\"\"\n    # The current field value is ignored\n    # Instead we use the full source record\n    source_record = kwargs.get(\"_source_record\", {})\n\n    fields = kwargs.get(\"fields\", [])\n    separator = kwargs.get(\"separator\", \" \")\n\n    values = []\n    for field in fields:\n        field_value = source_record.get(field)\n        if field_value:\n            values.append(str(field_value))\n\n    return separator.join(values)\n\n# Register transformers\nlinker.mapper.register_transformer(\"format_date\", format_date)\nlinker.mapper.register_transformer(\"combine_fields\", combine_fields)\n\n# Use transformers in mapping\nlinker.add_mapping(\n    source=\"get_users\",\n    target=\"create_contacts\",\n    fields=[\n        # Format date from YYYY-MM-DD to DD/MM/YYYY\n        {\n            \"source\": \"birth_date\",\n            \"target\": \"dateOfBirth\",\n            \"transform\": \"format_date\",\n            \"format_in\": \"%Y-%m-%d\",\n            \"format_out\": \"%d/%m/%Y\"\n        },\n        # Combine first and last name into full name\n        {\n            \"target\": \"fullName\",\n            \"transform\": \"combine_fields\",\n            \"fields\": [\"first_name\", \"last_name\"],\n            \"separator\": \" \"\n        }\n    ]\n)\n</code></pre>"},{"location":"examples/#conditional-mapping","title":"Conditional Mapping","text":"<p>Apply mappings based on conditions:</p> <pre><code>from apilinker import ApiLinker\n\n# Initialize ApiLinker\nlinker = ApiLinker()\n\n# Configure APIs\n# ...\n\n# Add mapping with conditions\nlinker.add_mapping(\n    source=\"get_orders\",\n    target=\"create_order\",\n    fields=[\n        {\"source\": \"id\", \"target\": \"external_id\"},\n        {\"source\": \"customer.name\", \"target\": \"customer_name\"},\n\n        # Only map the shipping address if it exists\n        {\n            \"source\": \"shipping_address\",\n            \"target\": \"shipping_details\",\n            \"condition\": {\n                \"field\": \"shipping_address\",\n                \"operator\": \"exists\"\n            }\n        },\n\n        # Apply different mapping based on status\n        {\n            \"source\": \"status\",\n            \"target\": \"status\",\n            \"transform\": \"map_status\",\n            \"condition\": {\n                \"field\": \"status\",\n                \"operator\": \"in\",\n                \"value\": [\"pending\", \"processing\"]\n            }\n        },\n\n        # Apply a default for completed orders\n        {\n            \"target\": \"status\",\n            \"value\": \"fulfilled\",\n            \"condition\": {\n                \"field\": \"status\",\n                \"operator\": \"equals\",\n                \"value\": \"completed\"\n            }\n        }\n    ]\n)\n\n# Register status mapper\ndef map_status(value, **kwargs):\n    status_map = {\n        \"pending\": \"new\",\n        \"processing\": \"in_progress\"\n    }\n    return status_map.get(value, value)\n\nlinker.mapper.register_transformer(\"map_status\", map_status)\n</code></pre>"},{"location":"examples/#real-world-use-cases","title":"Real-world Use Cases","text":"<p>For more detailed real-world examples, see:</p> <ul> <li>CRM to Marketing Platform</li> <li>E-commerce Inventory Sync</li> <li>Weather Data Collection</li> <li>GitHub to GitLab Migration</li> </ul> <p>Each example includes full code, configuration files, and explanations.</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#standard-installation","title":"Standard Installation","text":"<p>Install ApiLinker using pip:</p> <pre><code>pip install apilinker\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>To install from source for contributing:</p> <pre><code>git clone https://github.com/kkartas/apilinker.git\ncd apilinker\npip install -e \".[dev,docs]\"\n</code></pre>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<pre><code>python -c \"import apilinker; print(apilinker.__version__)\"\n</code></pre>"},{"location":"getting-started/interactive-tutorial/","title":"Interactive Tutorial","text":"<p>Try ApiLinker in your browser with our interactive Jupyter notebook!</p> <p></p>"},{"location":"getting-started/interactive-tutorial/#what-youll-learn","title":"What You'll Learn","text":"<p>This hands-on tutorial demonstrates:</p> <ol> <li>Basic API Integration: Connect to NCBI/PubMed and fetch research papers</li> <li>Data Transformation: Map fields and apply transformations</li> <li>Research Workflows: Cross-platform literature search</li> <li>Visualization: Plot citation networks and author collaborations</li> </ol>"},{"location":"getting-started/interactive-tutorial/#running-locally","title":"Running Locally","text":"<p>If you prefer to run the notebook on your local machine:</p> <pre><code>git clone https://github.com/kkartas/APILinker.git\ncd APILinker\npip install -e \".[dev]\"\npip install jupyter matplotlib pandas\njupyter notebook examples/ApiLinker_Research_Tutorial.ipynb\n</code></pre>"},{"location":"getting-started/interactive-tutorial/#notebook-contents","title":"Notebook Contents","text":""},{"location":"getting-started/interactive-tutorial/#section-1-installation-setup","title":"Section 1: Installation &amp; Setup","text":"<p>Learn how to install ApiLinker and verify the installation.</p>"},{"location":"getting-started/interactive-tutorial/#section-2-basic-connectivity","title":"Section 2: Basic Connectivity","text":"<p>Connect to the PubMed API and fetch your first research paper.</p> <pre><code>from apilinker.connectors.scientific import NCBIConnector\n\nncbi = NCBIConnector(email=\"your-email@university.edu\")\nresults = ncbi.search_pubmed(\"CRISPR gene editing\", max_results=10)\n</code></pre>"},{"location":"getting-started/interactive-tutorial/#section-3-cross-platform-research","title":"Section 3: Cross-Platform Research","text":"<p>Combine data from multiple research APIs (NCBI, arXiv, Semantic Scholar).</p>"},{"location":"getting-started/interactive-tutorial/#section-4-data-visualization","title":"Section 4: Data Visualization","text":"<p>Visualize research trends, citation networks, and co-authorship graphs.</p>"},{"location":"getting-started/interactive-tutorial/#section-5-advanced-workflows","title":"Section 5: Advanced Workflows","text":"<p>Build reproducible research pipelines with scheduling and error handling.</p>"},{"location":"getting-started/interactive-tutorial/#interactive-features","title":"Interactive Features","text":"<ul> <li>Live Code Execution: Run and modify code directly in the browser</li> <li>Instant Feedback: See results immediately without installation</li> <li>Visualization: Generate charts and graphs</li> <li>No Setup Required: Powered by Binder for zero-config execution</li> </ul>"},{"location":"getting-started/interactive-tutorial/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/interactive-tutorial/#binder-not-loading","title":"Binder Not Loading?","text":"<p>If Binder takes too long to build, try:</p> <ol> <li>Refresh the page and wait (first build can take 5-10 minutes)</li> <li>Run locally using the instructions above</li> <li>View the notebook on GitHub: ApiLinker_Research_Tutorial.ipynb</li> </ol>"},{"location":"getting-started/interactive-tutorial/#feedback","title":"Feedback","text":"<p>Found issues with the tutorial? Open an issue on GitHub.</p>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":""},{"location":"getting-started/quickstart/#cli-usage","title":"CLI Usage","text":"<ol> <li>Create a <code>config.yaml</code> file:</li> </ol> <pre><code>source:\n  type: rest\n  base_url: https://api.example.com\n  endpoints:\n    list: { path: /items, method: GET }\ntarget:\n  type: rest\n  base_url: https://api.dest.com\n  endpoints:\n    create: { path: /items, method: POST }\nmapping:\n  - source: list\n    target: create\n    fields:\n      - { source: id, target: external_id }\n</code></pre> <ol> <li>Run the sync:</li> </ol> <pre><code>apilinker sync --config config.yaml\n</code></pre>"},{"location":"getting-started/quickstart/#python-usage","title":"Python Usage","text":"<pre><code>from apilinker import ApiLinker\n\nlinker = ApiLinker()\nlinker.add_source(type=\"rest\", base_url=\"...\")\nlinker.add_target(type=\"rest\", base_url=\"...\")\nlinker.sync()\n</code></pre>"},{"location":"getting-started/tutorial/","title":"Getting Started with ApiLinker","text":"<p>This guide is designed for Python beginners who want to learn how to connect APIs using ApiLinker.</p>"},{"location":"getting-started/tutorial/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or newer installed</li> <li>Basic understanding of Python (variables, functions)</li> <li>A text editor or IDE (like VS Code, PyCharm, or even Notepad)</li> <li>Internet connection</li> </ul>"},{"location":"getting-started/tutorial/#installation","title":"Installation","text":"<p>First, let's install ApiLinker:</p> <pre><code>pip install apilinker\n</code></pre>"},{"location":"getting-started/tutorial/#basic-concepts","title":"Basic Concepts","text":"<p>Before diving in, here are the key concepts in ApiLinker:</p> <ol> <li>Source - The API where you get data from</li> <li>Target - The API where you send data to</li> <li>Endpoint - A specific operation in an API (like \"get users\" or \"create post\")</li> <li>Mapping - Rules for how data moves from source to target</li> <li>Transformer - A function that changes data format during transfer</li> </ol>"},{"location":"getting-started/tutorial/#your-first-apilinker-script","title":"Your First ApiLinker Script","text":"<p>Let's build a simple script that gets data from the free JSONPlaceholder API:</p> <pre><code># Step 1: Import the library\nfrom apilinker import ApiLinker\n\n# Step 2: Create an ApiLinker instance\nlinker = ApiLinker()\n\n# Step 3: Configure a source API\nlinker.add_source(\n    type=\"rest\",\n    base_url=\"https://jsonplaceholder.typicode.com\",\n    endpoints={\n        \"get_posts\": {\n            \"path\": \"/posts\",\n            \"method\": \"GET\",\n            \"params\": {\"_limit\": 5}  # Only get 5 posts\n        }\n    }\n)\n\n# Step 4: Fetch data from the source API\nposts = linker.fetch(\"get_posts\")\n\n# Step 5: Print the results\nprint(\"Posts retrieved:\")\nfor post in posts:\n    print(f\"- {post['title']}\")\n</code></pre> <p>Save this as <code>first_example.py</code> and run it:</p> <pre><code>python first_example.py\n</code></pre> <p>You should see a list of post titles printed to your console!</p>"},{"location":"getting-started/tutorial/#connecting-two-apis","title":"Connecting Two APIs","text":"<p>Now let's connect two APIs to move data from one to another:</p> <pre><code>from apilinker import ApiLinker\n\n# Create ApiLinker instance\nlinker = ApiLinker()\n\n# Configure source API (JSONPlaceholder)\nlinker.add_source(\n    type=\"rest\",\n    base_url=\"https://jsonplaceholder.typicode.com\",\n    endpoints={\n        \"get_posts\": {\n            \"path\": \"/posts\",\n            \"method\": \"GET\",\n            \"params\": {\"_limit\": 3}  # Get 3 posts\n        }\n    }\n)\n\n# Configure target API (also JSONPlaceholder in this demo)\nlinker.add_target(\n    type=\"rest\",\n    base_url=\"https://jsonplaceholder.typicode.com\",\n    endpoints={\n        \"create_comment\": {\n            \"path\": \"/comments\",\n            \"method\": \"POST\"\n        }\n    }\n)\n\n# Create a mapping between source and target\nlinker.add_mapping(\n    source=\"get_posts\",\n    target=\"create_comment\",\n    fields=[\n        # Map the post id to the comment's postId\n        {\"source\": \"id\", \"target\": \"postId\"},\n\n        # Create a fixed name\n        {\"target\": \"name\", \"value\": \"API Connector Test\"},\n\n        # Map the post title to the comment's email (just for demo purposes)\n        {\"source\": \"title\", \"target\": \"email\"},\n\n        # Map the post body to the comment's body\n        {\"source\": \"body\", \"target\": \"body\"}\n    ]\n)\n\n# Run the sync\nresult = linker.sync(dry_run=True)  # Use dry_run=True to prevent actual API calls\n\nprint(f\"Synced {result.count} posts to comments\")\nprint(\"Preview of the first transformed item:\")\nprint(result.preview[0] if result.preview else \"No preview available\")\n</code></pre> <p>Save this as <code>connecting_apis.py</code> and run it:</p> <pre><code>python connecting_apis.py\n</code></pre>"},{"location":"getting-started/tutorial/#understanding-results","title":"Understanding Results","text":"<p>When you run the code above, you'll see:</p> <ol> <li>How many records were synced</li> <li>A preview of what the data looks like after transformation</li> </ol>"},{"location":"getting-started/tutorial/#next-steps","title":"Next Steps","text":"<p>Now that you've created your first ApiLinker script, you can:</p> <ol> <li>Connect to real APIs - Replace the example APIs with ones you want to use</li> <li>Add authentication - Learn about auth options in the documentation</li> <li>Create transformers - Write functions to format data between systems</li> <li>Set up scheduling - Make your sync run on a regular schedule</li> </ol>"},{"location":"getting-started/tutorial/#common-questions-for-beginners","title":"Common Questions for Beginners","text":""},{"location":"getting-started/tutorial/#how-do-i-find-api-endpoints","title":"How do I find API endpoints?","text":"<p>Check the API documentation for the service you're using. They should list available endpoints, required parameters, and authentication methods.</p>"},{"location":"getting-started/tutorial/#what-if-my-api-requires-authentication","title":"What if my API requires authentication?","text":"<p>Add it to your source or target configuration:</p> <pre><code>linker.add_source(\n    type=\"rest\",\n    base_url=\"https://api.example.com\",\n    auth={\n        \"type\": \"api_key\",\n        \"header\": \"X-API-Key\",\n        \"key\": \"your_api_key_here\"  # Better to use environment variables!\n    },\n    endpoints={\n        # Your endpoints here\n    }\n)\n</code></pre>"},{"location":"getting-started/tutorial/#how-do-i-debug-issues","title":"How do I debug issues?","text":"<p>Enable debugging to see what's happening:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n\nlinker = ApiLinker(debug=True)\n# Rest of your code...\n</code></pre>"},{"location":"getting-started/tutorial/#how-do-i-handle-pagination","title":"How do I handle pagination?","text":"<p>ApiLinker can handle pagination automatically:</p> <pre><code>linker.add_source(\n    # Other configuration...\n    endpoints={\n        \"get_items\": {\n            \"path\": \"/items\",\n            \"method\": \"GET\",\n            \"pagination\": {\n                \"data_path\": \"data\",\n                \"next_page_path\": \"meta.next_page\",\n                \"page_param\": \"page\"\n            }\n        }\n    }\n)\n</code></pre>"},{"location":"getting-started/tutorial/#getting-help","title":"Getting Help","text":"<p>If you get stuck, here are resources to help:</p> <ul> <li>Documentation: Visit https://apilinker.readthedocs.io</li> <li>Example Code: Look at the examples in the GitHub repository</li> <li>Issues: If you find a bug, report it on GitHub Issues</li> </ul> <p>Remember, everyone starts somewhere! API integration gets easier with practice.</p>"},{"location":"plugins/","title":"Extending with Plugins","text":"<p>ApiLinker is designed to be easily extended through plugins. This guide explains how to create, register, and use custom plugins.</p>"},{"location":"plugins/#plugin-types","title":"Plugin Types","text":"<p>ApiLinker supports three main types of plugins:</p> <ol> <li>Transformer Plugins: Convert data between formats, perform calculations, or modify values</li> <li>Connector Plugins: Handle communication with different API types and protocols</li> <li>Auth Plugins: Implement authentication mechanisms for various services</li> </ol>"},{"location":"plugins/#creating-custom-plugins","title":"Creating Custom Plugins","text":""},{"location":"plugins/#transformer-plugins","title":"Transformer Plugins","text":"<p>Transformer plugins modify data during transfer between source and target. To create a transformer plugin:</p> <pre><code>from apilinker.core.plugins import TransformerPlugin\n\nclass MyTransformer(TransformerPlugin):\n    \"\"\"Description of what your transformer does.\"\"\"\n\n    plugin_name = \"my_transformer\"  # Name used to reference the plugin\n\n    def transform(self, value, **kwargs):\n        \"\"\"\n        Transform the input value.\n\n        Args:\n            value: The input value to transform\n            **kwargs: Additional parameters from the mapping configuration\n\n        Returns:\n            Transformed value\n        \"\"\"\n        # Your transformation logic here\n        transformed_value = do_something_with(value)\n        return transformed_value\n\n    def validate_input(self, value):\n        \"\"\"\n        Optional method to validate input before transformation.\n\n        Args:\n            value: The input value to validate\n\n        Returns:\n            bool: True if valid, False otherwise\n        \"\"\"\n        return isinstance(value, str)  # Example validation\n</code></pre>"},{"location":"plugins/#connector-plugins","title":"Connector Plugins","text":"<p>Connector plugins handle communication with different API types:</p> <pre><code>from apilinker.core.plugins import ConnectorPlugin\n\nclass MyConnector(ConnectorPlugin):\n    \"\"\"Custom connector for a specific API type.\"\"\"\n\n    plugin_name = \"my_connector\"\n\n    def connect(self, **kwargs):\n        \"\"\"\n        Establish a connection to the API.\n\n        Args:\n            **kwargs: Connection parameters\n\n        Returns:\n            Connection object or context\n        \"\"\"\n        # Create and return a connection object\n        return {\"connection_id\": \"12345\", \"base_url\": kwargs.get(\"base_url\")}\n\n    def fetch(self, connection, endpoint, **kwargs):\n        \"\"\"\n        Fetch data from the API.\n\n        Args:\n            connection: Connection object from connect()\n            endpoint: Endpoint path\n            **kwargs: Additional parameters\n\n        Returns:\n            API response data\n        \"\"\"\n        # Fetch data using the connection\n        return fetch_data(connection, endpoint, **kwargs)\n\n    def send(self, connection, endpoint, data, **kwargs):\n        \"\"\"\n        Send data to the API.\n\n        Args:\n            connection: Connection object from connect()\n            endpoint: Endpoint path\n            data: Data to send\n            **kwargs: Additional parameters\n\n        Returns:\n            API response data\n        \"\"\"\n        # Send data using the connection\n        return send_data(connection, endpoint, data, **kwargs)\n</code></pre>"},{"location":"plugins/#auth-plugins","title":"Auth Plugins","text":"<p>Auth plugins handle different authentication methods:</p> <pre><code>from apilinker.core.plugins import AuthPlugin\n\nclass MyAuth(AuthPlugin):\n    \"\"\"Custom authentication plugin.\"\"\"\n\n    plugin_name = \"my_auth\"\n\n    def authenticate(self, **kwargs):\n        \"\"\"\n        Authenticate with the API.\n\n        Args:\n            **kwargs: Authentication parameters\n\n        Returns:\n            dict: Authentication details including headers, tokens, etc.\n        \"\"\"\n        # Perform authentication\n        token = get_token(kwargs.get(\"client_id\"), kwargs.get(\"client_secret\"))\n\n        # Return authentication details\n        return {\n            \"type\": \"bearer\",\n            \"headers\": {\"Authorization\": f\"Bearer {token}\"},\n            \"token\": token\n        }\n\n    def validate_credentials(self, credentials):\n        \"\"\"\n        Validate authentication credentials.\n\n        Args:\n            credentials: Authentication credentials\n\n        Returns:\n            bool: True if valid, False otherwise\n        \"\"\"\n        return isinstance(credentials, dict) and \"token\" in credentials\n</code></pre>"},{"location":"plugins/#registering-plugins","title":"Registering Plugins","text":""},{"location":"plugins/#method-1-manual-registration","title":"Method 1: Manual Registration","text":"<p>Register your plugin directly with the plugin manager:</p> <pre><code>from apilinker import ApiLinker\nfrom my_plugins import MyTransformer\n\n# Initialize ApiLinker\nlinker = ApiLinker()\n\n# Register the plugin\nlinker.plugin_manager.register_plugin(MyTransformer)\n</code></pre>"},{"location":"plugins/#method-2-auto-discovery","title":"Method 2: Auto-Discovery","text":"<p>Place your plugin modules in one of these locations for automatic discovery:</p> <ol> <li>Built-in plugins: <code>apilinker/plugins/builtin/</code></li> <li>User plugins: <code>~/.apilinker/plugins/</code></li> <li>Custom directory specified in configuration</li> </ol> <p>The plugin module should be a <code>.py</code> file that defines your plugin class.</p>"},{"location":"plugins/#using-custom-plugins","title":"Using Custom Plugins","text":""},{"location":"plugins/#transformer-plugins_1","title":"Transformer Plugins","text":"<p>Use your transformer in mapping configurations:</p> <pre><code>linker.add_mapping(\n    source=\"get_users\",\n    target=\"create_contacts\",\n    fields=[\n        {\"source\": \"id\", \"target\": \"external_id\"},\n        {\"source\": \"phone\", \"target\": \"phoneNumber\", \"transform\": \"my_transformer\"}\n    ]\n)\n</code></pre> <p>Or use it programmatically:</p> <pre><code>transformer = linker.plugin_manager.get_transformer(\"my_transformer\")\ntransformed_value = transformer(\"original_value\", extra_param=\"value\")\n</code></pre>"},{"location":"plugins/#connector-plugins_1","title":"Connector Plugins","text":"<p>Specify your connector type in API configurations:</p> <pre><code>linker.add_source(\n    type=\"my_connector\",  # Your connector plugin name\n    base_url=\"https://api.example.com\",\n    # Other configuration...\n)\n</code></pre>"},{"location":"plugins/#auth-plugins_1","title":"Auth Plugins","text":"<p>Specify your auth type in API configurations:</p> <pre><code>linker.add_source(\n    type=\"rest\",\n    base_url=\"https://api.example.com\",\n    auth={\n        \"type\": \"my_auth\",  # Your auth plugin name\n        \"client_id\": \"${CLIENT_ID}\",\n        \"client_secret\": \"${CLIENT_SECRET}\"\n    }\n)\n</code></pre>"},{"location":"plugins/#best-practices","title":"Best Practices","text":"<ol> <li>Error Handling: Include robust error handling in your plugins</li> <li>Documentation: Add detailed docstrings to your plugin classes and methods</li> <li>Input Validation: Always validate inputs to prevent errors</li> <li>Version Compatibility: Specify the ApiLinker versions your plugin is compatible with</li> <li>Testing: Write unit tests for your plugins</li> </ol>"},{"location":"plugins/#examples","title":"Examples","text":"<p>For more detailed examples, see the example plugins documentation.</p>"},{"location":"reference/api/","title":"API Reference","text":"<p>This page provides comprehensive API documentation for all ApiLinker classes and modules.</p>"},{"location":"reference/api/#core-classes","title":"Core Classes","text":""},{"location":"reference/api/#apilinker","title":"ApiLinker","text":"<p>Main orchestrator class for API integration workflows.</p>"},{"location":"reference/api/#apilinker.ApiLinker","title":"<code>apilinker.ApiLinker</code>","text":"<p>Main class for connecting, mapping and transferring data between APIs.</p> <p>This class orchestrates the entire process of: 1. Connecting to source and target APIs 2. Fetching data from the source 3. Mapping fields according to configuration 4. Transforming data as needed 5. Sending data to the target 6. Scheduling recurring operations</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>Optional[str]</code> <p>Path to YAML/JSON configuration file</p> <code>None</code> <code>source_config</code> <code>Optional[Dict[str, Any]]</code> <p>Direct source configuration dictionary</p> <code>None</code> <code>target_config</code> <code>Optional[Dict[str, Any]]</code> <p>Direct target configuration dictionary</p> <code>None</code> <code>mapping_config</code> <code>Optional[Dict[str, Any]]</code> <p>Direct mapping configuration dictionary</p> <code>None</code> <code>schedule_config</code> <code>Optional[Dict[str, Any]]</code> <p>Direct scheduling configuration dictionary</p> <code>None</code> <code>log_level</code> <code>str</code> <p>Logging level (DEBUG, INFO, WARNING, ERROR)</p> <code>'INFO'</code> <code>log_file</code> <code>Optional[str]</code> <p>Path to log file</p> <code>None</code> Source code in <code>apilinker/api_linker.py</code> <pre><code>class ApiLinker:\n    \"\"\"\n    Main class for connecting, mapping and transferring data between APIs.\n\n    This class orchestrates the entire process of:\n    1. Connecting to source and target APIs\n    2. Fetching data from the source\n    3. Mapping fields according to configuration\n    4. Transforming data as needed\n    5. Sending data to the target\n    6. Scheduling recurring operations\n\n    Args:\n        config_path: Path to YAML/JSON configuration file\n        source_config: Direct source configuration dictionary\n        target_config: Direct target configuration dictionary\n        mapping_config: Direct mapping configuration dictionary\n        schedule_config: Direct scheduling configuration dictionary\n        log_level: Logging level (DEBUG, INFO, WARNING, ERROR)\n        log_file: Path to log file\n    \"\"\"\n\n    def __init__(\n        self,\n        config_path: Optional[str] = None,\n        source_config: Optional[Dict[str, Any]] = None,\n        target_config: Optional[Dict[str, Any]] = None,\n        mapping_config: Optional[Dict[str, Any]] = None,\n        schedule_config: Optional[Dict[str, Any]] = None,\n        error_handling_config: Optional[Dict[str, Any]] = None,\n        security_config: Optional[Dict[str, Any]] = None,\n        validation_config: Optional[Dict[str, Any]] = None,\n        observability_config: Optional[Dict[str, Any]] = None,\n        secret_manager_config: Optional[Dict[str, Any]] = None,\n        log_level: str = \"INFO\",\n        log_file: Optional[str] = None,\n    ) -&gt; None:\n        # Initialize logger\n        self.logger = setup_logger(log_level, log_file)\n        self.logger.info(\"Initializing ApiLinker\")\n\n        # Initialize components\n        self.source: Optional[ApiConnector] = None\n        self.target: Optional[ApiConnector] = None\n        self.mapper = FieldMapper()\n        self.scheduler = Scheduler()\n        self.validation_config = validation_config or {\"strict_mode\": False}\n        self.provenance = ProvenanceRecorder()\n        self.deduplicator = InMemoryDeduplicator()\n        self.state_store: Optional[StateStore] = None\n\n        # Initialize observability\n        self.telemetry = self._initialize_observability(observability_config)\n\n        # Initialize secret management\n        self.secret_manager = self._initialize_secret_manager(secret_manager_config)\n\n        # Initialize security system\n        self.security_manager = self._initialize_security(security_config)\n\n        # Initialize auth manager and integrate with security\n        self.auth_manager = AuthManager()\n        integrate_security_with_auth_manager(self.security_manager, self.auth_manager)\n\n        # Initialize error handling system\n        self.dlq, self.error_recovery_manager, self.error_analytics = (\n            create_error_handler()\n        )\n\n        # Load configuration if provided\n        if config_path:\n            self.load_config(config_path)\n        else:\n            # Set up direct configurations if provided\n            if source_config:\n                self.add_source(**source_config)\n            if target_config:\n                self.add_target(**target_config)\n            if mapping_config:\n                self.add_mapping(**mapping_config)\n            if schedule_config:\n                self.add_schedule(**schedule_config)\n            if error_handling_config:\n                self._configure_error_handling(error_handling_config)\n            if security_config:\n                self._configure_security(security_config)\n\n    def load_config(self, config_path: str) -&gt; None:\n        \"\"\"\n        Load configuration from a YAML or JSON file.\n\n        Args:\n            config_path: Path to the configuration file\n        \"\"\"\n        self.logger.info(f\"Loading configuration from {config_path}\")\n\n        # Resolve environment variables in config path\n        config_path = os.path.expandvars(config_path)\n\n        if not os.path.exists(config_path):\n            raise FileNotFoundError(f\"Configuration file not found: {config_path}\")\n\n        with open(config_path, \"r\") as f:\n            config = yaml.safe_load(f)\n\n        # Set up components from config\n        if \"source\" in config:\n            self.add_source(**config[\"source\"])\n\n        if \"target\" in config:\n            self.add_target(**config[\"target\"])\n\n        if \"mapping\" in config:\n            if isinstance(config[\"mapping\"], list):\n                for mapping in config[\"mapping\"]:\n                    self.add_mapping(**mapping)\n            else:\n                self.add_mapping(**config[\"mapping\"])\n\n        if \"schedule\" in config:\n            self.add_schedule(**config[\"schedule\"])\n\n        # Configure error handling if specified\n        if \"error_handling\" in config:\n            self._configure_error_handling(config[\"error_handling\"])\n\n        # Configure security if specified\n        if \"security\" in config:\n            self._configure_security(config[\"security\"])\n\n        # Configure secret management if specified\n        if \"secrets\" in config:\n            self.secret_manager = self._initialize_secret_manager(config[\"secrets\"])\n\n        # Validation configuration\n        if \"validation\" in config:\n            self.validation_config = config[\"validation\"]\n\n        if \"logging\" in config:\n            log_config = config[\"logging\"]\n            log_level = log_config.get(\"level\", \"INFO\")\n            log_file = log_config.get(\"file\")\n            self.logger = setup_logger(log_level, log_file)\n\n        # Provenance options\n        if \"provenance\" in config:\n            prov_cfg = config[\"provenance\"]\n            output_dir = prov_cfg.get(\"output_dir\")\n            jsonl_log = prov_cfg.get(\"jsonl_log\")\n            self.provenance = ProvenanceRecorder(\n                output_dir=output_dir, jsonl_log_path=jsonl_log\n            )\n\n        # Idempotency\n        if \"idempotency\" in config:\n            self.idempotency_config = config[\"idempotency\"]\n        else:\n            self.idempotency_config = {\"enabled\": False, \"salt\": \"\"}\n\n        # State store\n        if \"state\" in config:\n            st_cfg = config[\"state\"]\n            st_type = st_cfg.get(\"type\", \"file\")\n            if st_type == \"file\":\n                path = st_cfg.get(\"path\", \".apilinker/state.json\")\n                default_last_sync = st_cfg.get(\"default_last_sync\")\n                self.state_store = FileStateStore(\n                    path, default_last_sync=default_last_sync\n                )\n            elif st_type == \"sqlite\":\n                path = st_cfg.get(\"path\", \".apilinker/state.db\")\n                default_last_sync = st_cfg.get(\"default_last_sync\")\n                self.state_store = SQLiteStateStore(\n                    path, default_last_sync=default_last_sync\n                )\n\n    def add_source(\n        self,\n        type: str,\n        base_url: str,\n        auth: Optional[Dict[str, Any]] = None,\n        endpoints: Optional[Dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Add a source API connector.\n\n        Args:\n            type: Type of API connector (rest, graphql, etc.)\n            base_url: Base URL of the API\n            auth: Authentication configuration\n            endpoints: Configured endpoints\n            **kwargs: Additional configuration parameters\n        \"\"\"\n        self.logger.info(f\"Adding source connector: {type} for {base_url}\")\n\n        # Resolve secrets in authentication configuration\n        if auth:\n            auth = self._resolve_auth_secrets(auth)\n            auth_config = self.auth_manager.configure_auth(auth)\n        else:\n            auth_config = None\n\n        # Create source connector\n        self.source = ApiConnector(\n            connector_type=type,\n            base_url=base_url,\n            auth_config=auth_config,\n            endpoints=endpoints or {},\n            **kwargs,\n        )\n\n    def add_target(\n        self,\n        type: str,\n        base_url: str,\n        auth: Optional[Dict[str, Any]] = None,\n        endpoints: Optional[Dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Add a target API connector.\n\n        Args:\n            type: Type of API connector (rest, graphql, etc.)\n            base_url: Base URL of the API\n            auth: Authentication configuration\n            endpoints: Configured endpoints\n            **kwargs: Additional configuration parameters\n        \"\"\"\n        self.logger.info(f\"Adding target connector: {type} for {base_url}\")\n\n        # Resolve secrets in authentication configuration\n        if auth:\n            auth = self._resolve_auth_secrets(auth)\n            auth_config = self.auth_manager.configure_auth(auth)\n        else:\n            auth_config = None\n\n        # Create target connector\n        self.target = ApiConnector(\n            connector_type=type,\n            base_url=base_url,\n            auth_config=auth_config,\n            endpoints=endpoints or {},\n            **kwargs,\n        )\n\n    def add_mapping(\n        self, source: str, target: str, fields: List[Dict[str, Any]]\n    ) -&gt; None:\n        \"\"\"\n        Add a field mapping between source and target endpoints.\n\n        Args:\n            source: Source endpoint name\n            target: Target endpoint name\n            fields: List of field mappings\n        \"\"\"\n        self.logger.info(\n            f\"Adding mapping from {source} to {target} with {len(fields)} fields\"\n        )\n        self.mapper.add_mapping(source, target, fields)\n\n    def add_schedule(self, type: str, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Add a schedule for recurring syncs.\n\n        Args:\n            type: Type of schedule (interval, cron)\n            **kwargs: Schedule-specific parameters\n        \"\"\"\n        self.logger.info(f\"Adding schedule: {type}\")\n        self.scheduler.add_schedule(type, **kwargs)\n\n    def _initialize_security(\n        self, config: Optional[Dict[str, Any]] = None\n    ) -&gt; SecurityManager:\n        \"\"\"\n        Initialize the security system based on provided configuration.\n\n        Args:\n            config: Security configuration dictionary\n\n        Returns:\n            SecurityManager instance\n        \"\"\"\n        if not config:\n            config = {}\n\n        # Extract security configuration\n        master_password = config.get(\"master_password\")\n        storage_path = config.get(\"credential_storage_path\")\n        encryption_level = config.get(\"encryption_level\", \"none\")\n        encryption_key = config.get(\"encryption_key\")\n        enable_access_control = config.get(\"enable_access_control\", False)\n\n        # Initialize security manager\n        security_manager = SecurityManager(\n            master_password=master_password,\n            storage_path=storage_path,\n            encryption_level=encryption_level,\n            encryption_key=encryption_key,\n            enable_access_control=enable_access_control,\n        )\n\n        # Set up initial users if access control is enabled\n        if enable_access_control and \"users\" in config:\n            for user_config in config[\"users\"]:\n                username = user_config.get(\"username\")\n                role = user_config.get(\"role\", \"viewer\")\n                api_key = user_config.get(\"api_key\")\n\n                if username:\n                    security_manager.add_user(username, role, api_key)\n\n        return security_manager\n\n    def _initialize_observability(\n        self, config: Optional[Dict[str, Any]] = None\n    ) -&gt; TelemetryManager:\n        \"\"\"\n        Initialize the observability system based on provided configuration.\n\n        Args:\n            config: Observability configuration dictionary\n\n        Returns:\n            TelemetryManager instance\n        \"\"\"\n        if not config:\n            config = {}\n\n        # Create observability configuration\n        obs_config = ObservabilityConfig(\n            enabled=config.get(\"enabled\", True),\n            service_name=config.get(\"service_name\", \"apilinker\"),\n            enable_tracing=config.get(\"enable_tracing\", True),\n            enable_metrics=config.get(\"enable_metrics\", True),\n            export_to_console=config.get(\"export_to_console\", False),\n            export_to_prometheus=config.get(\"export_to_prometheus\", False),\n            prometheus_host=config.get(\"prometheus_host\", \"0.0.0.0\"),\n            prometheus_port=config.get(\"prometheus_port\", 9090),\n        )\n\n        return TelemetryManager(obs_config)\n\n    def _initialize_secret_manager(\n        self, config: Optional[Dict[str, Any]] = None\n    ) -&gt; Optional[SecretManager]:\n        \"\"\"\n        Initialize the secret management system based on provided configuration.\n\n        Args:\n            config: Secret manager configuration dictionary\n\n        Returns:\n            SecretManager instance or None if not configured\n        \"\"\"\n        if not config:\n            return None\n\n        try:\n            # Create secret manager configuration\n            from apilinker.core.secrets import SecretProvider, RotationStrategy\n\n            provider_str = config.get(\"provider\", \"env\")\n            provider = SecretProvider(provider_str)\n\n            rotation_str = config.get(\"rotation_strategy\", \"manual\")\n            rotation_strategy = RotationStrategy(rotation_str)\n\n            secret_config = SecretManagerConfig(\n                provider=provider,\n                vault_config=config.get(\"vault\"),\n                aws_config=config.get(\"aws\"),\n                azure_config=config.get(\"azure\"),\n                gcp_config=config.get(\"gcp\"),\n                rotation_strategy=rotation_strategy,\n                rotation_interval_days=config.get(\"rotation_interval_days\", 90),\n                cache_ttl_seconds=config.get(\"cache_ttl_seconds\", 300),\n                enable_least_privilege=config.get(\"enable_least_privilege\", True),\n            )\n\n            self.logger.info(f\"Initialized secret manager with provider: {provider}\")\n            return SecretManager(secret_config)\n\n        except Exception as e:\n            self.logger.warning(f\"Failed to initialize secret manager: {e}\")\n            return None\n\n    def _resolve_secret(self, value: Any) -&gt; Any:\n        \"\"\"\n        Resolve a secret reference to its actual value.\n\n        Secret references can be specified as:\n        - String starting with \"secret://\" (e.g., \"secret://api-key\")\n        - Dict with \"secret\" key (e.g., {\"secret\": \"api-key\"})\n\n        Args:\n            value: Value that may contain a secret reference\n\n        Returns:\n            Resolved secret value or original value if not a secret reference\n        \"\"\"\n        if not self.secret_manager:\n            return value\n\n        # Handle string secret references\n        if isinstance(value, str) and value.startswith(\"secret://\"):\n            secret_name = value[9:]  # Remove \"secret://\" prefix\n            try:\n                secret_value = self.secret_manager.get_secret(secret_name)\n                self.logger.debug(f\"Retrieved secret: {secret_name}\")\n                return secret_value\n            except (SecretNotFoundError, SecretAccessError) as e:\n                self.logger.error(f\"Failed to retrieve secret '{secret_name}': {e}\")\n                raise\n\n        # Handle dict secret references\n        if isinstance(value, dict) and \"secret\" in value:\n            secret_name = value[\"secret\"]\n            version = value.get(\"version\")\n            try:\n                secret_value = self.secret_manager.get_secret(secret_name, version)\n                self.logger.debug(f\"Retrieved secret: {secret_name}\")\n                return secret_value\n            except (SecretNotFoundError, SecretAccessError) as e:\n                self.logger.error(f\"Failed to retrieve secret '{secret_name}': {e}\")\n                raise\n\n        return value\n\n    def _resolve_auth_secrets(self, auth_config: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Recursively resolve secret references in authentication configuration.\n\n        Args:\n            auth_config: Authentication configuration that may contain secret references\n\n        Returns:\n            Authentication configuration with resolved secrets\n        \"\"\"\n        if not self.secret_manager:\n            return auth_config\n\n        resolved_config = {}\n        for key, value in auth_config.items():\n            if isinstance(value, dict):\n                # Recursively resolve nested dicts\n                resolved_config[key] = self._resolve_auth_secrets(value)\n            elif isinstance(value, list):\n                # Resolve each item in list\n                resolved_config[key] = [  # type: ignore[assignment]\n                    (\n                        self._resolve_secret(item)\n                        if isinstance(item, (str, dict))\n                        else item\n                    )\n                    for item in value\n                ]\n            else:\n                # Resolve individual value\n                resolved_config[key] = self._resolve_secret(value)\n\n        return resolved_config\n\n    def _configure_security(self, config: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Configure security features based on provided configuration.\n\n        Args:\n            config: Security configuration dictionary\n        \"\"\"\n        self.logger.info(\"Configuring security features\")\n\n        # Update encryption level if specified\n        if \"encryption_level\" in config:\n            encryption_level = config[\"encryption_level\"]\n            try:\n                if isinstance(encryption_level, str):\n                    encryption_level = EncryptionLevel[encryption_level.upper()]\n                self.security_manager.request_encryption.encryption_level = (\n                    encryption_level\n                )\n                self.logger.debug(f\"Updated encryption level to {encryption_level}\")\n            except (KeyError, ValueError):\n                self.logger.warning(f\"Invalid encryption level: {encryption_level}\")\n\n        # Add users if specified and access control is enabled\n        if self.security_manager.enable_access_control and \"users\" in config:\n            for user_config in config[\"users\"]:\n                username = user_config.get(\"username\")\n                role = user_config.get(\"role\", \"viewer\")\n                api_key = user_config.get(\"api_key\")\n\n                if username:\n                    self.security_manager.add_user(username, role, api_key)\n                    self.logger.debug(f\"Added user {username} with role {role}\")\n\n    def _configure_error_handling(self, config: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Configure the error handling system based on provided configuration.\n\n        Args:\n            config: Error handling configuration dictionary\n        \"\"\"\n        self.logger.info(\"Configuring error handling system\")\n\n        # Configure circuit breakers\n        if \"circuit_breakers\" in config:\n            for cb_name, cb_config in config[\"circuit_breakers\"].items():\n                failure_threshold = cb_config.get(\"failure_threshold\", 5)\n                reset_timeout = cb_config.get(\"reset_timeout_seconds\", 60)\n                half_open_max_calls = cb_config.get(\"half_open_max_calls\", 1)\n\n                # Create and register circuit breaker\n                circuit: CircuitBreaker = CircuitBreaker(\n                    name=cb_name,\n                    failure_threshold=failure_threshold,\n                    reset_timeout_seconds=reset_timeout,\n                    half_open_max_calls=half_open_max_calls,\n                )\n\n                self.error_recovery_manager.circuit_breakers[cb_name] = circuit\n                self.logger.debug(f\"Configured circuit breaker: {cb_name}\")\n\n        # Configure recovery strategies\n        if \"recovery_strategies\" in config:\n            for category_name, strategies in config[\"recovery_strategies\"].items():\n                try:\n                    error_category = ErrorCategory[category_name.upper()]\n                    strategy_list = [\n                        RecoveryStrategy[str(s).upper()] for s in strategies\n                    ]\n\n                    self.error_recovery_manager.set_strategy(\n                        error_category, strategy_list\n                    )\n                    self.logger.debug(\n                        f\"Configured recovery strategies for {category_name}: {strategies}\"\n                    )\n\n                except (KeyError, ValueError) as e:\n                    self.logger.warning(\n                        f\"Invalid recovery strategy configuration: {str(e)}\"\n                    )\n\n        # Configure DLQ\n        if \"dlq\" in config:\n            dlq_dir = config[\"dlq\"].get(\"directory\")\n            if dlq_dir:\n                self.dlq = DeadLetterQueue(dlq_dir)\n                self.error_recovery_manager.dlq = self.dlq\n                self.logger.info(f\"Configured Dead Letter Queue at {dlq_dir}\")\n\n    def get_error_analytics(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get error analytics summary.\n\n        Returns:\n            Dictionary with error statistics\n        \"\"\"\n        return self.error_analytics.get_summary()\n\n    def add_user(\n        self, username: str, role: str, api_key: Optional[str] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Add a user to the system with specified role.\n\n        Args:\n            username: Username to identify the user\n            role: Access role (admin, operator, viewer, developer)\n            api_key: Optional API key for authentication\n\n        Returns:\n            User data including generated API key if not provided\n        \"\"\"\n        if not self.security_manager.enable_access_control:\n            raise ValueError(\n                \"Access control is not enabled. Enable it in the security configuration.\"\n            )\n\n        return self.security_manager.add_user(username, role, api_key)\n\n    def list_users(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        List all users in the system.\n\n        Returns:\n            List of user data dictionaries\n        \"\"\"\n        if not self.security_manager.enable_access_control:\n            raise ValueError(\n                \"Access control is not enabled. Enable it in the security configuration.\"\n            )\n\n        users = []\n        for username in self.security_manager.access_control.users:\n            user_data = self.security_manager.access_control.get_user(username)\n            # Remove sensitive data like API key\n            if \"api_key\" in user_data:\n                user_data[\"api_key\"] = \"*\" * 8  # Mask API key\n            users.append(user_data)\n\n        return users\n\n    def store_credential(self, name: str, credential_data: Dict[str, Any]) -&gt; bool:\n        \"\"\"\n        Store API credentials securely.\n\n        Args:\n            name: Name to identify the credential\n            credential_data: Credential data to store\n\n        Returns:\n            True if successful, False otherwise\n        \"\"\"\n        return self.security_manager.store_credential(name, credential_data)\n\n    def get_credential(self, name: str) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"\n        Get stored API credentials.\n\n        Args:\n            name: Name of the credential\n\n        Returns:\n            Credential data if found, None otherwise\n        \"\"\"\n        return self.security_manager.get_credential(name)\n\n    def list_credentials(self) -&gt; List[str]:\n        \"\"\"\n        List available credential names.\n\n        Returns:\n            List of credential names\n        \"\"\"\n        return self.security_manager.list_credentials()\n\n    def process_dlq(\n        self, operation_type: Optional[str] = None, limit: int = 10\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Process items in the Dead Letter Queue for retry.\n\n        Args:\n            operation_type: Optional operation type to filter by\n            limit: Maximum number of items to process\n\n        Returns:\n            Dictionary with processing results\n        \"\"\"\n        self.logger.info(\n            f\"Processing Dead Letter Queue (type={operation_type}, limit={limit})\"\n        )\n\n        # Get DLQ items\n        items = self.dlq.get_items(limit=limit)\n\n        class DLQResults(TypedDict):\n            total_processed: int\n            successful: int\n            failed: int\n            items: List[Dict[str, Any]]\n\n        results: DLQResults = {\n            \"total_processed\": 0,\n            \"successful\": 0,\n            \"failed\": 0,\n            \"items\": [],\n        }\n\n        for item in items:\n            # Skip if not matching the requested operation type\n            if (\n                operation_type\n                and item.get(\"metadata\", {}).get(\"operation_type\") != operation_type\n            ):\n                continue\n\n            item_id = item.get(\"id\", \"unknown\")\n            payload = item.get(\"payload\", {})\n            metadata = item.get(\"metadata\", {})\n\n            retry_result = {\n                \"id\": item_id,\n                \"success\": False,\n                \"message\": \"Operation not retried\",\n            }\n\n            # Determine what type of operation this is and how to retry it\n            if \"endpoint\" in payload and \"source_\" in metadata.get(\n                \"operation_type\", \"\"\n            ):\n                # This is a source operation\n                try:\n                    self.source.fetch_data(\n                        payload.get(\"endpoint\"), payload.get(\"params\")\n                    )\n                    retry_result[\"success\"] = True\n                    retry_result[\"message\"] = \"Successfully retried source operation\"\n                    results[\"successful\"] += 1\n                except Exception as e:\n                    retry_result[\"message\"] = (\n                        f\"Failed to retry source operation: {str(e)}\"\n                    )\n                    results[\"failed\"] += 1\n\n            elif \"endpoint\" in payload and \"target_\" in metadata.get(\n                \"operation_type\", \"\"\n            ):\n                # This is a target operation\n                try:\n                    self.target.send_data(payload.get(\"endpoint\"), payload.get(\"data\"))\n                    retry_result[\"success\"] = True\n                    retry_result[\"message\"] = \"Successfully retried target operation\"\n                    results[\"successful\"] += 1\n                except Exception as e:\n                    retry_result[\"message\"] = (\n                        f\"Failed to retry target operation: {str(e)}\"\n                    )\n                    results[\"failed\"] += 1\n\n            else:\n                # Unknown operation type\n                retry_result[\"message\"] = \"Unknown operation type - cannot retry\"\n                results[\"failed\"] += 1\n\n            results[\"total_processed\"] += 1\n            results[\"items\"].append(retry_result)\n\n        self.logger.info(\n            f\"DLQ processing complete: {results['successful']} successful, {results['failed']} failed\"\n        )\n        return dict(results)\n\n    def fetch(\n        self,\n        endpoint: str,\n        params: Optional[Dict[str, Any]] = None,\n    ) -&gt; Any:\n        \"\"\"\n        Convenience wrapper to fetch data from the configured source connector.\n\n        Args:\n            endpoint: Source endpoint name to fetch from\n            params: Optional parameters for the request\n\n        Returns:\n            Parsed response payload from the source API\n        \"\"\"\n        if not self.source:\n            raise ValueError(\"Source connector is not configured\")\n        return self.source.fetch_data(endpoint, params)\n\n    def send(\n        self,\n        endpoint: str,\n        data: Union[Dict[str, Any], List[Dict[str, Any]]],\n        **kwargs: Any,\n    ) -&gt; Any:\n        \"\"\"\n        Convenience wrapper to send data to the configured target connector.\n\n        Args:\n            endpoint: Target endpoint name to send to\n            data: Payload to send (single item or list)\n\n        Returns:\n            Target connector response (if any)\n        \"\"\"\n        if not self.target:\n            raise ValueError(\"Target connector is not configured\")\n        return self.target.send_data(endpoint, data, **kwargs)\n\n    def sync(\n        self,\n        source_endpoint: Optional[str] = None,\n        target_endpoint: Optional[str] = None,\n        params: Optional[Dict[str, Any]] = None,\n        max_retries: int = 3,\n        retry_delay: float = 1.0,\n        retry_backoff_factor: float = 2.0,\n        retry_status_codes: Optional[List[int]] = None,\n    ) -&gt; SyncResult:\n        \"\"\"\n        Execute a sync operation between source and target APIs.\n\n        Args:\n            source_endpoint: Source endpoint to use (overrides mapping)\n            target_endpoint: Target endpoint to use (overrides mapping)\n            params: Additional parameters for the source API call\n            max_retries: Maximum number of retry attempts for transient failures\n            retry_delay: Initial delay between retries in seconds\n            retry_backoff_factor: Multiplicative factor for retry delay\n            retry_status_codes: HTTP status codes to retry (default: 429, 502, 503, 504)\n\n        Returns:\n            SyncResult: Result of the sync operation\n        \"\"\"\n        if not self.source or not self.target:\n            raise ValueError(\n                \"Source and target connectors must be configured before syncing\"\n            )\n\n        # If no endpoints specified, use the first mapping\n        if not source_endpoint or not target_endpoint:\n            mapping = self.mapper.get_first_mapping()\n            if not mapping:\n                raise ValueError(\"No mapping configured and no endpoints specified\")\n            source_endpoint = mapping[\"source\"]\n            target_endpoint = mapping[\"target\"]\n\n        # Generate correlation ID for this sync operation\n        correlation_id = str(uuid.uuid4())\n        start_time = time.time()\n\n        # Wrap entire sync operation in distributed tracing\n        with self.telemetry.trace_sync(\n            source_endpoint, target_endpoint, correlation_id\n        ):\n            # Start provenance\n            self.provenance.start_run(\n                correlation_id=correlation_id,\n                config_path=(\n                    config_path\n                    if (config_path := getattr(self, \"_last_config_path\", None))\n                    else None\n                ),\n                source_endpoint=source_endpoint,\n                target_endpoint=target_endpoint,\n            )\n\n            # Default retry status codes if none provided\n            if retry_status_codes is None:\n                retry_status_codes = [429, 502, 503, 504]  # Common transient failures\n\n            self.logger.info(\n                f\"[{correlation_id}] Starting sync from {source_endpoint} to {target_endpoint}\"\n            )\n\n            # Initialize result object\n            sync_result = SyncResult(correlation_id=correlation_id)\n\n            # Get circuit breaker for source endpoint\n            source_circuit_name = f\"source_{source_endpoint}\"\n            source_cb = self.error_recovery_manager.get_circuit_breaker(\n                source_circuit_name\n            )\n\n            # Check if user has permission for this operation\n            if self.security_manager.enable_access_control:\n                current_user = getattr(self, \"current_user\", None)\n                if current_user and not self.security_manager.check_permission(\n                    current_user, \"run_sync\"\n                ):\n                    raise PermissionError(\n                        f\"User {current_user} does not have permission to run sync operations\"\n                    )\n\n            # Merge last_sync into params from state store if not provided\n            if params is None:\n                effective_params = None\n            else:\n                effective_params = dict(params)\n            if self.state_store:\n                # Ensure we have a dict if we are going to inject\n                need_inject = (effective_params is None) or (\n                    \"updated_since\" not in effective_params\n                )\n                if need_inject:\n                    last_sync = self.state_store.get_last_sync(source_endpoint)\n                    if last_sync:\n                        effective_params = dict(effective_params or {})\n                        effective_params[\"updated_since\"] = last_sync\n\n            # Always use standard non-encrypted call\n            source_data, source_error = source_cb.execute(\n                lambda: self.source.fetch_data(source_endpoint, effective_params)\n            )\n\n            # If circuit breaker failed, try recovery strategies\n            if source_error:\n                # Create payload for retry\n                fetch_payload = {\"endpoint\": source_endpoint, \"params\": params}\n\n                # Apply recovery strategies\n                success, result, error = self.error_recovery_manager.handle_error(\n                    error=source_error,\n                    payload=fetch_payload,\n                    operation=lambda p: self.source.fetch_data(\n                        p[\"endpoint\"], p[\"params\"]\n                    ),\n                    operation_type=source_circuit_name,\n                    max_retries=max_retries,\n                    retry_delay=retry_delay,\n                    retry_backoff_factor=retry_backoff_factor,\n                )\n\n                if success:\n                    source_data = result\n                else:\n                    # Record the error for analytics\n                    self.error_analytics.record_error(error)\n\n                    # Update result with error details\n                    end_time = time.time()\n                    sync_result.duration_ms = int((end_time - start_time) * 1000)\n                    sync_result.success = False\n                    sync_result.errors.append(error.to_dict())\n                    self.logger.error(f\"[{correlation_id}] Sync failed: {error}\")\n                    return sync_result\n\n            try:\n                # Map fields according to configuration\n                transformed_data = self.mapper.map_data(\n                    source_endpoint, target_endpoint, source_data\n                )\n\n                # Optional strict validation against target request schema (if defined in connector)\n                if (\n                    self.validation_config.get(\"strict_mode\")\n                    and is_validator_available()\n                ):\n                    target_endpoint_cfg = (\n                        self.target.endpoints.get(target_endpoint)\n                        if self.target\n                        else None\n                    )\n                    if target_endpoint_cfg and target_endpoint_cfg.request_schema:\n                        if isinstance(transformed_data, list):\n                            for item in transformed_data:\n                                valid, diffs = validate_payload_against_schema(\n                                    item, target_endpoint_cfg.request_schema\n                                )\n                                if not valid:\n                                    raise ApiLinkerError(\n                                        message=\"Strict mode: target payload failed schema validation\",\n                                        error_category=ErrorCategory.VALIDATION,\n                                        status_code=0,\n                                        additional_context={\"diffs\": diffs},\n                                    )\n                        else:\n                            valid, diffs = validate_payload_against_schema(\n                                transformed_data, target_endpoint_cfg.request_schema\n                            )\n                            if not valid:\n                                raise ApiLinkerError(\n                                    message=\"Strict mode: target payload failed schema validation\",\n                                    error_category=ErrorCategory.VALIDATION,\n                                    status_code=0,\n                                    additional_context={\"diffs\": diffs},\n                                )\n\n                # Record source data metrics\n                source_count = len(source_data) if isinstance(source_data, list) else 1\n                sync_result.details[\"source_count\"] = source_count\n\n                # Get circuit breaker for target endpoint\n                target_circuit_name = f\"target_{target_endpoint}\"\n                target_cb = self.error_recovery_manager.get_circuit_breaker(\n                    target_circuit_name\n                )\n\n                # Idempotency: skip payloads we've already sent during replays\n                def _send():\n                    # If idempotency enabled, de-duplicate per item\n                    if self.idempotency_config.get(\"enabled\") and isinstance(\n                        transformed_data, list\n                    ):\n                        salt = self.idempotency_config.get(\"salt\", \"\")\n                        filtered = []\n                        for item in transformed_data:\n                            key = generate_idempotency_key(item, salt=salt)\n                            if not self.deduplicator.has_seen(target_endpoint, key):\n                                self.deduplicator.mark_seen(target_endpoint, key)\n                                filtered.append(item)\n                        payload: Union[Dict[str, Any], List[Dict[str, Any]]] = filtered\n                    else:\n                        payload = transformed_data\n                    return self.target.send_data(target_endpoint, payload)\n\n                # Always use standard non-encrypted call\n                target_result, target_error = target_cb.execute(_send)\n\n                # If circuit breaker failed, try recovery strategies\n                if target_error:\n                    # Create payload for retry\n                    send_payload = {\n                        \"endpoint\": target_endpoint,\n                        \"data\": transformed_data,\n                    }\n\n                    # Apply recovery strategies\n                    success, result, error = self.error_recovery_manager.handle_error(\n                        error=target_error,\n                        payload=send_payload,\n                        operation=lambda p: self.target.send_data(\n                            p[\"endpoint\"], p[\"data\"]\n                        ),\n                        operation_type=target_circuit_name,\n                        max_retries=max_retries,\n                        retry_delay=retry_delay,\n                        retry_backoff_factor=retry_backoff_factor,\n                    )\n\n                    if success:\n                        target_result = result\n                    else:\n                        # Record the error for analytics\n                        self.error_analytics.record_error(error)\n\n                        # Update result with error details\n                        end_time = time.time()\n                        sync_result.duration_ms = int((end_time - start_time) * 1000)\n                        sync_result.success = False\n                        sync_result.errors.append(error.to_dict())\n                        self.logger.error(f\"[{correlation_id}] Sync failed: {error}\")\n                        return sync_result\n\n                # Update result with success information\n                sync_result.count = (\n                    len(transformed_data) if isinstance(transformed_data, list) else 1\n                )\n                sync_result.success = True\n\n                # Set target response directly\n                if isinstance(target_result, dict):\n                    sync_result.target_response = target_result\n                else:\n                    sync_result.target_response = {}\n\n                # Calculate duration\n                end_time = time.time()\n                sync_result.duration_ms = int((end_time - start_time) * 1000)\n\n                self.logger.info(\n                    f\"[{correlation_id}] Sync completed successfully: {sync_result.count} items transferred in {sync_result.duration_ms}ms\"\n                )\n                # Update last_sync checkpoint\n                if self.state_store:\n                    self.state_store.set_last_sync(source_endpoint, now_iso())\n                # Complete provenance\n                self.provenance.complete_run(\n                    True, sync_result.count, sync_result.details\n                )\n\n                # Record telemetry metrics\n                self.telemetry.record_sync_completion(\n                    source_endpoint, target_endpoint, True, sync_result.count\n                )\n\n                return sync_result\n\n            except Exception as e:\n                # Convert to ApiLinkerError\n                error = ApiLinkerError.from_exception(\n                    e,\n                    error_category=ErrorCategory.MAPPING,\n                    correlation_id=correlation_id,\n                    operation_id=f\"mapping_{source_endpoint}_to_{target_endpoint}\",\n                )\n\n                # Record the error for analytics\n                self.error_analytics.record_error(error)\n\n                # Update result\n                end_time = time.time()\n                sync_result.duration_ms = int((end_time - start_time) * 1000)\n                sync_result.success = False\n                sync_result.errors.append(error.to_dict())\n\n                self.logger.error(\n                    f\"[{correlation_id}] Sync failed during mapping: {error}\"\n                )\n                # Record error in provenance\n                self.provenance.record_error(\n                    error.message,\n                    category=error.error_category.value,\n                    status_code=error.status_code,\n                    endpoint=target_endpoint,\n                )\n                self.provenance.complete_run(False, 0, {})\n\n                # Record telemetry metrics\n                self.telemetry.record_sync_completion(\n                    source_endpoint, target_endpoint, False, 0\n                )\n                self.telemetry.record_error(\n                    error.error_category.value, \"sync\", error.message\n                )\n\n            return sync_result\n\n    def start_scheduled_sync(self) -&gt; None:\n        \"\"\"Start scheduled sync operations.\"\"\"\n        self.logger.info(\"Starting scheduled sync\")\n        self.scheduler.start(self.sync)\n\n    def stop_scheduled_sync(self) -&gt; None:\n        \"\"\"Stop scheduled sync operations.\"\"\"\n        self.logger.info(\"Stopping scheduled sync\")\n        self.scheduler.stop()\n\n    def _with_retries(\n        self,\n        operation: Callable[[], Any],\n        operation_name: str,\n        max_retries: int,\n        retry_delay: float,\n        retry_backoff_factor: float,\n        retry_status_codes: List[int],\n        correlation_id: str,\n    ) -&gt; Tuple[Any, Optional[ErrorDetail]]:\n        \"\"\"\n        Execute an operation with configurable retry logic for transient failures.\n\n        Args:\n            operation: Callable function to execute\n            operation_name: Name of operation for logging\n            max_retries: Maximum number of retry attempts\n            retry_delay: Initial delay between retries in seconds\n            retry_backoff_factor: Multiplicative factor for retry delay\n            retry_status_codes: HTTP status codes that should trigger a retry\n            correlation_id: Correlation ID for tracing\n\n        Returns:\n            Tuple of (result, error_detail) - If successful, error_detail will be None\n        \"\"\"\n        current_delay = retry_delay\n\n        for attempt in range(max_retries + 1):\n            try:\n                if attempt &gt; 0:\n                    self.logger.info(\n                        f\"[{correlation_id}] Retry attempt {attempt}/{max_retries} for {operation_name} after {current_delay:.2f}s delay\"\n                    )\n                    time.sleep(current_delay)\n                    current_delay *= retry_backoff_factor\n\n                result = operation()\n\n                if attempt &gt; 0:\n                    self.logger.info(\n                        f\"[{correlation_id}] Retry succeeded for {operation_name}\"\n                    )\n\n                return result, None\n\n            except Exception as e:\n                status_code = getattr(e, \"status_code\", None)\n                response_body = getattr(e, \"response\", None)\n                request_url = getattr(e, \"url\", None)\n                request_method = getattr(e, \"method\", None)\n\n                # Convert response to string if it's not already\n                if response_body and not isinstance(response_body, str):\n                    try:\n                        response_body = str(response_body)[:1000]  # Limit size\n                    except:\n                        response_body = \"&lt;Unable to convert response to string&gt;\"\n\n                error_detail = ErrorDetail(\n                    message=str(e),\n                    status_code=status_code,\n                    response_body=response_body,\n                    request_url=request_url,\n                    request_method=request_method,\n                    timestamp=datetime.now().isoformat(),\n                    error_type=(\n                        \"transient_error\"\n                        if status_code in retry_status_codes\n                        else \"api_error\"\n                    ),\n                )\n\n                # Check if this is a retryable error\n                is_retryable = (\n                    status_code in retry_status_codes if status_code else False\n                )\n\n                if is_retryable and attempt &lt; max_retries:\n                    self.logger.warning(\n                        f\"[{correlation_id}] {operation_name} failed with retryable error (status: {status_code}): {str(e)}\"\n                    )\n                else:\n                    # Either not retryable or out of retries\n                    log_level = logging.WARNING if is_retryable else logging.ERROR\n                    retry_msg = (\n                        \"out of retry attempts\"\n                        if is_retryable\n                        else \"non-retryable error\"\n                    )\n\n                    self.logger.log(\n                        log_level,\n                        f\"[{correlation_id}] {operation_name} failed with {retry_msg}: {str(e)}\",\n                    )\n                    return None, error_detail\n\n        # We should never reach here, but just in case\n        fallback_error = ErrorDetail(\n            message=f\"Unknown error during {operation_name}\",\n            timestamp=datetime.now().isoformat(),\n            error_type=\"unknown_error\",\n        )\n        return None, fallback_error\n</code></pre>"},{"location":"reference/api/#apilinker.ApiLinker.__init__","title":"<code>__init__(config_path=None, source_config=None, target_config=None, mapping_config=None, schedule_config=None, error_handling_config=None, security_config=None, validation_config=None, observability_config=None, secret_manager_config=None, log_level='INFO', log_file=None)</code>","text":"Source code in <code>apilinker/api_linker.py</code> <pre><code>def __init__(\n    self,\n    config_path: Optional[str] = None,\n    source_config: Optional[Dict[str, Any]] = None,\n    target_config: Optional[Dict[str, Any]] = None,\n    mapping_config: Optional[Dict[str, Any]] = None,\n    schedule_config: Optional[Dict[str, Any]] = None,\n    error_handling_config: Optional[Dict[str, Any]] = None,\n    security_config: Optional[Dict[str, Any]] = None,\n    validation_config: Optional[Dict[str, Any]] = None,\n    observability_config: Optional[Dict[str, Any]] = None,\n    secret_manager_config: Optional[Dict[str, Any]] = None,\n    log_level: str = \"INFO\",\n    log_file: Optional[str] = None,\n) -&gt; None:\n    # Initialize logger\n    self.logger = setup_logger(log_level, log_file)\n    self.logger.info(\"Initializing ApiLinker\")\n\n    # Initialize components\n    self.source: Optional[ApiConnector] = None\n    self.target: Optional[ApiConnector] = None\n    self.mapper = FieldMapper()\n    self.scheduler = Scheduler()\n    self.validation_config = validation_config or {\"strict_mode\": False}\n    self.provenance = ProvenanceRecorder()\n    self.deduplicator = InMemoryDeduplicator()\n    self.state_store: Optional[StateStore] = None\n\n    # Initialize observability\n    self.telemetry = self._initialize_observability(observability_config)\n\n    # Initialize secret management\n    self.secret_manager = self._initialize_secret_manager(secret_manager_config)\n\n    # Initialize security system\n    self.security_manager = self._initialize_security(security_config)\n\n    # Initialize auth manager and integrate with security\n    self.auth_manager = AuthManager()\n    integrate_security_with_auth_manager(self.security_manager, self.auth_manager)\n\n    # Initialize error handling system\n    self.dlq, self.error_recovery_manager, self.error_analytics = (\n        create_error_handler()\n    )\n\n    # Load configuration if provided\n    if config_path:\n        self.load_config(config_path)\n    else:\n        # Set up direct configurations if provided\n        if source_config:\n            self.add_source(**source_config)\n        if target_config:\n            self.add_target(**target_config)\n        if mapping_config:\n            self.add_mapping(**mapping_config)\n        if schedule_config:\n            self.add_schedule(**schedule_config)\n        if error_handling_config:\n            self._configure_error_handling(error_handling_config)\n        if security_config:\n            self._configure_security(security_config)\n</code></pre>"},{"location":"reference/api/#apilinker.ApiLinker.add_source","title":"<code>add_source(type, base_url, auth=None, endpoints=None, **kwargs)</code>","text":"<p>Add a source API connector.</p> <p>Parameters:</p> Name Type Description Default <code>type</code> <code>str</code> <p>Type of API connector (rest, graphql, etc.)</p> required <code>base_url</code> <code>str</code> <p>Base URL of the API</p> required <code>auth</code> <code>Optional[Dict[str, Any]]</code> <p>Authentication configuration</p> <code>None</code> <code>endpoints</code> <code>Optional[Dict[str, Any]]</code> <p>Configured endpoints</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional configuration parameters</p> <code>{}</code> Source code in <code>apilinker/api_linker.py</code> <pre><code>def add_source(\n    self,\n    type: str,\n    base_url: str,\n    auth: Optional[Dict[str, Any]] = None,\n    endpoints: Optional[Dict[str, Any]] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Add a source API connector.\n\n    Args:\n        type: Type of API connector (rest, graphql, etc.)\n        base_url: Base URL of the API\n        auth: Authentication configuration\n        endpoints: Configured endpoints\n        **kwargs: Additional configuration parameters\n    \"\"\"\n    self.logger.info(f\"Adding source connector: {type} for {base_url}\")\n\n    # Resolve secrets in authentication configuration\n    if auth:\n        auth = self._resolve_auth_secrets(auth)\n        auth_config = self.auth_manager.configure_auth(auth)\n    else:\n        auth_config = None\n\n    # Create source connector\n    self.source = ApiConnector(\n        connector_type=type,\n        base_url=base_url,\n        auth_config=auth_config,\n        endpoints=endpoints or {},\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/api/#apilinker.ApiLinker.add_target","title":"<code>add_target(type, base_url, auth=None, endpoints=None, **kwargs)</code>","text":"<p>Add a target API connector.</p> <p>Parameters:</p> Name Type Description Default <code>type</code> <code>str</code> <p>Type of API connector (rest, graphql, etc.)</p> required <code>base_url</code> <code>str</code> <p>Base URL of the API</p> required <code>auth</code> <code>Optional[Dict[str, Any]]</code> <p>Authentication configuration</p> <code>None</code> <code>endpoints</code> <code>Optional[Dict[str, Any]]</code> <p>Configured endpoints</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional configuration parameters</p> <code>{}</code> Source code in <code>apilinker/api_linker.py</code> <pre><code>def add_target(\n    self,\n    type: str,\n    base_url: str,\n    auth: Optional[Dict[str, Any]] = None,\n    endpoints: Optional[Dict[str, Any]] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Add a target API connector.\n\n    Args:\n        type: Type of API connector (rest, graphql, etc.)\n        base_url: Base URL of the API\n        auth: Authentication configuration\n        endpoints: Configured endpoints\n        **kwargs: Additional configuration parameters\n    \"\"\"\n    self.logger.info(f\"Adding target connector: {type} for {base_url}\")\n\n    # Resolve secrets in authentication configuration\n    if auth:\n        auth = self._resolve_auth_secrets(auth)\n        auth_config = self.auth_manager.configure_auth(auth)\n    else:\n        auth_config = None\n\n    # Create target connector\n    self.target = ApiConnector(\n        connector_type=type,\n        base_url=base_url,\n        auth_config=auth_config,\n        endpoints=endpoints or {},\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/api/#apilinker.ApiLinker.add_mapping","title":"<code>add_mapping(source, target, fields)</code>","text":"<p>Add a field mapping between source and target endpoints.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Source endpoint name</p> required <code>target</code> <code>str</code> <p>Target endpoint name</p> required <code>fields</code> <code>List[Dict[str, Any]]</code> <p>List of field mappings</p> required Source code in <code>apilinker/api_linker.py</code> <pre><code>def add_mapping(\n    self, source: str, target: str, fields: List[Dict[str, Any]]\n) -&gt; None:\n    \"\"\"\n    Add a field mapping between source and target endpoints.\n\n    Args:\n        source: Source endpoint name\n        target: Target endpoint name\n        fields: List of field mappings\n    \"\"\"\n    self.logger.info(\n        f\"Adding mapping from {source} to {target} with {len(fields)} fields\"\n    )\n    self.mapper.add_mapping(source, target, fields)\n</code></pre>"},{"location":"reference/api/#apilinker.ApiLinker.sync","title":"<code>sync(source_endpoint=None, target_endpoint=None, params=None, max_retries=3, retry_delay=1.0, retry_backoff_factor=2.0, retry_status_codes=None)</code>","text":"<p>Execute a sync operation between source and target APIs.</p> <p>Parameters:</p> Name Type Description Default <code>source_endpoint</code> <code>Optional[str]</code> <p>Source endpoint to use (overrides mapping)</p> <code>None</code> <code>target_endpoint</code> <code>Optional[str]</code> <p>Target endpoint to use (overrides mapping)</p> <code>None</code> <code>params</code> <code>Optional[Dict[str, Any]]</code> <p>Additional parameters for the source API call</p> <code>None</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retry attempts for transient failures</p> <code>3</code> <code>retry_delay</code> <code>float</code> <p>Initial delay between retries in seconds</p> <code>1.0</code> <code>retry_backoff_factor</code> <code>float</code> <p>Multiplicative factor for retry delay</p> <code>2.0</code> <code>retry_status_codes</code> <code>Optional[List[int]]</code> <p>HTTP status codes to retry (default: 429, 502, 503, 504)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>SyncResult</code> <code>SyncResult</code> <p>Result of the sync operation</p> Source code in <code>apilinker/api_linker.py</code> <pre><code>def sync(\n    self,\n    source_endpoint: Optional[str] = None,\n    target_endpoint: Optional[str] = None,\n    params: Optional[Dict[str, Any]] = None,\n    max_retries: int = 3,\n    retry_delay: float = 1.0,\n    retry_backoff_factor: float = 2.0,\n    retry_status_codes: Optional[List[int]] = None,\n) -&gt; SyncResult:\n    \"\"\"\n    Execute a sync operation between source and target APIs.\n\n    Args:\n        source_endpoint: Source endpoint to use (overrides mapping)\n        target_endpoint: Target endpoint to use (overrides mapping)\n        params: Additional parameters for the source API call\n        max_retries: Maximum number of retry attempts for transient failures\n        retry_delay: Initial delay between retries in seconds\n        retry_backoff_factor: Multiplicative factor for retry delay\n        retry_status_codes: HTTP status codes to retry (default: 429, 502, 503, 504)\n\n    Returns:\n        SyncResult: Result of the sync operation\n    \"\"\"\n    if not self.source or not self.target:\n        raise ValueError(\n            \"Source and target connectors must be configured before syncing\"\n        )\n\n    # If no endpoints specified, use the first mapping\n    if not source_endpoint or not target_endpoint:\n        mapping = self.mapper.get_first_mapping()\n        if not mapping:\n            raise ValueError(\"No mapping configured and no endpoints specified\")\n        source_endpoint = mapping[\"source\"]\n        target_endpoint = mapping[\"target\"]\n\n    # Generate correlation ID for this sync operation\n    correlation_id = str(uuid.uuid4())\n    start_time = time.time()\n\n    # Wrap entire sync operation in distributed tracing\n    with self.telemetry.trace_sync(\n        source_endpoint, target_endpoint, correlation_id\n    ):\n        # Start provenance\n        self.provenance.start_run(\n            correlation_id=correlation_id,\n            config_path=(\n                config_path\n                if (config_path := getattr(self, \"_last_config_path\", None))\n                else None\n            ),\n            source_endpoint=source_endpoint,\n            target_endpoint=target_endpoint,\n        )\n\n        # Default retry status codes if none provided\n        if retry_status_codes is None:\n            retry_status_codes = [429, 502, 503, 504]  # Common transient failures\n\n        self.logger.info(\n            f\"[{correlation_id}] Starting sync from {source_endpoint} to {target_endpoint}\"\n        )\n\n        # Initialize result object\n        sync_result = SyncResult(correlation_id=correlation_id)\n\n        # Get circuit breaker for source endpoint\n        source_circuit_name = f\"source_{source_endpoint}\"\n        source_cb = self.error_recovery_manager.get_circuit_breaker(\n            source_circuit_name\n        )\n\n        # Check if user has permission for this operation\n        if self.security_manager.enable_access_control:\n            current_user = getattr(self, \"current_user\", None)\n            if current_user and not self.security_manager.check_permission(\n                current_user, \"run_sync\"\n            ):\n                raise PermissionError(\n                    f\"User {current_user} does not have permission to run sync operations\"\n                )\n\n        # Merge last_sync into params from state store if not provided\n        if params is None:\n            effective_params = None\n        else:\n            effective_params = dict(params)\n        if self.state_store:\n            # Ensure we have a dict if we are going to inject\n            need_inject = (effective_params is None) or (\n                \"updated_since\" not in effective_params\n            )\n            if need_inject:\n                last_sync = self.state_store.get_last_sync(source_endpoint)\n                if last_sync:\n                    effective_params = dict(effective_params or {})\n                    effective_params[\"updated_since\"] = last_sync\n\n        # Always use standard non-encrypted call\n        source_data, source_error = source_cb.execute(\n            lambda: self.source.fetch_data(source_endpoint, effective_params)\n        )\n\n        # If circuit breaker failed, try recovery strategies\n        if source_error:\n            # Create payload for retry\n            fetch_payload = {\"endpoint\": source_endpoint, \"params\": params}\n\n            # Apply recovery strategies\n            success, result, error = self.error_recovery_manager.handle_error(\n                error=source_error,\n                payload=fetch_payload,\n                operation=lambda p: self.source.fetch_data(\n                    p[\"endpoint\"], p[\"params\"]\n                ),\n                operation_type=source_circuit_name,\n                max_retries=max_retries,\n                retry_delay=retry_delay,\n                retry_backoff_factor=retry_backoff_factor,\n            )\n\n            if success:\n                source_data = result\n            else:\n                # Record the error for analytics\n                self.error_analytics.record_error(error)\n\n                # Update result with error details\n                end_time = time.time()\n                sync_result.duration_ms = int((end_time - start_time) * 1000)\n                sync_result.success = False\n                sync_result.errors.append(error.to_dict())\n                self.logger.error(f\"[{correlation_id}] Sync failed: {error}\")\n                return sync_result\n\n        try:\n            # Map fields according to configuration\n            transformed_data = self.mapper.map_data(\n                source_endpoint, target_endpoint, source_data\n            )\n\n            # Optional strict validation against target request schema (if defined in connector)\n            if (\n                self.validation_config.get(\"strict_mode\")\n                and is_validator_available()\n            ):\n                target_endpoint_cfg = (\n                    self.target.endpoints.get(target_endpoint)\n                    if self.target\n                    else None\n                )\n                if target_endpoint_cfg and target_endpoint_cfg.request_schema:\n                    if isinstance(transformed_data, list):\n                        for item in transformed_data:\n                            valid, diffs = validate_payload_against_schema(\n                                item, target_endpoint_cfg.request_schema\n                            )\n                            if not valid:\n                                raise ApiLinkerError(\n                                    message=\"Strict mode: target payload failed schema validation\",\n                                    error_category=ErrorCategory.VALIDATION,\n                                    status_code=0,\n                                    additional_context={\"diffs\": diffs},\n                                )\n                    else:\n                        valid, diffs = validate_payload_against_schema(\n                            transformed_data, target_endpoint_cfg.request_schema\n                        )\n                        if not valid:\n                            raise ApiLinkerError(\n                                message=\"Strict mode: target payload failed schema validation\",\n                                error_category=ErrorCategory.VALIDATION,\n                                status_code=0,\n                                additional_context={\"diffs\": diffs},\n                            )\n\n            # Record source data metrics\n            source_count = len(source_data) if isinstance(source_data, list) else 1\n            sync_result.details[\"source_count\"] = source_count\n\n            # Get circuit breaker for target endpoint\n            target_circuit_name = f\"target_{target_endpoint}\"\n            target_cb = self.error_recovery_manager.get_circuit_breaker(\n                target_circuit_name\n            )\n\n            # Idempotency: skip payloads we've already sent during replays\n            def _send():\n                # If idempotency enabled, de-duplicate per item\n                if self.idempotency_config.get(\"enabled\") and isinstance(\n                    transformed_data, list\n                ):\n                    salt = self.idempotency_config.get(\"salt\", \"\")\n                    filtered = []\n                    for item in transformed_data:\n                        key = generate_idempotency_key(item, salt=salt)\n                        if not self.deduplicator.has_seen(target_endpoint, key):\n                            self.deduplicator.mark_seen(target_endpoint, key)\n                            filtered.append(item)\n                    payload: Union[Dict[str, Any], List[Dict[str, Any]]] = filtered\n                else:\n                    payload = transformed_data\n                return self.target.send_data(target_endpoint, payload)\n\n            # Always use standard non-encrypted call\n            target_result, target_error = target_cb.execute(_send)\n\n            # If circuit breaker failed, try recovery strategies\n            if target_error:\n                # Create payload for retry\n                send_payload = {\n                    \"endpoint\": target_endpoint,\n                    \"data\": transformed_data,\n                }\n\n                # Apply recovery strategies\n                success, result, error = self.error_recovery_manager.handle_error(\n                    error=target_error,\n                    payload=send_payload,\n                    operation=lambda p: self.target.send_data(\n                        p[\"endpoint\"], p[\"data\"]\n                    ),\n                    operation_type=target_circuit_name,\n                    max_retries=max_retries,\n                    retry_delay=retry_delay,\n                    retry_backoff_factor=retry_backoff_factor,\n                )\n\n                if success:\n                    target_result = result\n                else:\n                    # Record the error for analytics\n                    self.error_analytics.record_error(error)\n\n                    # Update result with error details\n                    end_time = time.time()\n                    sync_result.duration_ms = int((end_time - start_time) * 1000)\n                    sync_result.success = False\n                    sync_result.errors.append(error.to_dict())\n                    self.logger.error(f\"[{correlation_id}] Sync failed: {error}\")\n                    return sync_result\n\n            # Update result with success information\n            sync_result.count = (\n                len(transformed_data) if isinstance(transformed_data, list) else 1\n            )\n            sync_result.success = True\n\n            # Set target response directly\n            if isinstance(target_result, dict):\n                sync_result.target_response = target_result\n            else:\n                sync_result.target_response = {}\n\n            # Calculate duration\n            end_time = time.time()\n            sync_result.duration_ms = int((end_time - start_time) * 1000)\n\n            self.logger.info(\n                f\"[{correlation_id}] Sync completed successfully: {sync_result.count} items transferred in {sync_result.duration_ms}ms\"\n            )\n            # Update last_sync checkpoint\n            if self.state_store:\n                self.state_store.set_last_sync(source_endpoint, now_iso())\n            # Complete provenance\n            self.provenance.complete_run(\n                True, sync_result.count, sync_result.details\n            )\n\n            # Record telemetry metrics\n            self.telemetry.record_sync_completion(\n                source_endpoint, target_endpoint, True, sync_result.count\n            )\n\n            return sync_result\n\n        except Exception as e:\n            # Convert to ApiLinkerError\n            error = ApiLinkerError.from_exception(\n                e,\n                error_category=ErrorCategory.MAPPING,\n                correlation_id=correlation_id,\n                operation_id=f\"mapping_{source_endpoint}_to_{target_endpoint}\",\n            )\n\n            # Record the error for analytics\n            self.error_analytics.record_error(error)\n\n            # Update result\n            end_time = time.time()\n            sync_result.duration_ms = int((end_time - start_time) * 1000)\n            sync_result.success = False\n            sync_result.errors.append(error.to_dict())\n\n            self.logger.error(\n                f\"[{correlation_id}] Sync failed during mapping: {error}\"\n            )\n            # Record error in provenance\n            self.provenance.record_error(\n                error.message,\n                category=error.error_category.value,\n                status_code=error.status_code,\n                endpoint=target_endpoint,\n            )\n            self.provenance.complete_run(False, 0, {})\n\n            # Record telemetry metrics\n            self.telemetry.record_sync_completion(\n                source_endpoint, target_endpoint, False, 0\n            )\n            self.telemetry.record_error(\n                error.error_category.value, \"sync\", error.message\n            )\n\n        return sync_result\n</code></pre>"},{"location":"reference/api/#apilinker.ApiLinker.fetch","title":"<code>fetch(endpoint, params=None)</code>","text":"<p>Convenience wrapper to fetch data from the configured source connector.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>Source endpoint name to fetch from</p> required <code>params</code> <code>Optional[Dict[str, Any]]</code> <p>Optional parameters for the request</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Parsed response payload from the source API</p> Source code in <code>apilinker/api_linker.py</code> <pre><code>def fetch(\n    self,\n    endpoint: str,\n    params: Optional[Dict[str, Any]] = None,\n) -&gt; Any:\n    \"\"\"\n    Convenience wrapper to fetch data from the configured source connector.\n\n    Args:\n        endpoint: Source endpoint name to fetch from\n        params: Optional parameters for the request\n\n    Returns:\n        Parsed response payload from the source API\n    \"\"\"\n    if not self.source:\n        raise ValueError(\"Source connector is not configured\")\n    return self.source.fetch_data(endpoint, params)\n</code></pre>"},{"location":"reference/api/#apilinker.ApiLinker.send","title":"<code>send(endpoint, data, **kwargs)</code>","text":"<p>Convenience wrapper to send data to the configured target connector.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>Target endpoint name to send to</p> required <code>data</code> <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> <p>Payload to send (single item or list)</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Target connector response (if any)</p> Source code in <code>apilinker/api_linker.py</code> <pre><code>def send(\n    self,\n    endpoint: str,\n    data: Union[Dict[str, Any], List[Dict[str, Any]]],\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"\n    Convenience wrapper to send data to the configured target connector.\n\n    Args:\n        endpoint: Target endpoint name to send to\n        data: Payload to send (single item or list)\n\n    Returns:\n        Target connector response (if any)\n    \"\"\"\n    if not self.target:\n        raise ValueError(\"Target connector is not configured\")\n    return self.target.send_data(endpoint, data, **kwargs)\n</code></pre>"},{"location":"reference/api/#apiconnector","title":"ApiConnector","text":"<p>Base class for all API connectors.</p>"},{"location":"reference/api/#apilinker.core.connector.ApiConnector","title":"<code>apilinker.core.connector.ApiConnector</code>","text":"<p>API Connector for interacting with REST APIs.</p> <p>This class handles the connection to APIs, making requests, and processing responses.</p> <p>Parameters:</p> Name Type Description Default <code>connector_type</code> <code>str</code> <p>Type of connector (rest, graphql, etc.)</p> required <code>base_url</code> <code>str</code> <p>Base URL for the API</p> required <code>auth_config</code> <code>Optional[AuthConfig]</code> <p>Authentication configuration</p> <code>None</code> <code>endpoints</code> <code>Optional[Dict[str, Dict[str, Any]]]</code> <p>Dictionary of endpoint configurations</p> <code>None</code> <code>timeout</code> <code>int</code> <p>Request timeout in seconds</p> <code>30</code> <code>retry_count</code> <code>int</code> <p>Number of retries on failure</p> <code>3</code> <code>retry_delay</code> <code>int</code> <p>Delay between retries in seconds</p> <code>1</code> Source code in <code>apilinker/core/connector.py</code> <pre><code>class ApiConnector:\n    \"\"\"\n    API Connector for interacting with REST APIs.\n\n    This class handles the connection to APIs, making requests, and\n    processing responses.\n\n    Args:\n        connector_type: Type of connector (rest, graphql, etc.)\n        base_url: Base URL for the API\n        auth_config: Authentication configuration\n        endpoints: Dictionary of endpoint configurations\n        timeout: Request timeout in seconds\n        retry_count: Number of retries on failure\n        retry_delay: Delay between retries in seconds\n    \"\"\"\n\n    def __init__(\n        self,\n        connector_type: str,\n        base_url: str,\n        auth_config: Optional[AuthConfig] = None,\n        endpoints: Optional[Dict[str, Dict[str, Any]]] = None,\n        timeout: int = 30,\n        retry_count: int = 3,\n        retry_delay: int = 1,\n        default_headers: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        self.connector_type = connector_type\n        self.base_url = base_url\n        self.auth_config = auth_config\n        self.timeout = timeout\n        self.retry_count = retry_count\n        self.retry_delay = retry_delay\n\n        # Default headers (may be provided via explicit parameter or legacy 'headers' kwarg)\n        if default_headers is None and \"headers\" in kwargs:\n            try:\n                default_headers = dict(kwargs.pop(\"headers\"))\n            except Exception:\n                default_headers = None\n        self.default_headers: Dict[str, str] = default_headers or {}\n        # Provide a backwards-compatible attribute name used by some connectors\n        self.headers: Dict[str, str] = self.default_headers\n\n        # Parse and store endpoint configurations\n        self.endpoints: Dict[str, EndpointConfig] = {}\n        if endpoints:\n            for name, config in endpoints.items():\n                self.endpoints[name] = EndpointConfig(**config)\n\n        # Store additional settings\n        self.settings: Dict[str, Any] = kwargs\n\n        # Create HTTP client with default settings\n        self.client = self._create_client()\n\n        # Initialize Rate Limit Manager\n        self.rate_limit_manager = RateLimitManager()\n\n        # Configure rate limiters for endpoints\n        for name, config in self.endpoints.items():\n            if config.rate_limit:\n                self.rate_limit_manager.create_limiter(name, config.rate_limit)\n\n        logger.debug(f\"Initialized {connector_type} connector for {base_url}\")\n\n    def _create_client(self) -&gt; httpx.Client:\n        \"\"\"Create an HTTP client with appropriate settings.\"\"\"\n        # Initialize with default parameters\n        auth = None\n        if (\n            self.auth_config\n            and self.auth_config.type == \"basic\"\n            and hasattr(self.auth_config, \"username\")\n            and hasattr(self.auth_config, \"password\")\n        ):\n            auth = httpx.BasicAuth(\n                username=getattr(self.auth_config, \"username\", \"\"),\n                password=getattr(self.auth_config, \"password\", \"\"),\n            )\n\n        # Create client with properly structured parameters\n        return httpx.Client(base_url=self.base_url, timeout=self.timeout, auth=auth)\n\n    def _prepare_request(\n        self, endpoint_name: str, params: Optional[Dict[str, Any]] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Prepare a request for the given endpoint.\n\n        Args:\n            endpoint_name: Name of the endpoint to use\n            params: Additional parameters to include in the request\n\n        Returns:\n            Dict containing request details (url, method, headers, params, json)\n        \"\"\"\n        if endpoint_name not in self.endpoints:\n            raise ValueError(f\"Endpoint '{endpoint_name}' not found in configuration\")\n\n        endpoint = self.endpoints[endpoint_name]\n\n        # Combine endpoint path with base URL\n        url = endpoint.path\n\n        # Combine params from endpoint config and provided params\n        request_params = endpoint.params.copy()\n        if params:\n            request_params.update(params)\n\n        # Prepare headers (merge default headers and endpoint-specific headers)\n        headers = {**(self.default_headers or {}), **endpoint.headers}\n\n        # Add auth headers if needed\n        if self.auth_config:\n            if (\n                self.auth_config.type == \"api_key\"\n                and hasattr(self.auth_config, \"in_header\")\n                and getattr(self.auth_config, \"in_header\", False)\n            ):\n                header_name = getattr(self.auth_config, \"header_name\", \"X-API-Key\")\n                key = getattr(self.auth_config, \"key\", \"\")\n                headers[header_name] = key\n            elif self.auth_config.type == \"bearer\" and hasattr(\n                self.auth_config, \"token\"\n            ):\n                headers[\"Authorization\"] = (\n                    f\"Bearer {getattr(self.auth_config, 'token', '')}\"\n                )\n\n        # Prepare request object\n        request = {\n            \"url\": url,\n            \"method\": endpoint.method,\n            \"headers\": headers,\n            \"params\": request_params,\n        }\n\n        # Add body if endpoint has a body template\n        if endpoint.body_template:\n            request[\"json\"] = endpoint.body_template\n\n        return request\n\n    def _process_response(\n        self, response: httpx.Response, endpoint_name: str\n    ) -&gt; Union[Dict[str, Any], List[Dict[str, Any]]]:\n        \"\"\"\n        Process the API response.\n\n        Args:\n            response: The HTTP response\n            endpoint_name: Name of the endpoint\n\n        Returns:\n            Parsed response data\n        \"\"\"\n        # Update rate limiter from response headers\n        self.rate_limit_manager.update_from_response(endpoint_name, response)\n\n        # Raise for HTTP errors\n        response.raise_for_status()\n\n        # Parse JSON response\n        data: Any = response.json()\n\n        # Extract data from response path if configured\n        endpoint = self.endpoints[endpoint_name]\n        if endpoint.response_path and isinstance(data, dict):\n            path_parts = endpoint.response_path.split(\".\")\n            current_data: Any = data\n            for part in path_parts:\n                if isinstance(current_data, dict) and part in current_data:\n                    current_data = current_data[part]\n                else:\n                    logger.warning(\n                        f\"Response path '{endpoint.response_path}' not found in response\"\n                    )\n                    break\n            # Only update data if we successfully navigated through the path\n            if current_data is not data:\n                data = current_data\n\n        # Validate against response schema if provided\n        endpoint = self.endpoints[endpoint_name]\n        if endpoint.response_schema and is_validator_available():\n            valid, diffs = validate_payload_against_schema(\n                data, endpoint.response_schema\n            )\n            if not valid:\n                logger.warning(\n                    \"Response schema validation failed for %s\\n%s\",\n                    endpoint_name,\n                    pretty_print_diffs(diffs),\n                )\n                # Record as a rate-limit or validation event via logger context (provenance layer may hook logs)\n\n        # Ensure we return a valid type\n        if isinstance(data, (dict, list)):\n            return data\n        else:\n            # If response isn't a dict or list, wrap it in a dict\n            return {\"value\": data}\n\n    def _handle_pagination(\n        self,\n        initial_data: Union[Dict[str, Any], List[Dict[str, Any]]],\n        endpoint_name: str,\n        params: Optional[Dict[str, Any]] = None,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Handle paginated responses if pagination is configured.\n\n        Args:\n            initial_data: Data from the first request\n            endpoint_name: Name of the endpoint\n            params: Request parameters\n\n        Returns:\n            Combined data from all pages\n        \"\"\"\n        endpoint = self.endpoints[endpoint_name]\n\n        # If no pagination config or initial data is not a dict, return as is\n        if not endpoint.pagination or not isinstance(initial_data, dict):\n            if isinstance(initial_data, list):\n                return initial_data\n            # Convert non-list data to a single-item list\n            return (\n                [initial_data]\n                if isinstance(initial_data, dict)\n                else [{\"value\": initial_data}]\n            )\n\n        # Extract the pagination configuration\n        pagination = endpoint.pagination\n        data_path = pagination.get(\"data_path\", \"\")\n        next_page_path = pagination.get(\"next_page_path\", \"\")\n        page_param = pagination.get(\"page_param\", \"page\")\n\n        # Extract the items from the first response\n        if data_path:\n            path_parts = data_path.split(\".\")\n            items: Any = initial_data\n            for part in path_parts:\n                if isinstance(items, dict) and part in items:\n                    items = items[part]\n                else:\n                    logger.warning(f\"Data path '{data_path}' not found in response\")\n                    return [initial_data]\n        else:\n            # If no data path is specified, the entire response is the data\n            items = initial_data\n\n        # Normalize items to a list of dicts\n        if not isinstance(items, list):\n            items_list: List[Dict[str, Any]] = (\n                [items] if isinstance(items, dict) else [{\"value\": items}]\n            )\n        else:\n            items_list = []\n            for elem in items:\n                if isinstance(elem, dict):\n                    items_list.append(elem)\n                else:\n                    items_list.append({\"value\": elem})\n\n        # Extract next page token/URL if available\n        next_page: Optional[Union[str, int]] = None\n        if next_page_path:\n            path_parts = next_page_path.split(\".\")\n            temp_next_page: Any = initial_data\n            for part in path_parts:\n                if isinstance(temp_next_page, dict) and part in temp_next_page:\n                    temp_next_page = temp_next_page[part]\n                else:\n                    temp_next_page = None\n                    break\n            # Only assign if it's a valid type for pagination\n            if isinstance(temp_next_page, (str, int)):\n                next_page = temp_next_page\n\n        # Return the items if there's no next page\n        if not next_page:\n            return items_list\n\n        # Fetch all pages\n        all_items: List[Dict[str, Any]] = list(items_list)\n        page = 2\n\n        while next_page:\n            # Update params for next page\n            next_params: Dict[str, Any] = params.copy() if params else {}\n\n            # Use either page number or next page token (refined to str|int in this loop)\n            next_params[page_param] = next_page\n\n            # Make the next request\n            try:\n                request = self._prepare_request(endpoint_name, next_params)\n                response = self.client.request(\n                    request[\"method\"],\n                    request[\"url\"],\n                    headers=request[\"headers\"],\n                    params=request[\"params\"],\n                    json=request.get(\"json\"),\n                )\n                response.raise_for_status()\n                page_data = response.json()\n\n                # Extract items from this page\n                page_items: Any\n                if data_path:\n                    path_parts = data_path.split(\".\")\n                    page_items = page_data\n                    for part in path_parts:\n                        if isinstance(page_items, dict) and part in page_items:\n                            page_items = page_items[part]\n                        else:\n                            page_items = []\n                            break\n                else:\n                    page_items = page_data\n\n                # Add items to the result, normalizing to list[dict]\n                if isinstance(page_items, list):\n                    for elem in page_items:\n                        if isinstance(elem, dict):\n                            all_items.append(elem)\n                        else:\n                            all_items.append({\"value\": elem})\n                else:\n                    all_items.append(\n                        page_items\n                        if isinstance(page_items, dict)\n                        else {\"value\": page_items}\n                    )\n\n                # Extract next page token\n                if next_page_path:\n                    path_parts = next_page_path.split(\".\")\n                    temp_next_page = page_data\n                    for part in path_parts:\n                        if isinstance(temp_next_page, dict) and part in temp_next_page:\n                            temp_next_page = temp_next_page[part]\n                        else:\n                            temp_next_page = None\n                            break\n                    # Only assign if it's a valid type for pagination\n                    if isinstance(temp_next_page, (str, int)):\n                        next_page = temp_next_page\n                    else:\n                        next_page = None\n                else:\n                    # If no next page path, just increment the page number\n                    page += 1\n                    next_page = (\n                        page if page &lt;= pagination.get(\"max_pages\", 10) else None\n                    )\n\n            except Exception as e:\n                logger.error(f\"Error fetching page {page}: {str(e)}\")\n                break\n\n        return all_items\n\n    def _categorize_error(self, exc: Exception) -&gt; Tuple[ErrorCategory, int]:\n        \"\"\"\n        Categorize an exception to determine its error category and status code.\n\n        Args:\n            exc: The exception to categorize\n\n        Returns:\n            Tuple of (ErrorCategory, status_code)\n        \"\"\"\n        # Default values\n        category = ErrorCategory.UNKNOWN\n        status_code = None\n\n        # Check for HTTP-specific errors\n        if isinstance(exc, httpx.TimeoutException):\n            category = ErrorCategory.TIMEOUT\n            status_code = 0  # Custom code for timeout\n        elif isinstance(exc, httpx.TransportError) or isinstance(\n            exc, httpx.RequestError\n        ):\n            category = ErrorCategory.NETWORK\n            status_code = 0  # Custom code for network/transport errors\n        elif isinstance(exc, httpx.HTTPStatusError):\n            status_code = exc.response.status_code\n\n            # Categorize based on HTTP status code\n            if status_code == 401 or status_code == 403:\n                category = ErrorCategory.AUTHENTICATION\n            elif status_code == 422 or status_code == 400:\n                category = ErrorCategory.VALIDATION\n            elif status_code == 429:\n                category = ErrorCategory.RATE_LIMIT\n            elif status_code &gt;= 500:\n                category = ErrorCategory.SERVER\n            elif status_code &gt;= 400:\n                category = ErrorCategory.CLIENT\n\n        return category, status_code\n\n    def _decode_sse_data(self, raw_data: str, decode_json: bool = True) -&gt; Any:\n        \"\"\"\n        Decode an SSE data field.\n\n        Args:\n            raw_data: Raw SSE data payload after line-join\n            decode_json: Whether to attempt JSON decoding\n\n        Returns:\n            Decoded object or raw string payload\n        \"\"\"\n        if not decode_json:\n            return raw_data\n\n        if raw_data.strip() == \"\":\n            return raw_data\n\n        try:\n            return json.loads(raw_data)\n        except Exception:\n            return raw_data\n\n    def _parse_sse_event(\n        self, raw_lines: List[str], decode_json: bool = True\n    ) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"\n        Parse a single SSE event block (lines between blank separators).\n\n        Args:\n            raw_lines: Raw lines from the SSE stream\n            decode_json: Whether to decode JSON payloads automatically\n\n        Returns:\n            Parsed event dictionary, or None if event is ignorable.\n            Returned event includes a transient ``has_data`` flag used internally.\n        \"\"\"\n        if not raw_lines:\n            return None\n\n        event_id: Optional[str] = None\n        event_name = \"message\"\n        data_lines: List[str] = []\n        retry_ms: Optional[int] = None\n\n        for line in raw_lines:\n            if line.startswith(\":\"):\n                # SSE comment/keepalive line\n                continue\n\n            if \":\" in line:\n                field, value = line.split(\":\", 1)\n                if value.startswith(\" \"):\n                    value = value[1:]\n            else:\n                field, value = line, \"\"\n\n            if field == \"event\":\n                event_name = value or \"message\"\n            elif field == \"data\":\n                data_lines.append(value)\n            elif field == \"id\":\n                # Ignore invalid ids per SSE spec guidance\n                if \"\\x00\" not in value:\n                    event_id = value\n            elif field == \"retry\":\n                try:\n                    parsed_retry = int(value)\n                    if parsed_retry &gt;= 0:\n                        retry_ms = parsed_retry\n                except (TypeError, ValueError):\n                    logger.debug(\"Ignoring invalid SSE retry value: %s\", value)\n\n        has_data = len(data_lines) &gt; 0\n        if not has_data and retry_ms is None and event_id is None:\n            return None\n\n        raw_data = \"\\n\".join(data_lines)\n        return {\n            \"id\": event_id,\n            \"event\": event_name,\n            \"data\": self._decode_sse_data(raw_data, decode_json) if has_data else None,\n            \"retry\": retry_ms,\n            \"raw_data\": raw_data,\n            \"has_data\": has_data,\n        }\n\n    def _build_sse_request(\n        self,\n        endpoint_name: str,\n        params: Optional[Dict[str, Any]] = None,\n        last_event_id: Optional[str] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Build a request dictionary for an SSE endpoint.\n\n        Adds SSE-friendly headers while preserving endpoint/default headers.\n        \"\"\"\n        request = self._prepare_request(endpoint_name, params)\n        headers = dict(request.get(\"headers\", {}))\n        headers.setdefault(\"Accept\", \"text/event-stream\")\n        headers.setdefault(\"Cache-Control\", \"no-cache\")\n        if last_event_id:\n            headers[\"Last-Event-ID\"] = last_event_id\n        request[\"headers\"] = headers\n        return request\n\n    def _raise_sse_error(\n        self, exc: Exception, endpoint_name: str, request: Dict[str, Any]\n    ) -&gt; None:\n        \"\"\"Raise an ``ApiLinkerError`` for SSE failures with rich context.\"\"\"\n        error_category, status_code = self._categorize_error(exc)\n        response_body = None\n        if hasattr(exc, \"response\"):\n            try:\n                response_body = exc.response.text[:1000]\n            except Exception:\n                response_body = \"&lt;Unable to read response body&gt;\"\n\n        raise ApiLinkerError(\n            message=f\"Failed to stream SSE from {endpoint_name}: {str(exc)}\",\n            error_category=error_category,\n            status_code=status_code,\n            response_body=response_body,\n            request_url=str(request.get(\"url\")),\n            request_method=request.get(\"method\", \"GET\"),\n            additional_context={\"endpoint\": endpoint_name},\n        )\n\n    def stream_sse(\n        self,\n        endpoint_name: str,\n        params: Optional[Dict[str, Any]] = None,\n        max_events: Optional[int] = None,\n        reconnect: bool = True,\n        reconnect_delay: float = 1.0,\n        max_reconnect_attempts: Optional[int] = None,\n        read_timeout: Optional[float] = None,\n        decode_json: bool = True,\n    ) -&gt; Generator[Dict[str, Any], None, None]:\n        \"\"\"\n        Stream events from an SSE endpoint.\n\n        Supports automatic reconnection, Last-Event-ID resume, and optional\n        JSON decoding of event payloads.\n\n        Args:\n            endpoint_name: Endpoint key configured in ``self.endpoints``.\n            params: Query parameters.\n            max_events: Stop after this many dispatched events.\n            reconnect: Whether to reconnect on disconnect/error.\n            reconnect_delay: Base reconnect delay in seconds.\n            max_reconnect_attempts: Maximum reconnect attempts (None = unlimited).\n            read_timeout: Per-stream read timeout. Defaults to connector timeout.\n            decode_json: Whether to decode JSON payloads automatically.\n\n        Yields:\n            Parsed SSE events with keys: ``id``, ``event``, ``data``, ``retry``, ``raw_data``.\n        \"\"\"\n        if endpoint_name not in self.endpoints:\n            raise ValueError(f\"Endpoint '{endpoint_name}' not found in configuration\")\n\n        endpoint = self.endpoints[endpoint_name]\n        sse_config = endpoint.sse or {}\n\n        if max_events is None and sse_config.get(\"max_events\") is not None:\n            max_events = int(sse_config[\"max_events\"])\n\n        reconnect = bool(sse_config.get(\"reconnect\", reconnect))\n        reconnect_delay = float(sse_config.get(\"reconnect_delay\", reconnect_delay))\n\n        if (\n            max_reconnect_attempts is None\n            and sse_config.get(\"max_reconnect_attempts\") is not None\n        ):\n            max_reconnect_attempts = int(sse_config[\"max_reconnect_attempts\"])\n\n        if read_timeout is None:\n            read_timeout = sse_config.get(\"read_timeout\")\n        if read_timeout is None:\n            read_timeout = self.timeout\n\n        decode_json = bool(sse_config.get(\"decode_json\", decode_json))\n\n        reconnect_attempts = 0\n        received_events = 0\n        last_event_id: Optional[str] = None\n        effective_reconnect_delay = max(0.0, reconnect_delay)\n\n        while True:\n            request = self._build_sse_request(endpoint_name, params, last_event_id)\n            try:\n                # Apply endpoint-level rate limiting before opening/renewing stream\n                self.rate_limit_manager.acquire(endpoint_name)\n\n                with self.client.stream(\n                    request[\"method\"],\n                    request[\"url\"],\n                    headers=request[\"headers\"],\n                    params=request[\"params\"],\n                    json=request.get(\"json\"),\n                    timeout=read_timeout,\n                ) as response:\n                    self.rate_limit_manager.update_from_response(\n                        endpoint_name, response\n                    )\n                    response.raise_for_status()\n                    logger.info(\n                        \"Connected to SSE endpoint: %s (%s %s)\",\n                        endpoint_name,\n                        request[\"method\"],\n                        request[\"url\"],\n                    )\n\n                    pending_lines: List[str] = []\n\n                    def _dispatch_event(\n                        event: Dict[str, Any],\n                    ) -&gt; Optional[Dict[str, Any]]:\n                        nonlocal effective_reconnect_delay, last_event_id, received_events\n\n                        retry_hint = event.get(\"retry\")\n                        if isinstance(retry_hint, int):\n                            effective_reconnect_delay = max(\n                                0.0, float(retry_hint) / 1000.0\n                            )\n\n                        event_id = event.get(\"id\")\n                        if event_id:\n                            last_event_id = str(event_id)\n\n                        if not event.get(\"has_data\"):\n                            # Retry/id-only control blocks are not dispatched\n                            return None\n\n                        event.pop(\"has_data\", None)\n                        received_events += 1\n                        return event\n\n                    for raw_line in response.iter_lines():\n                        if raw_line is None:\n                            continue\n\n                        line = raw_line.rstrip(\"\\r\")\n\n                        if line == \"\":\n                            parsed = self._parse_sse_event(\n                                pending_lines, decode_json=decode_json\n                            )\n                            pending_lines = []\n\n                            if parsed is None:\n                                continue\n\n                            event = _dispatch_event(parsed)\n                            if event is None:\n                                continue\n\n                            yield event\n                            if max_events is not None and received_events &gt;= max_events:\n                                return\n                        else:\n                            pending_lines.append(line)\n\n                    # Flush trailing buffered event block on connection close\n                    if pending_lines:\n                        parsed = self._parse_sse_event(\n                            pending_lines, decode_json=decode_json\n                        )\n                        if parsed is not None:\n                            event = _dispatch_event(parsed)\n                            if event is not None:\n                                yield event\n                                if (\n                                    max_events is not None\n                                    and received_events &gt;= max_events\n                                ):\n                                    return\n\n            except Exception as exc:\n                if not reconnect:\n                    self._raise_sse_error(exc, endpoint_name, request)\n\n                reconnect_attempts += 1\n                if (\n                    max_reconnect_attempts is not None\n                    and reconnect_attempts &gt; max_reconnect_attempts\n                ):\n                    self._raise_sse_error(exc, endpoint_name, request)\n\n                logger.warning(\n                    \"SSE stream error on endpoint '%s': %s. Reconnecting in %.2fs (attempt %d)\",\n                    endpoint_name,\n                    str(exc),\n                    effective_reconnect_delay,\n                    reconnect_attempts,\n                )\n                time.sleep(effective_reconnect_delay)\n                continue\n\n            # Reconnect when stream closes naturally, unless disabled.\n            if not reconnect:\n                return\n\n            reconnect_attempts += 1\n            if (\n                max_reconnect_attempts is not None\n                and reconnect_attempts &gt; max_reconnect_attempts\n            ):\n                return\n\n            logger.info(\n                \"SSE stream closed for endpoint '%s'. Reconnecting in %.2fs (attempt %d)\",\n                endpoint_name,\n                effective_reconnect_delay,\n                reconnect_attempts,\n            )\n            time.sleep(effective_reconnect_delay)\n\n    def consume_sse(\n        self,\n        endpoint_name: str,\n        params: Optional[Dict[str, Any]] = None,\n        processor: Optional[Callable[[List[Dict[str, Any]]], Any]] = None,\n        chunk_size: int = 50,\n        max_events: Optional[int] = None,\n        backpressure_buffer_size: int = 500,\n        drop_policy: str = \"block\",\n        **stream_kwargs: Any,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Consume SSE events in chunks with bounded in-memory buffering.\n\n        This method adds chunked processing and explicit backpressure handling:\n        - ``drop_policy='block'``: flushes buffer chunks before accepting more data.\n        - ``drop_policy='drop_oldest'``: drops oldest buffered events when full.\n\n        Args:\n            endpoint_name: Endpoint key configured in ``self.endpoints``.\n            params: Query parameters.\n            processor: Optional callback invoked with each chunk.\n            chunk_size: Number of events per processing chunk.\n            max_events: Maximum number of events to consume.\n            backpressure_buffer_size: Max in-memory event buffer.\n            drop_policy: ``block`` or ``drop_oldest``.\n            **stream_kwargs: Extra kwargs forwarded to ``stream_sse``.\n\n        Returns:\n            Summary dictionary with processing metrics and chunk results.\n        \"\"\"\n        if endpoint_name not in self.endpoints:\n            raise ValueError(f\"Endpoint '{endpoint_name}' not found in configuration\")\n\n        endpoint = self.endpoints[endpoint_name]\n        sse_config = endpoint.sse or {}\n\n        if max_events is None and sse_config.get(\"max_events\") is not None:\n            max_events = int(sse_config[\"max_events\"])\n\n        chunk_size = int(sse_config.get(\"chunk_size\", chunk_size))\n        backpressure_buffer_size = int(\n            sse_config.get(\"backpressure_buffer_size\", backpressure_buffer_size)\n        )\n        drop_policy = str(sse_config.get(\"drop_policy\", drop_policy)).lower()\n\n        if chunk_size &lt;= 0:\n            raise ValueError(\"chunk_size must be &gt; 0\")\n        if backpressure_buffer_size &lt;= 0:\n            raise ValueError(\"backpressure_buffer_size must be &gt; 0\")\n        if drop_policy not in {\"block\", \"drop_oldest\"}:\n            raise ValueError(\"drop_policy must be either 'block' or 'drop_oldest'\")\n\n        buffer: Deque[Dict[str, Any]] = deque(maxlen=backpressure_buffer_size)\n        dropped_events = 0\n        processed_events = 0\n        chunks_processed = 0\n        chunk_results: List[Any] = []\n\n        def _flush(force: bool = False) -&gt; bool:\n            nonlocal processed_events, chunks_processed\n            flushed = False\n\n            while buffer and (force or len(buffer) &gt;= chunk_size):\n                current_chunk_size = (\n                    chunk_size if len(buffer) &gt;= chunk_size else len(buffer)\n                )\n                chunk = [buffer.popleft() for _ in range(current_chunk_size)]\n                chunks_processed += 1\n                processed_events += len(chunk)\n                flushed = True\n\n                if processor:\n                    chunk_results.append(processor(chunk))\n                else:\n                    chunk_results.append(chunk)\n\n                # In non-force mode we flush at most one chunk each step.\n                if not force:\n                    break\n\n            return flushed\n\n        for event in self.stream_sse(\n            endpoint_name=endpoint_name,\n            params=params,\n            max_events=max_events,\n            **stream_kwargs,\n        ):\n            while len(buffer) &gt;= backpressure_buffer_size:\n                if drop_policy == \"drop_oldest\":\n                    buffer.popleft()\n                    dropped_events += 1\n                    break\n\n                # Block strategy: process buffered data before accepting more.\n                if not _flush(force=False):\n                    _flush(force=True)\n                    break\n\n            buffer.append(event)\n            _flush(force=False)\n\n        # Process any trailing events.\n        _flush(force=True)\n\n        return {\n            \"success\": True,\n            \"processed_events\": processed_events,\n            \"chunks_processed\": chunks_processed,\n            \"dropped_events\": dropped_events,\n            \"chunk_size\": chunk_size,\n            \"buffer_max_size\": backpressure_buffer_size,\n            \"results\": chunk_results,\n        }\n\n    def fetch_data(\n        self, endpoint_name: str, params: Optional[Dict[str, Any]] = None\n    ) -&gt; Union[Dict[str, Any], List[Dict[str, Any]]]:\n        \"\"\"\n        Fetch data from the specified endpoint.\n\n        Args:\n            endpoint_name: Name of the endpoint to use\n            params: Additional parameters for the request\n\n        Returns:\n            The parsed response data\n\n        Raises:\n            ApiLinkerError: On API request failure with enhanced error context\n        \"\"\"\n        if endpoint_name not in self.endpoints:\n            raise ValueError(f\"Endpoint '{endpoint_name}' not found in configuration\")\n\n        endpoint = self.endpoints[endpoint_name]\n        logger.info(\n            f\"Fetching data from endpoint: {endpoint_name} ({endpoint.method} {endpoint.path})\"\n        )\n\n        # Prepare the request\n        request = self._prepare_request(endpoint_name, params)\n\n        # Make the request with retries (note: main retries are now handled by the error recovery manager)\n        last_exception = None\n        for attempt in range(1, self.retry_count + 1):\n            try:\n                # Apply rate limiting\n                self.rate_limit_manager.acquire(endpoint_name)\n\n                response = self.client.request(\n                    request[\"method\"],\n                    request[\"url\"],\n                    headers=request[\"headers\"],\n                    params=request[\"params\"],\n                    json=request.get(\"json\"),\n                )\n                response.raise_for_status()\n\n                # Process the response\n                result = self._process_response(response, endpoint_name)\n\n                # Handle pagination if configured\n                if endpoint.pagination:\n                    result = self._handle_pagination(result, endpoint_name, params)\n\n                logger.info(f\"Data fetched successfully from {endpoint_name}\")\n                return result\n\n            except Exception as e:\n                last_exception = e\n                error_category, status_code = self._categorize_error(e)\n\n                # Log the error at an appropriate level\n                if attempt &lt; self.retry_count:\n                    wait_time = self.retry_delay * attempt\n                    logger.warning(\n                        f\"Error fetching data (attempt {attempt}/{self.retry_count}): {str(e)}\"\n                    )\n                    logger.info(f\"Retrying in {wait_time} seconds...\")\n                    time.sleep(wait_time)\n                else:\n                    logger.error(f\"All retry attempts failed: {str(e)}\")\n\n        # If we get here, all retry attempts failed\n        if last_exception:\n            error_category, status_code = self._categorize_error(last_exception)\n\n            # Extract response data if available\n            response_body = None\n            if hasattr(last_exception, \"response\"):\n                try:\n                    response_body = last_exception.response.text[:1000]  # Limit size\n                except:\n                    response_body = \"&lt;Unable to read response body&gt;\"\n\n            # Create an ApiLinkerError with enhanced context\n            raise ApiLinkerError(\n                message=f\"Failed to fetch data from {endpoint_name}: {str(last_exception)}\",\n                error_category=error_category,\n                status_code=status_code,\n                response_body=response_body,\n                request_url=str(request[\"url\"]),\n                request_method=request[\"method\"],\n                additional_context={\"endpoint\": endpoint_name, \"params\": params},\n            )\n\n        # Should not reach here\n        raise ApiLinkerError(\n            message=f\"Unexpected state fetching data from {endpoint_name}\",\n            error_category=ErrorCategory.UNKNOWN,\n            status_code=0,\n        )\n\n    def send_data(\n        self, endpoint_name: str, data: Union[Dict[str, Any], List[Dict[str, Any]]]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Send data to the specified endpoint.\n\n        Args:\n            endpoint_name: Name of the endpoint to use\n            data: Data to send\n\n        Returns:\n            The parsed response\n\n        Raises:\n            ApiLinkerError: On API request failure with enhanced error context\n        \"\"\"\n        if endpoint_name not in self.endpoints:\n            raise ValueError(f\"Endpoint '{endpoint_name}' not found in configuration\")\n\n        endpoint = self.endpoints[endpoint_name]\n        logger.info(\n            f\"Sending data to endpoint: {endpoint_name} ({endpoint.method} {endpoint.path})\"\n        )\n\n        # Prepare the request\n        request = self._prepare_request(endpoint_name)\n\n        # Validate single item(s) against request schema if provided\n        endpoint = self.endpoints[endpoint_name]\n        if endpoint.request_schema and is_validator_available():\n\n            def _validate_item(item: Any) -&gt; None:\n                valid, diffs = validate_payload_against_schema(\n                    item, endpoint.request_schema\n                )\n                if not valid:\n                    logger.error(\n                        \"Request schema validation failed for %s\\n%s\",\n                        endpoint_name,\n                        pretty_print_diffs(diffs),\n                    )\n                    raise ApiLinkerError(\n                        message=\"Request failed schema validation\",\n                        error_category=ErrorCategory.VALIDATION,\n                        status_code=0,\n                        additional_context={\"endpoint\": endpoint_name, \"diffs\": diffs},\n                    )\n\n            if isinstance(data, list):\n                for item in data:\n                    _validate_item(item)\n            else:\n                _validate_item(data)\n\n        # If data is a list, send each item individually\n        if isinstance(data, list):\n            results = []\n            successful = 0\n            failed = 0\n            failures = []\n\n            for item_index, item in enumerate(data):\n                try:\n                    # Apply rate limiting\n                    self.rate_limit_manager.acquire(endpoint_name)\n\n                    response = self.client.request(\n                        request[\"method\"],\n                        request[\"url\"],\n                        headers=request[\"headers\"],\n                        params=request[\"params\"],\n                        json=item,\n                    )\n                    response.raise_for_status()\n                    result = response.json() if response.content else {}\n                    results.append(result)\n                    successful += 1\n\n                except Exception as e:\n                    error_category, status_code = self._categorize_error(e)\n\n                    # Extract response data if available\n                    response_body = None\n                    if hasattr(e, \"response\"):\n                        try:\n                            response_body = e.response.text[:1000]  # Limit size\n                        except:\n                            response_body = \"&lt;Unable to read response body&gt;\"\n\n                    error = ApiLinkerError(\n                        message=f\"Failed to send data item {item_index} to {endpoint_name}: {str(e)}\",\n                        error_category=error_category,\n                        status_code=status_code,\n                        response_body=response_body,\n                        request_url=str(request[\"url\"]),\n                        request_method=request[\"method\"],\n                        additional_context={\n                            \"endpoint\": endpoint_name,\n                            \"item_index\": item_index,\n                        },\n                    )\n                    failures.append(error.to_dict())\n                    logger.error(f\"Error sending data item {item_index}: {error}\")\n                    failed += 1\n\n            logger.info(f\"Sent {successful} items successfully, {failed} failed\")\n            return {\n                \"success\": successful &gt; 0 and failed == 0,\n                \"sent_count\": successful,\n                \"failed_count\": failed,\n                \"results\": results,\n                \"failures\": failures,\n            }\n\n        # If data is a single item, send it\n        else:\n            # Make the request with retries (note: main retries now handled by error recovery manager)\n            last_exception = None\n\n            for attempt in range(1, self.retry_count + 1):\n                try:\n                    # Apply rate limiting\n                    self.rate_limit_manager.acquire(endpoint_name)\n\n                    response = self.client.request(\n                        request[\"method\"],\n                        request[\"url\"],\n                        headers=request[\"headers\"],\n                        params=request[\"params\"],\n                        json=data,\n                    )\n                    response.raise_for_status()\n                    result = response.json() if response.content else {}\n\n                    logger.info(f\"Data sent successfully to {endpoint_name}\")\n                    return {\n                        \"success\": True,\n                        \"result\": result,\n                    }\n\n                except Exception as e:\n                    last_exception = e\n\n                    # Log the error at an appropriate level\n                    if attempt &lt; self.retry_count:\n                        wait_time = self.retry_delay * attempt\n                        logger.warning(\n                            f\"Error sending data (attempt {attempt}/{self.retry_count}): {str(e)}\"\n                        )\n                        logger.info(f\"Retrying in {wait_time} seconds...\")\n                        time.sleep(wait_time)\n                    else:\n                        logger.error(f\"All retry attempts failed: {str(e)}\")\n\n            # If we get here, all retry attempts failed\n            if last_exception:\n                error_category, status_code = self._categorize_error(last_exception)\n\n                # Extract response data if available\n                response_body = None\n                if hasattr(last_exception, \"response\"):\n                    try:\n                        response_body = last_exception.response.text[\n                            :1000\n                        ]  # Limit size\n                    except:\n                        response_body = \"&lt;Unable to read response body&gt;\"\n\n                # Create an ApiLinkerError with enhanced context\n                raise ApiLinkerError(\n                    message=f\"Failed to send data to {endpoint_name}: {str(last_exception)}\",\n                    error_category=error_category,\n                    status_code=status_code,\n                    response_body=response_body,\n                    request_url=str(request[\"url\"]),\n                    request_method=request[\"method\"],\n                    additional_context={\"endpoint\": endpoint_name},\n                )\n\n            # This should not be reached\n            return {\"success\": False, \"error\": \"Unknown error\"}\n\n    def check_health(self) -&gt; HealthCheckResult:\n        \"\"\"\n        Check the health of the API connection.\n\n        Returns:\n            HealthCheckResult indicating the status.\n        \"\"\"\n        start_time = time.time()\n        try:\n            # Try to hit the base URL or a specific health endpoint if we had one configured\n            # For now, just check if we can connect to the base URL\n            response = self.client.get(\"/\", timeout=5.0)\n\n            latency = (time.time() - start_time) * 1000\n\n            status = HealthStatus.HEALTHY\n            message = f\"Connected to {self.base_url}\"\n\n            # If we get a 5xx error, the server is definitely having issues\n            if response.status_code &gt;= 500:\n                status = HealthStatus.UNHEALTHY\n                message = f\"Server returned {response.status_code}\"\n\n            return HealthCheckResult(\n                status=status,\n                component=f\"connector:{self.base_url}\",\n                message=message,\n                latency_ms=latency,\n                details={\"status_code\": response.status_code},\n            )\n\n        except Exception as e:\n            latency = (time.time() - start_time) * 1000\n            return HealthCheckResult(\n                status=HealthStatus.UNHEALTHY,\n                component=f\"connector:{self.base_url}\",\n                message=str(e),\n                latency_ms=latency,\n            )\n</code></pre>"},{"location":"reference/api/#apilinker.core.connector.ApiConnector.__init__","title":"<code>__init__(connector_type, base_url, auth_config=None, endpoints=None, timeout=30, retry_count=3, retry_delay=1, default_headers=None, **kwargs)</code>","text":"Source code in <code>apilinker/core/connector.py</code> <pre><code>def __init__(\n    self,\n    connector_type: str,\n    base_url: str,\n    auth_config: Optional[AuthConfig] = None,\n    endpoints: Optional[Dict[str, Dict[str, Any]]] = None,\n    timeout: int = 30,\n    retry_count: int = 3,\n    retry_delay: int = 1,\n    default_headers: Optional[Dict[str, str]] = None,\n    **kwargs: Any,\n) -&gt; None:\n    self.connector_type = connector_type\n    self.base_url = base_url\n    self.auth_config = auth_config\n    self.timeout = timeout\n    self.retry_count = retry_count\n    self.retry_delay = retry_delay\n\n    # Default headers (may be provided via explicit parameter or legacy 'headers' kwarg)\n    if default_headers is None and \"headers\" in kwargs:\n        try:\n            default_headers = dict(kwargs.pop(\"headers\"))\n        except Exception:\n            default_headers = None\n    self.default_headers: Dict[str, str] = default_headers or {}\n    # Provide a backwards-compatible attribute name used by some connectors\n    self.headers: Dict[str, str] = self.default_headers\n\n    # Parse and store endpoint configurations\n    self.endpoints: Dict[str, EndpointConfig] = {}\n    if endpoints:\n        for name, config in endpoints.items():\n            self.endpoints[name] = EndpointConfig(**config)\n\n    # Store additional settings\n    self.settings: Dict[str, Any] = kwargs\n\n    # Create HTTP client with default settings\n    self.client = self._create_client()\n\n    # Initialize Rate Limit Manager\n    self.rate_limit_manager = RateLimitManager()\n\n    # Configure rate limiters for endpoints\n    for name, config in self.endpoints.items():\n        if config.rate_limit:\n            self.rate_limit_manager.create_limiter(name, config.rate_limit)\n\n    logger.debug(f\"Initialized {connector_type} connector for {base_url}\")\n</code></pre>"},{"location":"reference/api/#apilinker.core.connector.ApiConnector.fetch_data","title":"<code>fetch_data(endpoint_name, params=None)</code>","text":"<p>Fetch data from the specified endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint_name</code> <code>str</code> <p>Name of the endpoint to use</p> required <code>params</code> <code>Optional[Dict[str, Any]]</code> <p>Additional parameters for the request</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> <p>The parsed response data</p> <p>Raises:</p> Type Description <code>ApiLinkerError</code> <p>On API request failure with enhanced error context</p> Source code in <code>apilinker/core/connector.py</code> <pre><code>def fetch_data(\n    self, endpoint_name: str, params: Optional[Dict[str, Any]] = None\n) -&gt; Union[Dict[str, Any], List[Dict[str, Any]]]:\n    \"\"\"\n    Fetch data from the specified endpoint.\n\n    Args:\n        endpoint_name: Name of the endpoint to use\n        params: Additional parameters for the request\n\n    Returns:\n        The parsed response data\n\n    Raises:\n        ApiLinkerError: On API request failure with enhanced error context\n    \"\"\"\n    if endpoint_name not in self.endpoints:\n        raise ValueError(f\"Endpoint '{endpoint_name}' not found in configuration\")\n\n    endpoint = self.endpoints[endpoint_name]\n    logger.info(\n        f\"Fetching data from endpoint: {endpoint_name} ({endpoint.method} {endpoint.path})\"\n    )\n\n    # Prepare the request\n    request = self._prepare_request(endpoint_name, params)\n\n    # Make the request with retries (note: main retries are now handled by the error recovery manager)\n    last_exception = None\n    for attempt in range(1, self.retry_count + 1):\n        try:\n            # Apply rate limiting\n            self.rate_limit_manager.acquire(endpoint_name)\n\n            response = self.client.request(\n                request[\"method\"],\n                request[\"url\"],\n                headers=request[\"headers\"],\n                params=request[\"params\"],\n                json=request.get(\"json\"),\n            )\n            response.raise_for_status()\n\n            # Process the response\n            result = self._process_response(response, endpoint_name)\n\n            # Handle pagination if configured\n            if endpoint.pagination:\n                result = self._handle_pagination(result, endpoint_name, params)\n\n            logger.info(f\"Data fetched successfully from {endpoint_name}\")\n            return result\n\n        except Exception as e:\n            last_exception = e\n            error_category, status_code = self._categorize_error(e)\n\n            # Log the error at an appropriate level\n            if attempt &lt; self.retry_count:\n                wait_time = self.retry_delay * attempt\n                logger.warning(\n                    f\"Error fetching data (attempt {attempt}/{self.retry_count}): {str(e)}\"\n                )\n                logger.info(f\"Retrying in {wait_time} seconds...\")\n                time.sleep(wait_time)\n            else:\n                logger.error(f\"All retry attempts failed: {str(e)}\")\n\n    # If we get here, all retry attempts failed\n    if last_exception:\n        error_category, status_code = self._categorize_error(last_exception)\n\n        # Extract response data if available\n        response_body = None\n        if hasattr(last_exception, \"response\"):\n            try:\n                response_body = last_exception.response.text[:1000]  # Limit size\n            except:\n                response_body = \"&lt;Unable to read response body&gt;\"\n\n        # Create an ApiLinkerError with enhanced context\n        raise ApiLinkerError(\n            message=f\"Failed to fetch data from {endpoint_name}: {str(last_exception)}\",\n            error_category=error_category,\n            status_code=status_code,\n            response_body=response_body,\n            request_url=str(request[\"url\"]),\n            request_method=request[\"method\"],\n            additional_context={\"endpoint\": endpoint_name, \"params\": params},\n        )\n\n    # Should not reach here\n    raise ApiLinkerError(\n        message=f\"Unexpected state fetching data from {endpoint_name}\",\n        error_category=ErrorCategory.UNKNOWN,\n        status_code=0,\n    )\n</code></pre>"},{"location":"reference/api/#apilinker.core.connector.ApiConnector.send_data","title":"<code>send_data(endpoint_name, data)</code>","text":"<p>Send data to the specified endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint_name</code> <code>str</code> <p>Name of the endpoint to use</p> required <code>data</code> <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> <p>Data to send</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The parsed response</p> <p>Raises:</p> Type Description <code>ApiLinkerError</code> <p>On API request failure with enhanced error context</p> Source code in <code>apilinker/core/connector.py</code> <pre><code>def send_data(\n    self, endpoint_name: str, data: Union[Dict[str, Any], List[Dict[str, Any]]]\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Send data to the specified endpoint.\n\n    Args:\n        endpoint_name: Name of the endpoint to use\n        data: Data to send\n\n    Returns:\n        The parsed response\n\n    Raises:\n        ApiLinkerError: On API request failure with enhanced error context\n    \"\"\"\n    if endpoint_name not in self.endpoints:\n        raise ValueError(f\"Endpoint '{endpoint_name}' not found in configuration\")\n\n    endpoint = self.endpoints[endpoint_name]\n    logger.info(\n        f\"Sending data to endpoint: {endpoint_name} ({endpoint.method} {endpoint.path})\"\n    )\n\n    # Prepare the request\n    request = self._prepare_request(endpoint_name)\n\n    # Validate single item(s) against request schema if provided\n    endpoint = self.endpoints[endpoint_name]\n    if endpoint.request_schema and is_validator_available():\n\n        def _validate_item(item: Any) -&gt; None:\n            valid, diffs = validate_payload_against_schema(\n                item, endpoint.request_schema\n            )\n            if not valid:\n                logger.error(\n                    \"Request schema validation failed for %s\\n%s\",\n                    endpoint_name,\n                    pretty_print_diffs(diffs),\n                )\n                raise ApiLinkerError(\n                    message=\"Request failed schema validation\",\n                    error_category=ErrorCategory.VALIDATION,\n                    status_code=0,\n                    additional_context={\"endpoint\": endpoint_name, \"diffs\": diffs},\n                )\n\n        if isinstance(data, list):\n            for item in data:\n                _validate_item(item)\n        else:\n            _validate_item(data)\n\n    # If data is a list, send each item individually\n    if isinstance(data, list):\n        results = []\n        successful = 0\n        failed = 0\n        failures = []\n\n        for item_index, item in enumerate(data):\n            try:\n                # Apply rate limiting\n                self.rate_limit_manager.acquire(endpoint_name)\n\n                response = self.client.request(\n                    request[\"method\"],\n                    request[\"url\"],\n                    headers=request[\"headers\"],\n                    params=request[\"params\"],\n                    json=item,\n                )\n                response.raise_for_status()\n                result = response.json() if response.content else {}\n                results.append(result)\n                successful += 1\n\n            except Exception as e:\n                error_category, status_code = self._categorize_error(e)\n\n                # Extract response data if available\n                response_body = None\n                if hasattr(e, \"response\"):\n                    try:\n                        response_body = e.response.text[:1000]  # Limit size\n                    except:\n                        response_body = \"&lt;Unable to read response body&gt;\"\n\n                error = ApiLinkerError(\n                    message=f\"Failed to send data item {item_index} to {endpoint_name}: {str(e)}\",\n                    error_category=error_category,\n                    status_code=status_code,\n                    response_body=response_body,\n                    request_url=str(request[\"url\"]),\n                    request_method=request[\"method\"],\n                    additional_context={\n                        \"endpoint\": endpoint_name,\n                        \"item_index\": item_index,\n                    },\n                )\n                failures.append(error.to_dict())\n                logger.error(f\"Error sending data item {item_index}: {error}\")\n                failed += 1\n\n        logger.info(f\"Sent {successful} items successfully, {failed} failed\")\n        return {\n            \"success\": successful &gt; 0 and failed == 0,\n            \"sent_count\": successful,\n            \"failed_count\": failed,\n            \"results\": results,\n            \"failures\": failures,\n        }\n\n    # If data is a single item, send it\n    else:\n        # Make the request with retries (note: main retries now handled by error recovery manager)\n        last_exception = None\n\n        for attempt in range(1, self.retry_count + 1):\n            try:\n                # Apply rate limiting\n                self.rate_limit_manager.acquire(endpoint_name)\n\n                response = self.client.request(\n                    request[\"method\"],\n                    request[\"url\"],\n                    headers=request[\"headers\"],\n                    params=request[\"params\"],\n                    json=data,\n                )\n                response.raise_for_status()\n                result = response.json() if response.content else {}\n\n                logger.info(f\"Data sent successfully to {endpoint_name}\")\n                return {\n                    \"success\": True,\n                    \"result\": result,\n                }\n\n            except Exception as e:\n                last_exception = e\n\n                # Log the error at an appropriate level\n                if attempt &lt; self.retry_count:\n                    wait_time = self.retry_delay * attempt\n                    logger.warning(\n                        f\"Error sending data (attempt {attempt}/{self.retry_count}): {str(e)}\"\n                    )\n                    logger.info(f\"Retrying in {wait_time} seconds...\")\n                    time.sleep(wait_time)\n                else:\n                    logger.error(f\"All retry attempts failed: {str(e)}\")\n\n        # If we get here, all retry attempts failed\n        if last_exception:\n            error_category, status_code = self._categorize_error(last_exception)\n\n            # Extract response data if available\n            response_body = None\n            if hasattr(last_exception, \"response\"):\n                try:\n                    response_body = last_exception.response.text[\n                        :1000\n                    ]  # Limit size\n                except:\n                    response_body = \"&lt;Unable to read response body&gt;\"\n\n            # Create an ApiLinkerError with enhanced context\n            raise ApiLinkerError(\n                message=f\"Failed to send data to {endpoint_name}: {str(last_exception)}\",\n                error_category=error_category,\n                status_code=status_code,\n                response_body=response_body,\n                request_url=str(request[\"url\"]),\n                request_method=request[\"method\"],\n                additional_context={\"endpoint\": endpoint_name},\n            )\n\n        # This should not be reached\n        return {\"success\": False, \"error\": \"Unknown error\"}\n</code></pre>"},{"location":"reference/api/#apilinker.core.connector.ApiConnector.stream_sse","title":"<code>stream_sse(endpoint_name, params=None, max_events=None, reconnect=True, reconnect_delay=1.0, max_reconnect_attempts=None, read_timeout=None, decode_json=True)</code>","text":"<p>Stream events from an SSE endpoint.</p> <p>Supports automatic reconnection, Last-Event-ID resume, and optional JSON decoding of event payloads.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint_name</code> <code>str</code> <p>Endpoint key configured in <code>self.endpoints</code>.</p> required <code>params</code> <code>Optional[Dict[str, Any]]</code> <p>Query parameters.</p> <code>None</code> <code>max_events</code> <code>Optional[int]</code> <p>Stop after this many dispatched events.</p> <code>None</code> <code>reconnect</code> <code>bool</code> <p>Whether to reconnect on disconnect/error.</p> <code>True</code> <code>reconnect_delay</code> <code>float</code> <p>Base reconnect delay in seconds.</p> <code>1.0</code> <code>max_reconnect_attempts</code> <code>Optional[int]</code> <p>Maximum reconnect attempts (None = unlimited).</p> <code>None</code> <code>read_timeout</code> <code>Optional[float]</code> <p>Per-stream read timeout. Defaults to connector timeout.</p> <code>None</code> <code>decode_json</code> <code>bool</code> <p>Whether to decode JSON payloads automatically.</p> <code>True</code> <p>Yields:</p> Type Description <code>Dict[str, Any]</code> <p>Parsed SSE events with keys: <code>id</code>, <code>event</code>, <code>data</code>, <code>retry</code>, <code>raw_data</code>.</p> Source code in <code>apilinker/core/connector.py</code> <pre><code>def stream_sse(\n    self,\n    endpoint_name: str,\n    params: Optional[Dict[str, Any]] = None,\n    max_events: Optional[int] = None,\n    reconnect: bool = True,\n    reconnect_delay: float = 1.0,\n    max_reconnect_attempts: Optional[int] = None,\n    read_timeout: Optional[float] = None,\n    decode_json: bool = True,\n) -&gt; Generator[Dict[str, Any], None, None]:\n    \"\"\"\n    Stream events from an SSE endpoint.\n\n    Supports automatic reconnection, Last-Event-ID resume, and optional\n    JSON decoding of event payloads.\n\n    Args:\n        endpoint_name: Endpoint key configured in ``self.endpoints``.\n        params: Query parameters.\n        max_events: Stop after this many dispatched events.\n        reconnect: Whether to reconnect on disconnect/error.\n        reconnect_delay: Base reconnect delay in seconds.\n        max_reconnect_attempts: Maximum reconnect attempts (None = unlimited).\n        read_timeout: Per-stream read timeout. Defaults to connector timeout.\n        decode_json: Whether to decode JSON payloads automatically.\n\n    Yields:\n        Parsed SSE events with keys: ``id``, ``event``, ``data``, ``retry``, ``raw_data``.\n    \"\"\"\n    if endpoint_name not in self.endpoints:\n        raise ValueError(f\"Endpoint '{endpoint_name}' not found in configuration\")\n\n    endpoint = self.endpoints[endpoint_name]\n    sse_config = endpoint.sse or {}\n\n    if max_events is None and sse_config.get(\"max_events\") is not None:\n        max_events = int(sse_config[\"max_events\"])\n\n    reconnect = bool(sse_config.get(\"reconnect\", reconnect))\n    reconnect_delay = float(sse_config.get(\"reconnect_delay\", reconnect_delay))\n\n    if (\n        max_reconnect_attempts is None\n        and sse_config.get(\"max_reconnect_attempts\") is not None\n    ):\n        max_reconnect_attempts = int(sse_config[\"max_reconnect_attempts\"])\n\n    if read_timeout is None:\n        read_timeout = sse_config.get(\"read_timeout\")\n    if read_timeout is None:\n        read_timeout = self.timeout\n\n    decode_json = bool(sse_config.get(\"decode_json\", decode_json))\n\n    reconnect_attempts = 0\n    received_events = 0\n    last_event_id: Optional[str] = None\n    effective_reconnect_delay = max(0.0, reconnect_delay)\n\n    while True:\n        request = self._build_sse_request(endpoint_name, params, last_event_id)\n        try:\n            # Apply endpoint-level rate limiting before opening/renewing stream\n            self.rate_limit_manager.acquire(endpoint_name)\n\n            with self.client.stream(\n                request[\"method\"],\n                request[\"url\"],\n                headers=request[\"headers\"],\n                params=request[\"params\"],\n                json=request.get(\"json\"),\n                timeout=read_timeout,\n            ) as response:\n                self.rate_limit_manager.update_from_response(\n                    endpoint_name, response\n                )\n                response.raise_for_status()\n                logger.info(\n                    \"Connected to SSE endpoint: %s (%s %s)\",\n                    endpoint_name,\n                    request[\"method\"],\n                    request[\"url\"],\n                )\n\n                pending_lines: List[str] = []\n\n                def _dispatch_event(\n                    event: Dict[str, Any],\n                ) -&gt; Optional[Dict[str, Any]]:\n                    nonlocal effective_reconnect_delay, last_event_id, received_events\n\n                    retry_hint = event.get(\"retry\")\n                    if isinstance(retry_hint, int):\n                        effective_reconnect_delay = max(\n                            0.0, float(retry_hint) / 1000.0\n                        )\n\n                    event_id = event.get(\"id\")\n                    if event_id:\n                        last_event_id = str(event_id)\n\n                    if not event.get(\"has_data\"):\n                        # Retry/id-only control blocks are not dispatched\n                        return None\n\n                    event.pop(\"has_data\", None)\n                    received_events += 1\n                    return event\n\n                for raw_line in response.iter_lines():\n                    if raw_line is None:\n                        continue\n\n                    line = raw_line.rstrip(\"\\r\")\n\n                    if line == \"\":\n                        parsed = self._parse_sse_event(\n                            pending_lines, decode_json=decode_json\n                        )\n                        pending_lines = []\n\n                        if parsed is None:\n                            continue\n\n                        event = _dispatch_event(parsed)\n                        if event is None:\n                            continue\n\n                        yield event\n                        if max_events is not None and received_events &gt;= max_events:\n                            return\n                    else:\n                        pending_lines.append(line)\n\n                # Flush trailing buffered event block on connection close\n                if pending_lines:\n                    parsed = self._parse_sse_event(\n                        pending_lines, decode_json=decode_json\n                    )\n                    if parsed is not None:\n                        event = _dispatch_event(parsed)\n                        if event is not None:\n                            yield event\n                            if (\n                                max_events is not None\n                                and received_events &gt;= max_events\n                            ):\n                                return\n\n        except Exception as exc:\n            if not reconnect:\n                self._raise_sse_error(exc, endpoint_name, request)\n\n            reconnect_attempts += 1\n            if (\n                max_reconnect_attempts is not None\n                and reconnect_attempts &gt; max_reconnect_attempts\n            ):\n                self._raise_sse_error(exc, endpoint_name, request)\n\n            logger.warning(\n                \"SSE stream error on endpoint '%s': %s. Reconnecting in %.2fs (attempt %d)\",\n                endpoint_name,\n                str(exc),\n                effective_reconnect_delay,\n                reconnect_attempts,\n            )\n            time.sleep(effective_reconnect_delay)\n            continue\n\n        # Reconnect when stream closes naturally, unless disabled.\n        if not reconnect:\n            return\n\n        reconnect_attempts += 1\n        if (\n            max_reconnect_attempts is not None\n            and reconnect_attempts &gt; max_reconnect_attempts\n        ):\n            return\n\n        logger.info(\n            \"SSE stream closed for endpoint '%s'. Reconnecting in %.2fs (attempt %d)\",\n            endpoint_name,\n            effective_reconnect_delay,\n            reconnect_attempts,\n        )\n        time.sleep(effective_reconnect_delay)\n</code></pre>"},{"location":"reference/api/#apilinker.core.connector.ApiConnector.consume_sse","title":"<code>consume_sse(endpoint_name, params=None, processor=None, chunk_size=50, max_events=None, backpressure_buffer_size=500, drop_policy='block', **stream_kwargs)</code>","text":"<p>Consume SSE events in chunks with bounded in-memory buffering.</p> <p>This method adds chunked processing and explicit backpressure handling: - <code>drop_policy='block'</code>: flushes buffer chunks before accepting more data. - <code>drop_policy='drop_oldest'</code>: drops oldest buffered events when full.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint_name</code> <code>str</code> <p>Endpoint key configured in <code>self.endpoints</code>.</p> required <code>params</code> <code>Optional[Dict[str, Any]]</code> <p>Query parameters.</p> <code>None</code> <code>processor</code> <code>Optional[Callable[[List[Dict[str, Any]]], Any]]</code> <p>Optional callback invoked with each chunk.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>Number of events per processing chunk.</p> <code>50</code> <code>max_events</code> <code>Optional[int]</code> <p>Maximum number of events to consume.</p> <code>None</code> <code>backpressure_buffer_size</code> <code>int</code> <p>Max in-memory event buffer.</p> <code>500</code> <code>drop_policy</code> <code>str</code> <p><code>block</code> or <code>drop_oldest</code>.</p> <code>'block'</code> <code>**stream_kwargs</code> <code>Any</code> <p>Extra kwargs forwarded to <code>stream_sse</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Summary dictionary with processing metrics and chunk results.</p> Source code in <code>apilinker/core/connector.py</code> <pre><code>def consume_sse(\n    self,\n    endpoint_name: str,\n    params: Optional[Dict[str, Any]] = None,\n    processor: Optional[Callable[[List[Dict[str, Any]]], Any]] = None,\n    chunk_size: int = 50,\n    max_events: Optional[int] = None,\n    backpressure_buffer_size: int = 500,\n    drop_policy: str = \"block\",\n    **stream_kwargs: Any,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Consume SSE events in chunks with bounded in-memory buffering.\n\n    This method adds chunked processing and explicit backpressure handling:\n    - ``drop_policy='block'``: flushes buffer chunks before accepting more data.\n    - ``drop_policy='drop_oldest'``: drops oldest buffered events when full.\n\n    Args:\n        endpoint_name: Endpoint key configured in ``self.endpoints``.\n        params: Query parameters.\n        processor: Optional callback invoked with each chunk.\n        chunk_size: Number of events per processing chunk.\n        max_events: Maximum number of events to consume.\n        backpressure_buffer_size: Max in-memory event buffer.\n        drop_policy: ``block`` or ``drop_oldest``.\n        **stream_kwargs: Extra kwargs forwarded to ``stream_sse``.\n\n    Returns:\n        Summary dictionary with processing metrics and chunk results.\n    \"\"\"\n    if endpoint_name not in self.endpoints:\n        raise ValueError(f\"Endpoint '{endpoint_name}' not found in configuration\")\n\n    endpoint = self.endpoints[endpoint_name]\n    sse_config = endpoint.sse or {}\n\n    if max_events is None and sse_config.get(\"max_events\") is not None:\n        max_events = int(sse_config[\"max_events\"])\n\n    chunk_size = int(sse_config.get(\"chunk_size\", chunk_size))\n    backpressure_buffer_size = int(\n        sse_config.get(\"backpressure_buffer_size\", backpressure_buffer_size)\n    )\n    drop_policy = str(sse_config.get(\"drop_policy\", drop_policy)).lower()\n\n    if chunk_size &lt;= 0:\n        raise ValueError(\"chunk_size must be &gt; 0\")\n    if backpressure_buffer_size &lt;= 0:\n        raise ValueError(\"backpressure_buffer_size must be &gt; 0\")\n    if drop_policy not in {\"block\", \"drop_oldest\"}:\n        raise ValueError(\"drop_policy must be either 'block' or 'drop_oldest'\")\n\n    buffer: Deque[Dict[str, Any]] = deque(maxlen=backpressure_buffer_size)\n    dropped_events = 0\n    processed_events = 0\n    chunks_processed = 0\n    chunk_results: List[Any] = []\n\n    def _flush(force: bool = False) -&gt; bool:\n        nonlocal processed_events, chunks_processed\n        flushed = False\n\n        while buffer and (force or len(buffer) &gt;= chunk_size):\n            current_chunk_size = (\n                chunk_size if len(buffer) &gt;= chunk_size else len(buffer)\n            )\n            chunk = [buffer.popleft() for _ in range(current_chunk_size)]\n            chunks_processed += 1\n            processed_events += len(chunk)\n            flushed = True\n\n            if processor:\n                chunk_results.append(processor(chunk))\n            else:\n                chunk_results.append(chunk)\n\n            # In non-force mode we flush at most one chunk each step.\n            if not force:\n                break\n\n        return flushed\n\n    for event in self.stream_sse(\n        endpoint_name=endpoint_name,\n        params=params,\n        max_events=max_events,\n        **stream_kwargs,\n    ):\n        while len(buffer) &gt;= backpressure_buffer_size:\n            if drop_policy == \"drop_oldest\":\n                buffer.popleft()\n                dropped_events += 1\n                break\n\n            # Block strategy: process buffered data before accepting more.\n            if not _flush(force=False):\n                _flush(force=True)\n                break\n\n        buffer.append(event)\n        _flush(force=False)\n\n    # Process any trailing events.\n    _flush(force=True)\n\n    return {\n        \"success\": True,\n        \"processed_events\": processed_events,\n        \"chunks_processed\": chunks_processed,\n        \"dropped_events\": dropped_events,\n        \"chunk_size\": chunk_size,\n        \"buffer_max_size\": backpressure_buffer_size,\n        \"results\": chunk_results,\n    }\n</code></pre>"},{"location":"reference/api/#apilinker.core.connector.ApiConnector.check_health","title":"<code>check_health()</code>","text":"<p>Check the health of the API connection.</p> <p>Returns:</p> Type Description <code>HealthCheckResult</code> <p>HealthCheckResult indicating the status.</p> Source code in <code>apilinker/core/connector.py</code> <pre><code>def check_health(self) -&gt; HealthCheckResult:\n    \"\"\"\n    Check the health of the API connection.\n\n    Returns:\n        HealthCheckResult indicating the status.\n    \"\"\"\n    start_time = time.time()\n    try:\n        # Try to hit the base URL or a specific health endpoint if we had one configured\n        # For now, just check if we can connect to the base URL\n        response = self.client.get(\"/\", timeout=5.0)\n\n        latency = (time.time() - start_time) * 1000\n\n        status = HealthStatus.HEALTHY\n        message = f\"Connected to {self.base_url}\"\n\n        # If we get a 5xx error, the server is definitely having issues\n        if response.status_code &gt;= 500:\n            status = HealthStatus.UNHEALTHY\n            message = f\"Server returned {response.status_code}\"\n\n        return HealthCheckResult(\n            status=status,\n            component=f\"connector:{self.base_url}\",\n            message=message,\n            latency_ms=latency,\n            details={\"status_code\": response.status_code},\n        )\n\n    except Exception as e:\n        latency = (time.time() - start_time) * 1000\n        return HealthCheckResult(\n            status=HealthStatus.UNHEALTHY,\n            component=f\"connector:{self.base_url}\",\n            message=str(e),\n            latency_ms=latency,\n        )\n</code></pre>"},{"location":"reference/api/#fieldmapper","title":"FieldMapper","text":"<p>Data transformation and field mapping engine.</p>"},{"location":"reference/api/#apilinker.core.mapper.FieldMapper","title":"<code>apilinker.core.mapper.FieldMapper</code>","text":"<p>Maps fields between source and target APIs, including transformations.</p> <p>This class handles the mapping of fields from source to target format, including nested fields, transformations, and filtering.</p> Source code in <code>apilinker/core/mapper.py</code> <pre><code>class FieldMapper:\n    \"\"\"\n    Maps fields between source and target APIs, including transformations.\n\n    This class handles the mapping of fields from source to target format,\n    including nested fields, transformations, and filtering.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        self.mappings: List[Dict[str, Any]] = []\n        self.transformers: Dict[str, Callable] = self._register_built_in_transformers()\n        logger.debug(\"Initialized FieldMapper\")\n\n    def _register_built_in_transformers(self) -&gt; Dict[str, Callable]:\n        \"\"\"Register built-in transformation functions.\"\"\"\n        return {\n            # Date/time transformations\n            \"iso_to_timestamp\": lambda v: (\n                int(datetime.fromisoformat(v.replace(\"Z\", \"+00:00\")).timestamp())\n                if v\n                else None\n            ),\n            \"timestamp_to_iso\": lambda v: (\n                datetime.fromtimestamp(int(v)).isoformat() if v else None\n            ),\n            # String transformations\n            \"lowercase\": lambda v: v.lower() if v else None,\n            \"uppercase\": lambda v: v.upper() if v else None,\n            \"strip\": lambda v: v.strip() if v else None,\n            # Type conversions\n            \"to_string\": lambda v: str(v) if v is not None else \"\",\n            \"to_int\": lambda v: int(float(v)) if v else 0,\n            \"to_float\": lambda v: float(v) if v else 0.0,\n            \"to_bool\": lambda v: bool(v) if v is not None else False,\n            # Null handling\n            \"default_empty_string\": lambda v: v if v is not None else \"\",\n            \"default_zero\": lambda v: v if v is not None else 0,\n            \"none_if_empty\": lambda v: v if v else None,\n        }\n\n    def register_transformer(self, name: str, func: Callable) -&gt; None:\n        \"\"\"\n        Register a custom transformation function.\n\n        Args:\n            name: Name of the transformer\n            func: Function that performs the transformation\n        \"\"\"\n        if name in self.transformers:\n            logger.warning(f\"Overwriting existing transformer: {name}\")\n        self.transformers[name] = func\n        logger.debug(f\"Registered transformer: {name}\")\n\n    def transform(self, value: Any, transformer: Union[str, Dict[str, Any]]) -&gt; Any:\n        \"\"\"\n        Convenience wrapper to apply a single transformer to a value.\n\n        Args:\n            value: Input value\n            transformer: Transformer name or transformer dict {name, params}\n\n        Returns:\n            Transformed value\n        \"\"\"\n        return self.apply_transform(value, transformer)\n\n    def load_custom_transformer(\n        self, module_path: str, function_name: str, alias: Optional[str] = None\n    ) -&gt; None:\n        \"\"\"\n        Load a custom transformer function from a Python module.\n\n        Args:\n            module_path: Import path for the module\n            function_name: Name of the function in the module\n            alias: Optional alternative name to register the transformer under\n        \"\"\"\n        try:\n            module = importlib.import_module(module_path)\n            func = getattr(module, function_name)\n\n            # Register with the provided alias or the original function name\n            name = alias or function_name\n            self.register_transformer(name, func)\n            logger.info(\n                f\"Loaded custom transformer from {module_path}.{function_name} as {name}\"\n            )\n\n        except (ImportError, AttributeError) as e:\n            logger.error(f\"Error loading custom transformer: {str(e)}\")\n            raise ValueError(\n                f\"Failed to load transformer from {module_path}.{function_name}: {str(e)}\"\n            )\n\n    def add_mapping(\n        self, source: str, target: str, fields: List[Dict[str, Any]]\n    ) -&gt; None:\n        \"\"\"\n        Add a field mapping between source and target endpoints.\n\n        Args:\n            source: Source endpoint name\n            target: Target endpoint name\n            fields: List of field mappings (dicts with source, target, and optional transform)\n        \"\"\"\n        # Validate that required fields are present\n        for field in fields:\n            if \"source\" not in field or \"target\" not in field:\n                raise ValueError(\n                    \"Field mapping must contain 'source' and 'target' keys\"\n                )\n\n        self.mappings.append({\"source\": source, \"target\": target, \"fields\": fields})\n        logger.debug(\n            f\"Added mapping from {source} to {target} with {len(fields)} fields\"\n        )\n\n    def get_mappings(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Get all defined mappings.\"\"\"\n        return self.mappings\n\n    def get_first_mapping(self) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Get the first mapping if any exists.\"\"\"\n        return self.mappings[0] if self.mappings else None\n\n    def get_value_from_path(self, data: Dict[str, Any], path: str) -&gt; Any:\n        \"\"\"\n        Extract a value from nested dictionary using dot notation.\n\n        Args:\n            data: Source data dictionary\n            path: Path to the value using dot notation (e.g., \"user.address.city\")\n\n        Returns:\n            The value at the specified path or None if not found\n        \"\"\"\n        if not path:\n            return data\n\n        # Handle array indexing in path (e.g., \"items[0].name\")\n        parts: List[Union[str, int]] = []\n        current_part = \"\"\n        i = 0\n        while i &lt; len(path):\n            if path[i] == \"[\":\n                if current_part:\n                    parts.append(current_part)\n                    current_part = \"\"\n\n                # Extract the array index\n                i += 1\n                index_str = \"\"\n                while i &lt; len(path) and path[i] != \"]\":\n                    index_str += path[i]\n                    i += 1\n\n                # Add the index as a separate part\n                if index_str.isdigit():\n                    parts.append(int(index_str))\n                i += 1  # Skip the closing bracket\n            elif path[i] == \".\":\n                if current_part:\n                    parts.append(current_part)\n                    current_part = \"\"\n                i += 1\n            else:\n                current_part += path[i]\n                i += 1\n\n        if current_part:\n            parts.append(current_part)\n\n        # Navigate through the path\n        current: Any = data\n        for part in parts:\n            if isinstance(current, dict) and isinstance(part, str):\n                if part in current:\n                    current = current[part]\n                else:\n                    return None\n            elif isinstance(current, list) and isinstance(part, int):\n                if 0 &lt;= part &lt; len(current):\n                    current = current[part]\n                else:\n                    return None\n            else:\n                return None\n\n        return current\n\n    def set_value_at_path(self, data: Dict[str, Any], path: str, value: Any) -&gt; None:\n        \"\"\"\n        Set a value in a nested dictionary using dot notation.\n\n        Args:\n            data: Target data dictionary\n            path: Path where to set the value using dot notation\n            value: Value to set\n        \"\"\"\n        if not path:\n            return\n\n        parts = path.split(\".\")\n\n        # Navigate to the parent of the leaf node\n        current = data\n        for i, part in enumerate(parts[:-1]):\n            if part not in current:\n                current[part] = {}\n            current = current[part]\n\n        # Set the value at the leaf node\n        current[parts[-1]] = value\n\n    def apply_transform(\n        self,\n        value: Any,\n        transform: Union[str, List[str], Dict[str, Any]],\n        params: Dict[str, Any] = None,\n    ) -&gt; Any:\n        \"\"\"\n        Apply a transformation to a value.\n\n        Args:\n            value: The value to transform\n            transform: The transformation to apply (string name or list of transforms)\n            params: Optional parameters for the transformer\n\n        Returns:\n            Transformed value\n        \"\"\"\n        # Handle None values\n        if value is None:\n            # Special case transformers that handle None\n            if transform == \"default_empty_string\":\n                return \"\"\n            elif transform == \"default_zero\":\n                return 0\n            else:\n                return None\n\n        # Handle empty params\n        if params is None:\n            params = {}\n\n        # Handle list of transformations\n        if isinstance(transform, list):\n            result = value\n            for t in transform:\n                result = self.apply_transform(result, t)\n            return result\n\n        # Handle transformation with parameters\n        if isinstance(transform, dict):\n            name = transform.get(\"name\")\n            transform_params = transform.get(\"params\", {})\n\n            if not name or name not in self.transformers:\n                logger.warning(f\"Unknown transformer: {name}\")\n                return value\n\n            return self.transformers[name](value, **transform_params)\n\n        # Simple string transformer name\n        if transform not in self.transformers:\n            logger.warning(f\"Unknown transformer: {transform}\")\n            return value\n\n        # Special handling for list inputs with certain transforms\n        if isinstance(value, list) and transform in [\"lowercase\", \"uppercase\", \"strip\"]:\n            return [\n                self.transformers[transform](item) if isinstance(item, str) else item\n                for item in value\n            ]\n\n        # Apply the transformation with parameters if provided\n        if params:\n            return self.transformers[transform](value, **params)\n        else:\n            return self.transformers[transform](value)\n\n    def map_data(\n        self,\n        source_endpoint: str,\n        target_endpoint: str,\n        data: Union[Dict[str, Any], List[Dict[str, Any]]],\n    ) -&gt; Union[Dict[str, Any], List[Dict[str, Any]]]:\n        \"\"\"\n        Map data from source format to target format.\n\n        Args:\n            source_endpoint: Source endpoint name\n            target_endpoint: Target endpoint name\n            data: Data to map\n\n        Returns:\n            Mapped data in target format\n        \"\"\"\n        # Find the appropriate mapping\n        mapping = None\n        for m in self.mappings:\n            if m[\"source\"] == source_endpoint and m[\"target\"] == target_endpoint:\n                mapping = m\n                break\n\n        if not mapping:\n            logger.warning(\n                f\"No mapping found for {source_endpoint} to {target_endpoint}\"\n            )\n            return data\n\n        logger.debug(f\"Mapping data using {len(mapping['fields'])} field mappings\")\n\n        # Handle list of items\n        if isinstance(data, list):\n            return [self._map_item(item, mapping[\"fields\"]) for item in data]\n\n        # Handle single item\n        return self._map_item(data, mapping[\"fields\"])\n\n    def _map_item(\n        self, item: Dict[str, Any], fields: List[Dict[str, Any]]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Map a single data item according to field mappings.\n\n        Args:\n            item: Source data item\n            fields: Field mapping configurations\n\n        Returns:\n            Mapped data item\n        \"\"\"\n        result: Dict[str, Any] = {}\n\n        for field in fields:\n            source_path = field[\"source\"]\n            target_path = field[\"target\"]\n\n            # Get source value\n            source_value = self.get_value_from_path(item, source_path)\n\n            # Apply transformation if specified\n            if \"transform\" in field and field[\"transform\"]:\n                transform = field[\"transform\"]\n                if isinstance(transform, str):\n                    # Single transformation\n                    source_value = self.apply_transform(source_value, transform)\n                elif isinstance(transform, list):\n                    # Multiple transformations in sequence\n                    for t in transform:\n                        source_value = self.apply_transform(source_value, t)\n                else:\n                    logger.warning(f\"Unknown transform format: {transform}\")\n\n            # Check conditions if specified\n            if \"condition\" in field:\n                condition = field[\"condition\"]\n                if condition.get(\"field\"):\n                    condition_value = self.get_value_from_path(item, condition[\"field\"])\n                    operator = condition.get(\"operator\", \"eq\")\n                    compare_value = condition.get(\"value\")\n\n                    skip = False\n                    if operator == \"eq\" and condition_value != compare_value:\n                        skip = True\n                    elif operator == \"ne\" and condition_value == compare_value:\n                        skip = True\n                    elif operator == \"gt\" and not (condition_value &gt; compare_value):\n                        skip = True\n                    elif operator == \"lt\" and not (condition_value &lt; compare_value):\n                        skip = True\n                    elif operator == \"exists\" and condition_value is None:\n                        skip = True\n                    elif operator == \"not_exists\" and condition_value is not None:\n                        skip = True\n\n                    if skip:\n                        continue\n\n            # Set value in result\n            if source_value is not None or field.get(\"include_nulls\", False):\n                self.set_value_at_path(result, target_path, source_value)\n\n        return result\n</code></pre>"},{"location":"reference/api/#apilinker.core.mapper.FieldMapper.add_mapping","title":"<code>add_mapping(source, target, fields)</code>","text":"<p>Add a field mapping between source and target endpoints.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Source endpoint name</p> required <code>target</code> <code>str</code> <p>Target endpoint name</p> required <code>fields</code> <code>List[Dict[str, Any]]</code> <p>List of field mappings (dicts with source, target, and optional transform)</p> required Source code in <code>apilinker/core/mapper.py</code> <pre><code>def add_mapping(\n    self, source: str, target: str, fields: List[Dict[str, Any]]\n) -&gt; None:\n    \"\"\"\n    Add a field mapping between source and target endpoints.\n\n    Args:\n        source: Source endpoint name\n        target: Target endpoint name\n        fields: List of field mappings (dicts with source, target, and optional transform)\n    \"\"\"\n    # Validate that required fields are present\n    for field in fields:\n        if \"source\" not in field or \"target\" not in field:\n            raise ValueError(\n                \"Field mapping must contain 'source' and 'target' keys\"\n            )\n\n    self.mappings.append({\"source\": source, \"target\": target, \"fields\": fields})\n    logger.debug(\n        f\"Added mapping from {source} to {target} with {len(fields)} fields\"\n    )\n</code></pre>"},{"location":"reference/api/#apilinker.core.mapper.FieldMapper.transform","title":"<code>transform(value, transformer)</code>","text":"<p>Convenience wrapper to apply a single transformer to a value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>Input value</p> required <code>transformer</code> <code>Union[str, Dict[str, Any]]</code> <p>Transformer name or transformer dict {name, params}</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Transformed value</p> Source code in <code>apilinker/core/mapper.py</code> <pre><code>def transform(self, value: Any, transformer: Union[str, Dict[str, Any]]) -&gt; Any:\n    \"\"\"\n    Convenience wrapper to apply a single transformer to a value.\n\n    Args:\n        value: Input value\n        transformer: Transformer name or transformer dict {name, params}\n\n    Returns:\n        Transformed value\n    \"\"\"\n    return self.apply_transform(value, transformer)\n</code></pre>"},{"location":"reference/api/#apilinker.core.mapper.FieldMapper.register_transformer","title":"<code>register_transformer(name, func)</code>","text":"<p>Register a custom transformation function.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the transformer</p> required <code>func</code> <code>Callable</code> <p>Function that performs the transformation</p> required Source code in <code>apilinker/core/mapper.py</code> <pre><code>def register_transformer(self, name: str, func: Callable) -&gt; None:\n    \"\"\"\n    Register a custom transformation function.\n\n    Args:\n        name: Name of the transformer\n        func: Function that performs the transformation\n    \"\"\"\n    if name in self.transformers:\n        logger.warning(f\"Overwriting existing transformer: {name}\")\n    self.transformers[name] = func\n    logger.debug(f\"Registered transformer: {name}\")\n</code></pre>"},{"location":"reference/api/#monitoring-alerting","title":"Monitoring &amp; Alerting","text":"<p>System monitoring and health checks.</p>"},{"location":"reference/api/#apilinker.core.monitoring.MonitoringManager","title":"<code>apilinker.core.monitoring.MonitoringManager</code>","text":"<p>Manages health checks and alerts.</p> Source code in <code>apilinker/core/monitoring.py</code> <pre><code>class MonitoringManager:\n    \"\"\"\n    Manages health checks and alerts.\n    \"\"\"\n\n    def __init__(self):\n        self.integrations: List[AlertIntegration] = []\n        self.rules: List[AlertRule] = []\n        self.alert_history: List[Alert] = []\n        self.health_checks: Dict[str, Callable[[], Union[bool, HealthCheckResult]]] = {}\n\n    def add_integration(self, integration: AlertIntegration):\n        \"\"\"Add an alert integration.\"\"\"\n        self.integrations.append(integration)\n\n    def add_rule(self, rule: AlertRule):\n        \"\"\"Add an alert rule.\"\"\"\n        self.rules.append(rule)\n\n    def register_health_check(\n        self, component: str, check_func: Callable[[], Union[bool, HealthCheckResult]]\n    ) -&gt; None:\n        \"\"\"Register a health check function for a component.\"\"\"\n        self.health_checks[component] = check_func\n\n    def run_health_checks(self) -&gt; Dict[str, HealthCheckResult]:\n        \"\"\"Run all registered health checks.\"\"\"\n        results: Dict[str, HealthCheckResult] = {}\n        context: Dict[str, Any] = {}\n\n        for component, check_func in self.health_checks.items():\n            try:\n                start_time = time.time()\n                result = check_func()\n                latency = (time.time() - start_time) * 1000\n\n                if isinstance(result, bool):\n                    status = HealthStatus.HEALTHY if result else HealthStatus.UNHEALTHY\n                    message = \"OK\" if result else \"Check failed\"\n                    details: Dict[str, Any] = {}\n                else:\n                    # result is HealthCheckResult\n                    status = result.status\n                    message = result.message\n                    details = result.details\n                    # Use the latency from the result if provided, or the measured one\n                    if result.latency_ms &gt; 0:\n                        latency = result.latency_ms\n\n                results[component] = HealthCheckResult(\n                    status=status,\n                    component=component,\n                    message=message,\n                    latency_ms=latency,\n                    details=details,\n                )\n                context[f\"{component}_status\"] = status\n                context[f\"{component}_latency_ms\"] = latency\n\n            except Exception as e:\n                logger.error(f\"Health check failed for {component}: {e}\")\n                results[component] = HealthCheckResult(\n                    status=HealthStatus.UNHEALTHY,\n                    component=component,\n                    message=str(e),\n                    latency_ms=0.0,\n                )\n                context[f\"{component}_status\"] = HealthStatus.UNHEALTHY\n\n        # Evaluate alert rules\n        self._evaluate_rules(context)\n\n        return results\n\n    def _evaluate_rules(self, context: Dict[str, Any]):\n        \"\"\"Evaluate all alert rules against the current context.\"\"\"\n        for rule in self.rules:\n            if rule.should_trigger(context):\n                alert = Alert(\n                    id=f\"{rule.name}_{int(time.time())}\",\n                    rule_name=rule.name,\n                    severity=rule.severity,\n                    message=f\"Alert rule '{rule.name}' triggered\",\n                    details=context,\n                )\n                self._trigger_alert(alert)\n\n    def _trigger_alert(self, alert: Alert):\n        \"\"\"Trigger an alert across all integrations.\"\"\"\n        logger.warning(f\"Triggering alert: {alert.rule_name} - {alert.message}\")\n        self.alert_history.append(alert)\n\n        # Deduplication check (simple version: check if same rule triggered recently)\n        # Note: Real deduplication is complex, this is a basic implementation\n\n        for integration in self.integrations:\n            integration.send_alert(alert)\n\n    def get_alert_history(self, limit: int = 100) -&gt; List[Alert]:\n        \"\"\"Get the history of triggered alerts.\"\"\"\n        return sorted(self.alert_history, key=lambda x: x.timestamp, reverse=True)[\n            :limit\n        ]\n</code></pre>"},{"location":"reference/api/#apilinker.core.monitoring.MonitoringManager.__init__","title":"<code>__init__()</code>","text":"Source code in <code>apilinker/core/monitoring.py</code> <pre><code>def __init__(self):\n    self.integrations: List[AlertIntegration] = []\n    self.rules: List[AlertRule] = []\n    self.alert_history: List[Alert] = []\n    self.health_checks: Dict[str, Callable[[], Union[bool, HealthCheckResult]]] = {}\n</code></pre>"},{"location":"reference/api/#apilinker.core.monitoring.MonitoringManager.register_health_check","title":"<code>register_health_check(component, check_func)</code>","text":"<p>Register a health check function for a component.</p> Source code in <code>apilinker/core/monitoring.py</code> <pre><code>def register_health_check(\n    self, component: str, check_func: Callable[[], Union[bool, HealthCheckResult]]\n) -&gt; None:\n    \"\"\"Register a health check function for a component.\"\"\"\n    self.health_checks[component] = check_func\n</code></pre>"},{"location":"reference/api/#apilinker.core.monitoring.MonitoringManager.add_rule","title":"<code>add_rule(rule)</code>","text":"<p>Add an alert rule.</p> Source code in <code>apilinker/core/monitoring.py</code> <pre><code>def add_rule(self, rule: AlertRule):\n    \"\"\"Add an alert rule.\"\"\"\n    self.rules.append(rule)\n</code></pre>"},{"location":"reference/api/#apilinker.core.monitoring.MonitoringManager.add_integration","title":"<code>add_integration(integration)</code>","text":"<p>Add an alert integration.</p> Source code in <code>apilinker/core/monitoring.py</code> <pre><code>def add_integration(self, integration: AlertIntegration):\n    \"\"\"Add an alert integration.\"\"\"\n    self.integrations.append(integration)\n</code></pre>"},{"location":"reference/api/#apilinker.core.monitoring.MonitoringManager.run_health_checks","title":"<code>run_health_checks()</code>","text":"<p>Run all registered health checks.</p> Source code in <code>apilinker/core/monitoring.py</code> <pre><code>def run_health_checks(self) -&gt; Dict[str, HealthCheckResult]:\n    \"\"\"Run all registered health checks.\"\"\"\n    results: Dict[str, HealthCheckResult] = {}\n    context: Dict[str, Any] = {}\n\n    for component, check_func in self.health_checks.items():\n        try:\n            start_time = time.time()\n            result = check_func()\n            latency = (time.time() - start_time) * 1000\n\n            if isinstance(result, bool):\n                status = HealthStatus.HEALTHY if result else HealthStatus.UNHEALTHY\n                message = \"OK\" if result else \"Check failed\"\n                details: Dict[str, Any] = {}\n            else:\n                # result is HealthCheckResult\n                status = result.status\n                message = result.message\n                details = result.details\n                # Use the latency from the result if provided, or the measured one\n                if result.latency_ms &gt; 0:\n                    latency = result.latency_ms\n\n            results[component] = HealthCheckResult(\n                status=status,\n                component=component,\n                message=message,\n                latency_ms=latency,\n                details=details,\n            )\n            context[f\"{component}_status\"] = status\n            context[f\"{component}_latency_ms\"] = latency\n\n        except Exception as e:\n            logger.error(f\"Health check failed for {component}: {e}\")\n            results[component] = HealthCheckResult(\n                status=HealthStatus.UNHEALTHY,\n                component=component,\n                message=str(e),\n                latency_ms=0.0,\n            )\n            context[f\"{component}_status\"] = HealthStatus.UNHEALTHY\n\n    # Evaluate alert rules\n    self._evaluate_rules(context)\n\n    return results\n</code></pre>"},{"location":"reference/api/#apilinker.core.monitoring.MonitoringManager.get_alert_history","title":"<code>get_alert_history(limit=100)</code>","text":"<p>Get the history of triggered alerts.</p> Source code in <code>apilinker/core/monitoring.py</code> <pre><code>def get_alert_history(self, limit: int = 100) -&gt; List[Alert]:\n    \"\"\"Get the history of triggered alerts.\"\"\"\n    return sorted(self.alert_history, key=lambda x: x.timestamp, reverse=True)[\n        :limit\n    ]\n</code></pre>"},{"location":"reference/api/#research-connectors","title":"Research Connectors","text":"<p>For research connector documentation, see Research Connectors Guide.</p>"},{"location":"reference/api/#available-connectors","title":"Available Connectors","text":"<ul> <li>NCBIConnector: PubMed and GenBank</li> <li>ArXivConnector: arXiv preprints  </li> <li>CrossRefConnector: Citation data</li> <li>SemanticScholarConnector: AI-powered search</li> <li>PubChemConnector: Chemical compounds</li> <li>ORCIDConnector: Researcher profiles</li> <li>GitHubConnector: Code repositories</li> <li>NASAConnector: Earth/space data</li> <li>SSEConnector: Real-time Server-Sent Events streams</li> </ul>"},{"location":"tutorials/","title":"ApiLinker Tutorials","text":"<p>This section contains step-by-step tutorials for common use cases. These tutorials will guide you through the process of setting up and using ApiLinker for various scenarios.</p>"},{"location":"tutorials/#available-tutorials","title":"Available Tutorials","text":"<ol> <li>Getting Started with ApiLinker</li> <li>Syncing Data Between Two APIs</li> <li>Creating Custom Transformers</li> <li>Setting Up Scheduled Syncs</li> <li>Error Handling and Validation</li> </ol>"},{"location":"tutorials/#tutorial-difficulty","title":"Tutorial Difficulty","text":"<p>Each tutorial is marked with a difficulty level:</p> <ul> <li>\ud83d\udfe2 Beginner: No prior experience with ApiLinker required</li> <li>\ud83d\udfe1 Intermediate: Basic understanding of ApiLinker and Python required</li> <li>\ud83d\udd34 Advanced: In-depth knowledge of ApiLinker and Python programming required</li> </ul>"},{"location":"tutorials/api_to_api_sync/","title":"Tutorial: Syncing Data Between Two APIs","text":"<p>\ud83d\udfe2 Difficulty: Beginner</p> <p>This tutorial guides you through the process of setting up ApiLinker to sync data between two REST APIs.</p>"},{"location":"tutorials/api_to_api_sync/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>How to configure source and target APIs</li> <li>How to map fields between different data structures</li> <li>How to run a sync operation</li> <li>How to handle common scenarios like authentication and pagination</li> </ul>"},{"location":"tutorials/api_to_api_sync/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or newer installed</li> <li>ApiLinker installed (<code>pip install apilinker</code>)</li> <li>Basic understanding of REST APIs</li> <li>Access to test APIs (we'll use JSONPlaceholder, a free testing API)</li> </ul>"},{"location":"tutorials/api_to_api_sync/#step-1-create-a-new-python-script","title":"Step 1: Create a New Python Script","text":"<p>Create a new file named <code>api_sync.py</code> and open it in your preferred editor.</p>"},{"location":"tutorials/api_to_api_sync/#step-2-import-apilinker-and-set-up-basic-structure","title":"Step 2: Import ApiLinker and Set Up Basic Structure","text":"<pre><code>from apilinker import ApiLinker\nimport logging\nimport os\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger('api_sync')\n\n# Initialize ApiLinker\nlinker = ApiLinker()\n\n# We'll add configurations in the next steps\n</code></pre>"},{"location":"tutorials/api_to_api_sync/#step-3-configure-source-api","title":"Step 3: Configure Source API","text":"<p>We'll use JSONPlaceholder as our source API to fetch user data:</p> <pre><code># Add source API configuration\nlinker.add_source(\n    type=\"rest\",\n    base_url=\"https://jsonplaceholder.typicode.com\",\n    endpoints={\n        \"get_users\": {\n            \"path\": \"/users\",\n            \"method\": \"GET\"\n        }\n    }\n)\n\nlogger.info(\"Source API configured\")\n</code></pre>"},{"location":"tutorials/api_to_api_sync/#step-4-configure-target-api","title":"Step 4: Configure Target API","text":"<p>For the target, we'll use JSONPlaceholder's posts endpoint (normally, this would be a different API):</p> <pre><code># Add target API configuration\nlinker.add_target(\n    type=\"rest\",\n    base_url=\"https://jsonplaceholder.typicode.com\",\n    endpoints={\n        \"create_post\": {\n            \"path\": \"/posts\",\n            \"method\": \"POST\"\n        }\n    }\n)\n\nlogger.info(\"Target API configured\")\n</code></pre>"},{"location":"tutorials/api_to_api_sync/#step-5-create-field-mappings","title":"Step 5: Create Field Mappings","text":"<p>Now, let's define how fields from the source API map to the target API:</p> <pre><code># Define field mappings\nlinker.add_mapping(\n    source=\"get_users\",\n    target=\"create_post\",\n    fields=[\n        # Map user ID to the userId field in posts\n        {\"source\": \"id\", \"target\": \"userId\"},\n\n        # Use the user's name as the post title\n        {\"source\": \"name\", \"target\": \"title\"},\n\n        # Use the user's company catchphrase as the post body\n        {\"source\": \"company.catchPhrase\", \"target\": \"body\"}\n    ]\n)\n\nlogger.info(\"Field mappings configured\")\n</code></pre>"},{"location":"tutorials/api_to_api_sync/#step-6-define-transformers-optional","title":"Step 6: Define Transformers (Optional)","text":"<p>Let's add a simple transformer to format the post title:</p> <pre><code># Define a custom transformer function\ndef format_title(value, **kwargs):\n    \"\"\"Convert the title to a specific format.\"\"\"\n    if not value:\n        return \"Untitled\"\n    return f\"User Profile: {value}\"\n\n# Register the transformer\nlinker.mapper.register_transformer(\"format_title\", format_title)\n\n# Update the mapping to use the transformer\nlinker.add_mapping(\n    source=\"get_users\",\n    target=\"create_post\",\n    fields=[\n        {\"source\": \"id\", \"target\": \"userId\"},\n        # Apply the transformer to the title\n        {\"source\": \"name\", \"target\": \"title\", \"transform\": \"format_title\"},\n        {\"source\": \"company.catchPhrase\", \"target\": \"body\"}\n    ],\n    # Replace the previous mapping\n    replace=True\n)\n\nlogger.info(\"Transformers configured\")\n</code></pre>"},{"location":"tutorials/api_to_api_sync/#step-7-run-a-dry-run-first","title":"Step 7: Run a Dry Run First","text":"<p>It's always a good idea to test with a dry run before performing actual API calls:</p> <pre><code># Perform a dry run (no actual API calls to the target)\nlogger.info(\"Starting dry run...\")\ndry_run_result = linker.sync(dry_run=True)\n\n# Print preview of the first record\nif dry_run_result.preview:\n    logger.info(f\"Dry run preview of first record: {dry_run_result.preview[0]}\")\nelse:\n    logger.info(\"No records to sync\")\n\nlogger.info(f\"Dry run would sync {dry_run_result.count} records\")\n</code></pre>"},{"location":"tutorials/api_to_api_sync/#step-8-run-the-actual-sync","title":"Step 8: Run the Actual Sync","text":"<p>If you're satisfied with the dry run results, perform the actual sync:</p> <pre><code># Perform the actual sync\nlogger.info(\"Starting sync...\")\nresult = linker.sync()\n\nlogger.info(f\"Sync completed. Synced {result.count} records\")\nif result.errors:\n    logger.warning(f\"Encountered {len(result.errors)} errors during sync\")\n    for error in result.errors[:5]:  # Show first 5 errors\n        logger.warning(f\"Error: {error}\")\n</code></pre>"},{"location":"tutorials/api_to_api_sync/#step-9-complete-script","title":"Step 9: Complete Script","text":"<p>Here's the complete script:</p> <pre><code>from apilinker import ApiLinker\nimport logging\nimport os\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger('api_sync')\n\n# Initialize ApiLinker\nlinker = ApiLinker()\n\n# Add source API configuration\nlinker.add_source(\n    type=\"rest\",\n    base_url=\"https://jsonplaceholder.typicode.com\",\n    endpoints={\n        \"get_users\": {\n            \"path\": \"/users\",\n            \"method\": \"GET\"\n        }\n    }\n)\nlogger.info(\"Source API configured\")\n\n# Add target API configuration\nlinker.add_target(\n    type=\"rest\",\n    base_url=\"https://jsonplaceholder.typicode.com\",\n    endpoints={\n        \"create_post\": {\n            \"path\": \"/posts\",\n            \"method\": \"POST\"\n        }\n    }\n)\nlogger.info(\"Target API configured\")\n\n# Define a custom transformer function\ndef format_title(value, **kwargs):\n    \"\"\"Convert the title to a specific format.\"\"\"\n    if not value:\n        return \"Untitled\"\n    return f\"User Profile: {value}\"\n\n# Register the transformer\nlinker.mapper.register_transformer(\"format_title\", format_title)\n\n# Define field mappings\nlinker.add_mapping(\n    source=\"get_users\",\n    target=\"create_post\",\n    fields=[\n        {\"source\": \"id\", \"target\": \"userId\"},\n        # Apply the transformer to the title\n        {\"source\": \"name\", \"target\": \"title\", \"transform\": \"format_title\"},\n        {\"source\": \"company.catchPhrase\", \"target\": \"body\"}\n    ]\n)\nlogger.info(\"Field mappings configured\")\n\n# Perform a dry run\nlogger.info(\"Starting dry run...\")\ndry_run_result = linker.sync(dry_run=True)\n\n# Print preview of the first record\nif dry_run_result.preview:\n    logger.info(f\"Dry run preview of first record: {dry_run_result.preview[0]}\")\nelse:\n    logger.info(\"No records to sync\")\n\nlogger.info(f\"Dry run would sync {dry_run_result.count} records\")\n\n# Ask for confirmation before actual sync\nuser_input = input(\"Proceed with actual sync? (yes/no): \")\nif user_input.lower() not in [\"yes\", \"y\"]:\n    logger.info(\"Sync cancelled by user\")\n    exit()\n\n# Perform the actual sync\nlogger.info(\"Starting sync...\")\nresult = linker.sync()\n\nlogger.info(f\"Sync completed. Synced {result.count} records\")\nif result.errors:\n    logger.warning(f\"Encountered {len(result.errors)} errors during sync\")\n    for error in result.errors[:5]:  # Show first 5 errors\n        logger.warning(f\"Error: {error}\")\n</code></pre>"},{"location":"tutorials/api_to_api_sync/#step-10-run-the-script","title":"Step 10: Run the Script","text":"<p>Save the script and run it from your terminal:</p> <pre><code>python api_sync.py\n</code></pre>"},{"location":"tutorials/api_to_api_sync/#whats-next","title":"What's Next?","text":"<p>Now that you've successfully synced data between two APIs, you can:</p> <ol> <li>Explore Advanced Features:</li> <li>Try pagination for larger datasets</li> <li>Add authentication for secured APIs</li> <li> <p>Set up scheduling for periodic syncs</p> </li> <li> <p>Try with Your Own APIs:</p> </li> <li>Replace JSONPlaceholder with your actual APIs</li> <li> <p>Adapt field mappings to your specific data structures</p> </li> <li> <p>Check Out Other Tutorials:</p> </li> <li>Creating Custom Transformers</li> <li>Setting Up Scheduled Syncs</li> <li>Error Handling and Validation</li> </ol>"},{"location":"tutorials/api_to_api_sync/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/api_to_api_sync/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<ul> <li>404 Not Found: Double-check your API endpoint paths</li> <li>Missing Fields: Verify field names in your source data</li> <li>Authentication Errors: Ensure credentials are correct and properly formatted</li> </ul> <p>For more help, check the FAQ or open an issue on GitHub.</p>"},{"location":"tutorials/custom_transformers/","title":"Tutorial: Creating Custom Transformers","text":"<p>\ud83d\udfe1 Difficulty: Intermediate</p> <p>This tutorial guides you through the process of creating custom transformers to modify data as it's mapped between APIs.</p>"},{"location":"tutorials/custom_transformers/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>How to create simple and complex data transformers</li> <li>How to register transformers with ApiLinker</li> <li>How to use transformers in field mappings</li> <li>How to handle transformer parameters and edge cases</li> </ul>"},{"location":"tutorials/custom_transformers/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or newer installed</li> <li>ApiLinker installed (<code>pip install apilinker</code>)</li> <li>Basic understanding of Python functions</li> <li>Completion of the API-to-API Sync tutorial</li> </ul>"},{"location":"tutorials/custom_transformers/#step-1-understanding-transformers","title":"Step 1: Understanding Transformers","text":"<p>Transformers are functions that modify data during the mapping process. They can:</p> <ul> <li>Format values (e.g., convert dates, format phone numbers)</li> <li>Convert data types (e.g., string to number, array to string)</li> <li>Apply business logic (e.g., calculate totals, validate values)</li> </ul> <p>A transformer function must: 1. Accept a value as its first argument 2. Accept arbitrary keyword arguments (**kwargs) 3. Return the transformed value</p>"},{"location":"tutorials/custom_transformers/#step-2-create-a-simple-transformer","title":"Step 2: Create a Simple Transformer","text":"<p>Let's start with a basic transformer that formats a phone number:</p> <pre><code>from apilinker import ApiLinker\n\n# Initialize ApiLinker\nlinker = ApiLinker()\n\n# Define a phone number formatter\ndef format_phone(value, **kwargs):\n    \"\"\"Format a phone number to (XXX) XXX-XXXX format.\"\"\"\n    if not value:  # Handle None/empty values\n        return \"\"\n\n    # Remove non-digit characters\n    digits = ''.join(c for c in value if c.isdigit())\n\n    # Format based on length\n    if len(digits) == 10:  # US phone number\n        return f\"({digits[0:3]}) {digits[3:6]}-{digits[6:10]}\"\n    elif len(digits) &gt; 10:  # International number\n        return f\"+{digits[0]} ({digits[1:4]}) {digits[4:7]}-{digits[7:11]}\"\n    else:\n        return value  # Return original if not enough digits\n\n# Register the transformer with ApiLinker\nlinker.mapper.register_transformer(\"format_phone\", format_phone)\n\n# Test the transformer\noriginal_phone = \"5551234567\"\nformatted_phone = linker.mapper.transform(original_phone, \"format_phone\")\nprint(f\"Original: {original_phone}\")\nprint(f\"Formatted: {formatted_phone}\")\n</code></pre>"},{"location":"tutorials/custom_transformers/#step-3-create-a-transformer-with-parameters","title":"Step 3: Create a Transformer with Parameters","text":"<p>Transformers can accept additional parameters through **kwargs:</p> <pre><code># Define a transformer that accepts parameters\ndef truncate_text(value, **kwargs):\n    \"\"\"Truncate text to a specified length and add ellipsis if needed.\"\"\"\n    if not value or not isinstance(value, str):\n        return value\n\n    # Get max_length from kwargs with default value of 100\n    max_length = kwargs.get(\"max_length\", 100)\n\n    # Get ellipsis text with default value of \"...\"\n    ellipsis = kwargs.get(\"ellipsis\", \"...\")\n\n    # Truncate if longer than max_length\n    if len(value) &gt; max_length:\n        return value[:max_length - len(ellipsis)] + ellipsis\n\n    return value\n\n# Register the transformer\nlinker.mapper.register_transformer(\"truncate_text\", truncate_text)\n\n# Test with different parameters\nlong_text = \"This is a very long text that needs to be truncated to fit within specific limits for display purposes.\"\nprint(truncate_text(long_text, max_length=50))  # Default ellipsis\nprint(truncate_text(long_text, max_length=30, ellipsis=\" [more]\"))  # Custom ellipsis\n</code></pre>"},{"location":"tutorials/custom_transformers/#step-4-create-a-class-based-transformer","title":"Step 4: Create a Class-Based Transformer","text":"<p>For more complex transformers, you can use the TransformerPlugin base class:</p> <pre><code>from apilinker.core.plugins import TransformerPlugin\n\nclass SentimentAnalysisTransformer(TransformerPlugin):\n    \"\"\"A transformer plugin that analyzes text sentiment.\"\"\"\n\n    plugin_name = \"sentiment_analysis\"\n\n    def validate_input(self, value):\n        \"\"\"Validate the input before transformation.\"\"\"\n        return isinstance(value, str)\n\n    def transform(self, value, **kwargs):\n        \"\"\"\n        Simple sentiment analysis implementation.\n\n        Args:\n            value: The text to analyze\n            **kwargs: Additional parameters\n                threshold: Sensitivity threshold (default: 0.1)\n\n        Returns:\n            dict: Sentiment analysis results\n        \"\"\"\n        if not self.validate_input(value) or not value:\n            return {\"sentiment\": \"neutral\", \"score\": 0.0}\n\n        # Get threshold parameter with default value\n        threshold = kwargs.get(\"threshold\", 0.1)\n\n        # Define sentiment word lists\n        positive_words = [\"good\", \"great\", \"excellent\", \"happy\", \"positive\", \"like\", \"love\"]\n        negative_words = [\"bad\", \"poor\", \"terrible\", \"unhappy\", \"negative\", \"hate\", \"dislike\"]\n\n        # Convert to lowercase for case-insensitive matching\n        text = value.lower()\n\n        # Count positive and negative words\n        positive_count = sum(1 for word in positive_words if word in text)\n        negative_count = sum(1 for word in negative_words if word in text)\n\n        # Calculate sentiment score\n        total = positive_count + negative_count\n        score = 0.0 if total == 0 else (positive_count - negative_count) / total\n\n        # Determine sentiment based on score and threshold\n        sentiment = \"neutral\"\n        if score &gt; threshold:\n            sentiment = \"positive\"\n        elif score &lt; -threshold:\n            sentiment = \"negative\"\n\n        return {\n            \"sentiment\": sentiment,\n            \"score\": score,\n            \"positive_count\": positive_count,\n            \"negative_count\": negative_count\n        }\n\n# Register the plugin class\nlinker.plugin_manager.register_plugin(SentimentAnalysisTransformer)\n\n# Test the sentiment analysis transformer\ntext1 = \"I really love this product, it's excellent and makes me happy!\"\ntext2 = \"This is terrible, I hate it and it makes me unhappy.\"\ntext3 = \"This is a neutral statement without strong sentiment.\"\n\n# Get the transformer\ntransformer = linker.plugin_manager.get_transformer(\"sentiment_analysis\")\n\n# Analyze texts\nprint(f\"Text 1: {transformer(text1)}\")\nprint(f\"Text 2: {transformer(text2)}\")\nprint(f\"Text 3: {transformer(text3)}\")\nprint(f\"Text 3 (higher threshold): {transformer(text3, threshold=0.3)}\")\n</code></pre>"},{"location":"tutorials/custom_transformers/#step-5-using-transformers-in-field-mappings","title":"Step 5: Using Transformers in Field Mappings","text":"<p>Now, let's use our transformers in field mappings:</p> <pre><code># Configure source and target APIs\nlinker.add_source(\n    type=\"rest\",\n    base_url=\"https://api.example.com/v1\",\n    endpoints={\n        \"get_customers\": {\n            \"path\": \"/customers\",\n            \"method\": \"GET\"\n        }\n    }\n)\n\nlinker.add_target(\n    type=\"rest\",\n    base_url=\"https://api.destination.com/v2\",\n    endpoints={\n        \"create_contact\": {\n            \"path\": \"/contacts\",\n            \"method\": \"POST\"\n        }\n    }\n)\n\n# Define mapping with transformers\nlinker.add_mapping(\n    source=\"get_customers\",\n    target=\"create_contact\",\n    fields=[\n        {\"source\": \"id\", \"target\": \"external_id\"},\n\n        # Use phone formatter\n        {\"source\": \"phone\", \"target\": \"phoneNumber\", \"transform\": \"format_phone\"},\n\n        # Truncate description\n        {\n            \"source\": \"description\", \n            \"target\": \"bio\", \n            \"transform\": \"truncate_text\", \n            \"max_length\": 200,  # Pass parameter to transformer\n            \"ellipsis\": \" (continued...)\"\n        },\n\n        # Analyze feedback sentiment\n        {\n            \"source\": \"feedback\", \n            \"target\": \"sentiment_data\", \n            \"transform\": \"sentiment_analysis\",\n            \"threshold\": 0.2  # Custom threshold\n        }\n    ]\n)\n</code></pre>"},{"location":"tutorials/custom_transformers/#step-6-chain-multiple-transformers","title":"Step 6: Chain Multiple Transformers","text":"<p>You can also chain multiple transformers to apply them in sequence:</p> <pre><code># Define a string normalization transformer\ndef normalize_string(value, **kwargs):\n    \"\"\"Normalize string by removing extra whitespace and converting to lowercase.\"\"\"\n    if not value or not isinstance(value, str):\n        return value\n    return \" \".join(value.split()).lower()\n\nlinker.mapper.register_transformer(\"normalize_string\", normalize_string)\n\n# Use chained transformers in a mapping\nlinker.add_mapping(\n    source=\"get_products\",\n    target=\"create_item\",\n    fields=[\n        # Chain transformers - first normalize, then truncate\n        {\n            \"source\": \"description\",\n            \"target\": \"short_desc\",\n            \"transform\": [\n                \"normalize_string\",  # Applied first\n                {\n                    \"name\": \"truncate_text\",  # Applied second\n                    \"max_length\": 100\n                }\n            ]\n        }\n    ]\n)\n</code></pre>"},{"location":"tutorials/custom_transformers/#step-7-error-handling-in-transformers","title":"Step 7: Error Handling in Transformers","text":"<p>Good transformers should handle errors gracefully:</p> <pre><code>def safe_date_converter(value, **kwargs):\n    \"\"\"Convert date strings to a consistent format with error handling.\"\"\"\n    if not value:\n        return None\n\n    input_format = kwargs.get(\"input_format\", \"%Y-%m-%d\")\n    output_format = kwargs.get(\"output_format\", \"%d/%m/%Y\")\n\n    try:\n        from datetime import datetime\n        date_obj = datetime.strptime(value, input_format)\n        return date_obj.strftime(output_format)\n    except ValueError:\n        # Handle parsing errors\n        fallback = kwargs.get(\"fallback\")\n        if fallback:\n            return fallback\n        return value  # Return original if can't convert\n    except Exception as e:\n        # Log other errors\n        print(f\"Error in date_converter: {e}\")\n        return value\n\nlinker.mapper.register_transformer(\"safe_date_converter\", safe_date_converter)\n</code></pre>"},{"location":"tutorials/custom_transformers/#step-8-testing-your-transformers","title":"Step 8: Testing Your Transformers","text":"<p>Always test your transformers with various inputs including edge cases:</p> <pre><code>def test_transformers():\n    \"\"\"Test transformers with various inputs.\"\"\"\n    # Test phone formatter\n    test_phones = [\n        \"5551234567\",              # Basic US number\n        \"+15551234567\",            # US with country code\n        \"555-123-4567\",            # With dashes\n        \"(555) 123-4567\",          # Already formatted\n        \"123456\",                  # Too short\n        None,                      # None\n        \"\"                         # Empty string\n    ]\n    print(\"\\nTesting phone formatter:\")\n    for phone in test_phones:\n        try:\n            result = linker.mapper.transform(phone, \"format_phone\")\n            print(f\"  Input: {phone!r} \u2192 Output: {result!r}\")\n        except Exception as e:\n            print(f\"  Error with {phone!r}: {e}\")\n\n    # Test truncate_text\n    test_texts = [\n        \"Short text\",              # Shorter than limit\n        \"This is a longer text that should be truncated\",  # Longer than default limit\n        None,                      # None\n        123                        # Non-string\n    ]\n    print(\"\\nTesting text truncation:\")\n    for text in test_texts:\n        try:\n            result = linker.mapper.transform(text, \"truncate_text\", max_length=20)\n            print(f\"  Input: {text!r} \u2192 Output: {result!r}\")\n        except Exception as e:\n            print(f\"  Error with {text!r}: {e}\")\n\n# Run transformer tests\ntest_transformers()\n</code></pre>"},{"location":"tutorials/custom_transformers/#step-9-complete-example","title":"Step 9: Complete Example","text":"<p>Here's a complete script that demonstrates creating and using custom transformers:</p> <pre><code>from apilinker import ApiLinker\nfrom apilinker.core.plugins import TransformerPlugin\n\n# Initialize ApiLinker\nlinker = ApiLinker()\n\n# ------ Simple transformers ------\n\ndef format_phone(value, **kwargs):\n    \"\"\"Format a phone number to (XXX) XXX-XXXX format.\"\"\"\n    if not value:\n        return \"\"\n\n    # Remove non-digit characters\n    digits = ''.join(c for c in value if c.isdigit())\n\n    # Format based on length\n    if len(digits) == 10:  # US phone number\n        return f\"({digits[0:3]}) {digits[3:6]}-{digits[6:10]}\"\n    elif len(digits) &gt; 10:  # International number\n        return f\"+{digits[0]} ({digits[1:4]}) {digits[4:7]}-{digits[7:11]}\"\n    else:\n        return value  # Return original if not enough digits\n\ndef truncate_text(value, **kwargs):\n    \"\"\"Truncate text to a specified length and add ellipsis if needed.\"\"\"\n    if not value or not isinstance(value, str):\n        return value\n\n    max_length = kwargs.get(\"max_length\", 100)\n    ellipsis = kwargs.get(\"ellipsis\", \"...\")\n\n    if len(value) &gt; max_length:\n        return value[:max_length - len(ellipsis)] + ellipsis\n\n    return value\n\ndef safe_date_converter(value, **kwargs):\n    \"\"\"Convert date strings to a consistent format with error handling.\"\"\"\n    if not value:\n        return None\n\n    input_format = kwargs.get(\"input_format\", \"%Y-%m-%d\")\n    output_format = kwargs.get(\"output_format\", \"%d/%m/%Y\")\n\n    try:\n        from datetime import datetime\n        date_obj = datetime.strptime(value, input_format)\n        return date_obj.strftime(output_format)\n    except ValueError:\n        fallback = kwargs.get(\"fallback\")\n        if fallback:\n            return fallback\n        return value  # Return original if can't convert\n    except Exception as e:\n        print(f\"Error in date_converter: {e}\")\n        return value\n\n# ------ Class-based transformer plugin ------\n\nclass SentimentAnalysisTransformer(TransformerPlugin):\n    \"\"\"A transformer plugin that analyzes text sentiment.\"\"\"\n\n    plugin_name = \"sentiment_analysis\"\n\n    def validate_input(self, value):\n        \"\"\"Validate the input before transformation.\"\"\"\n        return isinstance(value, str)\n\n    def transform(self, value, **kwargs):\n        \"\"\"Simple sentiment analysis implementation.\"\"\"\n        if not self.validate_input(value) or not value:\n            return {\"sentiment\": \"neutral\", \"score\": 0.0}\n\n        threshold = kwargs.get(\"threshold\", 0.1)\n\n        positive_words = [\"good\", \"great\", \"excellent\", \"happy\", \"positive\", \"like\", \"love\"]\n        negative_words = [\"bad\", \"poor\", \"terrible\", \"unhappy\", \"negative\", \"hate\", \"dislike\"]\n\n        text = value.lower()\n\n        positive_count = sum(1 for word in positive_words if word in text)\n        negative_count = sum(1 for word in negative_words if word in text)\n\n        total = positive_count + negative_count\n        score = 0.0 if total == 0 else (positive_count - negative_count) / total\n\n        sentiment = \"neutral\"\n        if score &gt; threshold:\n            sentiment = \"positive\"\n        elif score &lt; -threshold:\n            sentiment = \"negative\"\n\n        return {\n            \"sentiment\": sentiment,\n            \"score\": score,\n            \"positive_count\": positive_count,\n            \"negative_count\": negative_count\n        }\n\n# ------ Register transformers ------\n\n# Register function-based transformers\nlinker.mapper.register_transformer(\"format_phone\", format_phone)\nlinker.mapper.register_transformer(\"truncate_text\", truncate_text)\nlinker.mapper.register_transformer(\"safe_date_converter\", safe_date_converter)\n\n# Register class-based transformer plugin\nlinker.plugin_manager.register_plugin(SentimentAnalysisTransformer)\n\n# ------ Test the transformers ------\n\ndef test_transformers():\n    \"\"\"Test transformers with various inputs.\"\"\"\n    # Test phone formatter\n    test_phones = [\"5551234567\", \"+15551234567\", \"555-123-4567\", \"(555) 123-4567\", \"123456\", None, \"\"]\n    print(\"\\nTesting phone formatter:\")\n    for phone in test_phones:\n        try:\n            result = linker.mapper.transform(phone, \"format_phone\")\n            print(f\"  Input: {phone!r} \u2192 Output: {result!r}\")\n        except Exception as e:\n            print(f\"  Error with {phone!r}: {e}\")\n\n    # Test date converter\n    test_dates = [\"2023-01-15\", \"01/15/2023\", \"2023-13-01\", None, \"\"]\n    print(\"\\nTesting date converter:\")\n    for date in test_dates:\n        try:\n            result = linker.mapper.transform(date, \"safe_date_converter\", \n                                           input_format=\"%Y-%m-%d\", \n                                           fallback=\"INVALID DATE\")\n            print(f\"  Input: {date!r} \u2192 Output: {result!r}\")\n        except Exception as e:\n            print(f\"  Error with {date!r}: {e}\")\n\n    # Test sentiment analysis\n    test_texts = [\n        \"I really love this product, it's excellent and makes me happy!\",\n        \"This is terrible, I hate it and it makes me unhappy.\",\n        \"This is a neutral statement without strong sentiment.\",\n        None\n    ]\n    print(\"\\nTesting sentiment analysis:\")\n    transformer = linker.plugin_manager.get_transformer(\"sentiment_analysis\")\n    for text in test_texts:\n        try:\n            result = transformer(text)\n            print(f\"  Input: {text!r}\")\n            print(f\"  Output: {result}\")\n        except Exception as e:\n            print(f\"  Error with {text!r}: {e}\")\n\n# Run transformer tests\ntest_transformers()\n</code></pre>"},{"location":"tutorials/custom_transformers/#whats-next","title":"What's Next?","text":"<p>Now that you've learned how to create custom transformers, you can:</p> <ol> <li>Create More Advanced Transformers:</li> <li>Transform complex data structures</li> <li>Integrate with external libraries (e.g., pandas for data analysis)</li> <li> <p>Access external services within transformers (with caching)</p> </li> <li> <p>Learn About Error Handling:</p> </li> <li>Improve error handling in transformers</li> <li> <p>Set up validation to prevent errors</p> </li> <li> <p>Check Out Other Tutorials:</p> </li> <li>Setting Up Scheduled Syncs</li> <li>Error Handling and Validation</li> </ol>"},{"location":"tutorials/custom_transformers/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/custom_transformers/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<ul> <li>Function Not Found: Ensure transformer is registered before use</li> <li>Type Errors: Add proper type checking in your transformer</li> <li>Parameter Issues: Verify parameter names in mapping match your function</li> </ul> <p>For more help, check the FAQ or open an issue on GitHub.</p>"},{"location":"user-guide/authentication/","title":"Authentication","text":"<p>ApiLinker supports a wide range of authentication methods to connect to various APIs.</p>"},{"location":"user-guide/authentication/#supported-methods","title":"Supported Methods","text":""},{"location":"user-guide/authentication/#api-key","title":"API Key","text":"<pre><code>auth:\n  type: api_key\n  key: your_key\n  header: X-API-Key\n</code></pre>"},{"location":"user-guide/authentication/#bearer-token","title":"Bearer Token","text":"<pre><code>auth:\n  type: bearer\n  token: your_token\n</code></pre>"},{"location":"user-guide/authentication/#basic-auth","title":"Basic Auth","text":"<pre><code>auth:\n  type: basic\n  username: user\n  password: pass\n</code></pre>"},{"location":"user-guide/authentication/#oauth2","title":"OAuth2","text":"<p>ApiLinker supports Client Credentials flow and other OAuth2 patterns.</p> <pre><code>auth:\n  type: oauth2_client_credentials\n  client_id: id\n  client_secret: secret\n  token_url: https://auth.example.com/token\n</code></pre>"},{"location":"user-guide/comparison/","title":"ApiLinker vs. Other Integration Tools","text":"<p>This guide compares ApiLinker with other popular API integration tools to help you choose the right solution for your needs.</p>"},{"location":"user-guide/comparison/#feature-comparison","title":"Feature Comparison","text":"Feature ApiLinker Zapier n8n Apache Airflow MuleSoft Basics Open Source \u2705 \u274c \u2705 (Community) \u2705 \u274c Self-Hosted \u2705 \u274c \u2705 \u2705 \u2705 (Enterprise) Code-Driven \u2705 \u274c Partial \u2705 Partial Config-Driven \u2705 \u2705 \u2705 Partial \u2705 Learning Curve Medium Low Medium High High Features REST API Support \u2705 \u2705 \u2705 \u2705 \u2705 GraphQL Support \u274c Partial \u2705 \u2705 \u2705 SOAP Support \u274c \u274c \u2705 \u2705 \u2705 Pagination Handling \u2705 (Basic) \u2705 \u2705 \u2705 \u2705 Field Mapping \u2705 \u2705 \u2705 \u2705 \u2705 Data Transformations \u2705 (Built-in &amp; Custom) Limited \u2705 \u2705 \u2705 Scheduling \u2705 (Interval, Cron) \u2705 \u2705 \u2705 \u2705 SSE Streaming \u2705 \u274c Partial \u274c Partial Error Handling \u2705 (Basic Retries) Limited \u2705 \u2705 \u2705 Authentication API Key \u2705 \u2705 \u2705 \u2705 \u2705 Bearer Token \u2705 \u2705 \u2705 \u2705 \u2705 Basic Auth \u2705 \u2705 \u2705 \u2705 \u2705 OAuth2 \u2705 (Client Credentials) \u2705 \u2705 \u2705 \u2705 Custom Auth \u274c \u274c \u2705 \u2705 \u2705 Extensions Custom Plugins \u2705 Limited \u2705 \u2705 \u2705 Developer SDK \u274c Limited \u2705 \u2705 \u2705 Use Cases Personal Projects \u2705 \u2705 \u2705 \u274c \u274c Small Business \u2705 \u2705 \u2705 \u274c \u274c Enterprise \u2705 \u2705 \u2705 \u2705 \u2705 Research/Academic \u2705 Limited Limited \u2705 Limited Performance Large Data Handling \u2705 (Pagination) Limited Limited \u2705 \u2705 Low Resource Usage \u2705 N/A Moderate High High Dependencies External Dependencies Minimal (httpx, pydantic, yaml) N/A Moderate Many Many Docker Support \u274c N/A \u2705 \u2705 \u2705"},{"location":"user-guide/comparison/#when-to-use-apilinker","title":"When to Use ApiLinker","text":"<p>ApiLinker is the best choice when you need:</p> <ol> <li>Code-first approach - You prefer writing and maintaining Python code rather than using GUI tools</li> <li>Minimal dependencies - You need a lightweight solution with few external dependencies</li> <li>Full customizability - You need to implement custom logic for data transformations or API handling</li> <li>Research applications - You need reproducible data pipelines for research and academic work</li> <li>Embedding in other applications - You need to integrate API connectivity into your existing Python application</li> </ol>"},{"location":"user-guide/comparison/#when-to-consider-alternatives","title":"When to Consider Alternatives","text":"<p>Consider other tools when:</p> <ol> <li>GUI-based workflow - You prefer a visual interface for designing integrations (consider Zapier or n8n)</li> <li>Pre-built connectors - You need a large library of pre-built API integrations (consider Zapier)</li> <li>Complex orchestration - You need advanced workflow orchestration with DAGs (consider Apache Airflow)</li> <li>Enterprise integration - You need a full enterprise integration platform with governance (consider MuleSoft)</li> </ol>"},{"location":"user-guide/comparison/#detailed-comparisons","title":"Detailed Comparisons","text":""},{"location":"user-guide/comparison/#apilinker-vs-zapier","title":"ApiLinker vs. Zapier","text":"<p>ApiLinker advantages: - Open source and self-hosted - Full code control in Python - Lower long-term cost - More flexible transformations - No limits on task executions</p> <p>Zapier advantages: - No-code visual interface - 3,000+ pre-built integrations - Hosted solution (no infrastructure management) - Simpler for non-developers - Built-in templates for common workflows</p>"},{"location":"user-guide/comparison/#apilinker-vs-n8n","title":"ApiLinker vs. n8n","text":"<p>ApiLinker advantages: - Python-native (better for data science/ML workflows) - Lighter weight with fewer dependencies - Designed for embedding in other applications - Simpler learning curve for Python developers</p> <p>n8n advantages: - Visual workflow editor - More built-in integrations - Browser-based interface - Support for more protocols out of the box</p>"},{"location":"user-guide/comparison/#apilinker-vs-apache-airflow","title":"ApiLinker vs. Apache Airflow","text":"<p>ApiLinker advantages: - Focused on API integration specifically - Much lighter weight and simpler to use - Lower learning curve - Better for simple integration tasks</p> <p>Apache Airflow advantages: - More powerful workflow orchestration - Better for complex multi-step pipelines - More monitoring and observability features - Stronger community and ecosystem</p>"},{"location":"user-guide/comparison/#code-example-comparisons","title":"Code Example Comparisons","text":""},{"location":"user-guide/comparison/#simple-api-integration","title":"Simple API Integration","text":""},{"location":"user-guide/comparison/#apilinker","title":"ApiLinker","text":"<pre><code>from apilinker import ApiLinker\n\n# Initialize\nlinker = ApiLinker()\n\n# Configure source\nlinker.add_source(\n    type=\"rest\",\n    base_url=\"https://api.source.com\",\n    endpoints={\"get_users\": {\"path\": \"/users\"}}\n)\n\n# Configure target\nlinker.add_target(\n    type=\"rest\",\n    base_url=\"https://api.target.com\",\n    endpoints={\"create_users\": {\"path\": \"/users\", \"method\": \"POST\"}}\n)\n\n# Map fields\nlinker.add_mapping(\n    source=\"get_users\",\n    target=\"create_users\",\n    fields=[\n        {\"source\": \"id\", \"target\": \"user_id\"},\n        {\"source\": \"name\", \"target\": \"full_name\"}\n    ]\n)\n\n# Run the sync\nresult = linker.sync()\n</code></pre>"},{"location":"user-guide/comparison/#apache-airflow","title":"Apache Airflow","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\nimport requests\nimport json\n\ndef fetch_users():\n    response = requests.get(\"https://api.source.com/users\")\n    return response.json()\n\ndef transform_users(ti):\n    users = ti.xcom_pull(task_ids=['fetch_users'])[0]\n    transformed = []\n    for user in users:\n        transformed.append({\n            \"user_id\": user[\"id\"],\n            \"full_name\": user[\"name\"]\n        })\n    return transformed\n\ndef push_users(ti):\n    users = ti.xcom_pull(task_ids=['transform_users'])[0]\n    for user in users:\n        requests.post(\"https://api.target.com/users\", json=user)\n\nwith DAG('api_sync', start_date=datetime(2023, 1, 1), schedule_interval='@daily') as dag:\n    fetch = PythonOperator(\n        task_id='fetch_users',\n        python_callable=fetch_users\n    )\n\n    transform = PythonOperator(\n        task_id='transform_users',\n        python_callable=transform_users\n    )\n\n    push = PythonOperator(\n        task_id='push_users',\n        python_callable=push_users\n    )\n\n    fetch &gt;&gt; transform &gt;&gt; push\n</code></pre>"},{"location":"user-guide/comparison/#with-error-handling-and-scheduling","title":"With Error Handling and Scheduling","text":""},{"location":"user-guide/comparison/#apilinker_1","title":"ApiLinker","text":"<pre><code>from apilinker import ApiLinker\n\n# Initialize with error handling\nlinker = ApiLinker()\n\n# Add error handler\ndef handle_error(error, context):\n    print(f\"Error: {error}\")\n    return True  # Retry\n\nlinker.add_error_handler(handle_error)\n\n# Configure APIs and mapping\n# ...\n\n# Add scheduling\nlinker.add_schedule(interval_minutes=60)\n\n# Start scheduled sync\nlinker.start_scheduled_sync()\n</code></pre>"},{"location":"user-guide/comparison/#n8n-javascript","title":"n8n (JavaScript)","text":"<pre><code>// Node 1: HTTP Request (Source API)\nconst sourceResponse = await $node[\"HTTP Request\"].makeRequest({\n  url: \"https://api.source.com/users\",\n  method: \"GET\"\n});\n\n// Node 2: Function (Transform data)\nconst transformedData = sourceResponse.data.map(user =&gt; {\n  return {\n    user_id: user.id,\n    full_name: user.name\n  };\n});\n\n// Node 3: HTTP Request (Target API)\nfor (const user of transformedData) {\n  try {\n    await $node[\"HTTP Request 2\"].makeRequest({\n      url: \"https://api.target.com/users\",\n      method: \"POST\",\n      body: user\n    });\n  } catch (error) {\n    // Error handling\n    $node[\"Error Handler\"].record(error);\n\n    // Retry logic\n    await new Promise(r =&gt; setTimeout(r, 5000));\n    await $node[\"HTTP Request 2\"].makeRequest({\n      url: \"https://api.target.com/users\",\n      method: \"POST\",\n      body: user\n    });\n  }\n}\n</code></pre>"},{"location":"user-guide/comparison/#conclusion","title":"Conclusion","text":"<p>ApiLinker offers a unique combination of simplicity, flexibility, and performance with a code-first approach that makes it ideal for developers, data engineers, and researchers who need programmatic control over their API integrations.</p> <p>While visual tools like Zapier and n8n excel for business users and simple integrations, ApiLinker shines when you need deeper customization, tighter integration with Python code, or when working with complex data transformations.</p> <p>Choose ApiLinker when you value: - Lightweight implementation - Python-native development - Full code control - Reproducible data pipelines - Integration with existing Python applications</p>"},{"location":"user-guide/configuration/","title":"Configuration Guide","text":"<p>ApiLinker uses a YAML configuration format to define sources, targets, mappings, and other settings.</p>"},{"location":"user-guide/configuration/#basic-structure","title":"Basic Structure","text":"<pre><code>source:\n  # Source API configuration\ntarget:\n  # Target API configuration\nmapping:\n  # Field mapping rules\nschedule:\n  # Automation settings\nlogging:\n  # Logging preferences\n</code></pre>"},{"location":"user-guide/configuration/#source-and-target","title":"Source and Target","text":"<p>Both sections share the same structure:</p> <pre><code>source:\n  type: rest\n  base_url: https://api.example.com\n  auth:\n    type: bearer\n    token: ${API_TOKEN}\n  endpoints:\n    list_items:\n      path: /items\n      method: GET\n</code></pre>"},{"location":"user-guide/configuration/#environment-variables","title":"Environment Variables","text":"<p>You can reference environment variables using the <code>${VAR_NAME}</code> syntax. This is recommended for sensitive values if not using the Secret Manager.</p>"},{"location":"user-guide/configuration/#validation","title":"Validation","text":"<p>You can enforce schema validation on requests and responses:</p> <pre><code>validation:\n  strict_mode: true\n</code></pre>"},{"location":"user-guide/configuration/#sse-endpoint-configuration","title":"SSE Endpoint Configuration","text":"<p>ApiLinker supports Server-Sent Events (SSE) with built-in reconnect and chunked consumption controls.</p> <pre><code>source:\n  type: sse\n  base_url: https://events.example.com\n  endpoints:\n    feed:\n      path: /stream\n      method: GET\n      sse:\n        reconnect: true\n        reconnect_delay: 1.0\n        max_reconnect_attempts: 10\n        read_timeout: 60\n        decode_json: true\n        chunk_size: 50\n        backpressure_buffer_size: 500\n        drop_policy: block  # block | drop_oldest\n</code></pre> <p>Use <code>stream_sse(...)</code> for event-by-event processing and <code>consume_sse(...)</code> for chunked/backpressure-aware processing.</p>"},{"location":"user-guide/connectors/","title":"Connectors","text":""},{"location":"user-guide/connectors/#connectors-index","title":"Connectors Index","text":"<p>This page lists built-in connectors with links to source files and basic usage.</p>"},{"location":"user-guide/connectors/#scientific-connectors","title":"Scientific Connectors","text":"<ul> <li>NCBI (PubMed, GenBank) \u2014 <code>apilinker/connectors/scientific/ncbi.py</code></li> <li>Usage: <code>from apilinker import NCBIConnector</code></li> <li>arXiv \u2014 <code>apilinker/connectors/scientific/arxiv.py</code></li> <li>Usage: <code>from apilinker import ArXivConnector</code></li> <li>CrossRef \u2014 <code>apilinker/connectors/scientific/crossref.py</code></li> <li>Usage: <code>from apilinker import CrossRefConnector</code></li> <li>Semantic Scholar \u2014 <code>apilinker/connectors/scientific/semantic_scholar.py</code></li> <li>Usage: <code>from apilinker import SemanticScholarConnector</code></li> <li>PubChem \u2014 <code>apilinker/connectors/scientific/pubchem.py</code></li> <li>Usage: <code>from apilinker import PubChemConnector</code></li> <li>ORCID \u2014 <code>apilinker/connectors/scientific/orcid.py</code></li> <li>Usage: <code>from apilinker import ORCIDConnector</code></li> </ul>"},{"location":"user-guide/connectors/#general-research-connectors","title":"General Research Connectors","text":"<ul> <li>GitHub \u2014 <code>apilinker/connectors/general/github.py</code></li> <li>Usage: <code>from apilinker import GitHubConnector</code></li> <li>NASA \u2014 <code>apilinker/connectors/general/nasa.py</code></li> <li>Usage: <code>from apilinker import NASAConnector</code></li> <li>SSE - <code>apilinker/connectors/general/sse.py</code></li> <li>Usage: <code>from apilinker import SSEConnector</code></li> </ul>"},{"location":"user-guide/connectors/#notes","title":"Notes","text":"<ul> <li>All connectors inherit from the common <code>ApiConnector</code> base and expose endpoints via their <code>.endpoints</code> mapping.</li> <li>Authentication varies per provider (see each source file for details).</li> <li>Respect rate limits and terms of service; include a descriptive User-Agent where applicable.</li> </ul> <p>The following connectors are exported at the top level in <code>apilinker/__init__.py</code> and available for import as shown above.</p>"},{"location":"user-guide/cookbook/","title":"ApiLinker Cookbook","text":"<p>This cookbook provides practical recipes for common integration tasks.</p>"},{"location":"user-guide/cookbook/#working-with-pagination","title":"Working with Pagination","text":""},{"location":"user-guide/cookbook/#problem","title":"Problem","text":"<p>You need to fetch large datasets that span multiple pages.</p>"},{"location":"user-guide/cookbook/#solution","title":"Solution","text":"<pre><code>linker.add_source(\n    type=\"rest\",\n    base_url=\"https://api.example.com\",\n    endpoints={\n        \"get_users\": {\n            \"path\": \"/users\",\n            \"method\": \"GET\",\n            # Pagination configuration based on the API response format\n            \"pagination\": {\n                \"data_path\": \"data\",                # Path to items array in response\n                \"next_page_path\": \"meta.next_page\", # Path to next page URL/token\n                \"page_param\": \"page\"                # Query param name for page number\n            }\n        }\n    }\n)\n\n# ApiLinker will automatically handle pagination\nall_users = linker.fetch(\"get_users\")\n</code></pre>"},{"location":"user-guide/cookbook/#common-pagination-patterns","title":"Common Pagination Patterns","text":""},{"location":"user-guide/cookbook/#page-number-pagination","title":"Page Number Pagination","text":"<pre><code>\"pagination\": {\n    \"data_path\": \"data\",\n    \"page_param\": \"page\"  # Will increment: page=1, page=2, etc.\n}\n</code></pre>"},{"location":"user-guide/cookbook/#next-url-pagination","title":"Next URL Pagination","text":"<pre><code>\"pagination\": {\n    \"data_path\": \"data\",\n    \"next_page_path\": \"links.next\"  # Response contains full next URL\n}\n</code></pre>"},{"location":"user-guide/cookbook/#implementing-robust-error-handling","title":"Implementing Robust Error Handling","text":""},{"location":"user-guide/cookbook/#problem_1","title":"Problem","text":"<p>You need reliable API integrations that can handle service outages and temporary network issues.</p>"},{"location":"user-guide/cookbook/#solution_1","title":"Solution","text":"<p>Use APILinker's robust error handling and recovery system:</p> <pre><code># In your config.yaml\nerror_handling:\n  # Configure circuit breakers to prevent cascading failures\n  circuit_breakers:\n    source_customers_api:\n      failure_threshold: 5        # Open circuit after 5 consecutive failures\n      reset_timeout_seconds: 60   # Wait 60 seconds before testing service again\n      half_open_max_calls: 1      # Allow 1 test call in half-open state\n\n  # Configure error handling strategies by error category\n  recovery_strategies:\n    network:                      # Network connectivity issues\n      - exponential_backoff       # First try with increasing delays\n      - circuit_breaker           # Then use circuit breaker if still failing\n    # rate limiting: not built-in; use server guidance and retries\n      - exponential_backoff       # Back off and retry\n    server:                       # Server errors (5xx)\n      - exponential_backoff\n      - circuit_breaker\n    timeout:                      # Request timeout errors\n      - exponential_backoff\n\n  # Configure Dead Letter Queue for failed operations\n  dlq:\n    directory: \"./dlq\"           # Store failed operations here\n</code></pre>"},{"location":"user-guide/cookbook/#accessing-error-analytics","title":"Accessing Error Analytics","text":"<pre><code>from apilinker import ApiLinker\n\n# Initialize with error handling config\nlinker = ApiLinker(config_path=\"config.yaml\")\n\n# Get error statistics\nanalytics = linker.get_error_analytics()\nprint(f\"Recent error rate: {analytics['recent_error_rate']} errors/minute\")\nprint(f\"Most common errors: {analytics['top_errors']}\")\n\n# Check for failed operations in DLQ\nitems = linker.dlq.get_items(limit=10)\nif items:\n    print(f\"Found {len(items)} failed operations in DLQ\")\n\n    # Process specific types of failed operations\n    results = linker.process_dlq(operation_type=\"source_customers_api\")\n    print(f\"Processed {results['successful']} items successfully\")\n</code></pre>"},{"location":"user-guide/cookbook/#handling-different-error-types","title":"Handling Different Error Types","text":"<pre><code>from apilinker.core.error_handling import ErrorCategory, RecoveryStrategy\n\n# Configure specific recovery strategies programmatically\nlinker.error_recovery_manager.set_strategy(\n    ErrorCategory.RATE_LIMIT,\n    [\n        RecoveryStrategy.EXPONENTIAL_BACKOFF,\n        RecoveryStrategy.SKIP  # Skip rate-limited operations\n    ],\n    operation_type=\"fetch_users\"  # Only for this operation\n)\n\n# Execute with enhanced error handling\ntry:\n    result = linker.sync(\"fetch_users\", \"create_users\")\n    print(f\"Synced {result.count} users\")\n\n    # Check if any errors occurred\n    if not result.success:\n        print(f\"Completed with {len(result.errors)} errors\")\n        for error in result.errors:\n            print(f\"- {error['message']}\")\n\nexcept Exception as e:\n    print(f\"Critical error: {str(e)}\")\n</code></pre>"},{"location":"user-guide/cookbook/#explanation","title":"Explanation","text":"<p>The ApiConnector's _handle_pagination method automatically: 1. Extracts data items from each response using the data_path 2. Determines the next page using next_page_path if available 3. Increments page parameters for subsequent requests 4. Combines results from all pages</p>"},{"location":"user-guide/cookbook/#handling-api-rate-limits","title":"Handling API Rate Limits","text":""},{"location":"user-guide/cookbook/#problem_2","title":"Problem","text":"<p>Your API requests are getting rate limited (HTTP 429).</p>"},{"location":"user-guide/cookbook/#solution_2","title":"Solution","text":"<pre><code>linker.add_source(\n    type=\"rest\",\n    base_url=\"https://api.example.com\",\n    # Configure retry settings\n    retry_count=3,         # Try 3 times before failing\n    retry_delay=2,         # Wait 2 seconds between retries\n    # Add longer timeout for slow APIs\n    timeout=30,            # 30 second timeout\n    endpoints={\n        # Your endpoints here\n    }\n)\n\n# For manual handling of 429s with backoff\ndef handle_rate_limits(func):\n    def wrapper(*args, **kwargs):\n        max_attempts = 3\n        for attempt in range(max_attempts):\n            try:\n                return func(*args, **kwargs)\n            except Exception as e:\n                if \"rate limit\" in str(e).lower() and attempt &lt; max_attempts - 1:\n                    wait_time = (attempt + 1) * 5  # 5, 10, 15 seconds\n                    print(f\"Rate limited. Waiting {wait_time} seconds...\")\n                    time.sleep(wait_time)\n                else:\n                    raise\n    return wrapper\n\n# Apply to your function\n@handle_rate_limits\ndef fetch_data():\n    return linker.fetch(\"get_data\")\n</code></pre>"},{"location":"user-guide/cookbook/#explanation_1","title":"Explanation","text":"<ul> <li>The retry mechanism is built into ApiConnector for temporary failures</li> <li>For specific rate limit handling, implement a decorator or wrapper function in your app</li> <li>The connector will automatically use exponential backoff between retries</li> </ul>"},{"location":"user-guide/cookbook/#transforming-nested-json-structures","title":"Transforming Nested JSON Structures","text":""},{"location":"user-guide/cookbook/#problem_3","title":"Problem","text":"<p>You need to extract and transform data from complex nested JSON structures.</p>"},{"location":"user-guide/cookbook/#solution_3","title":"Solution","text":"<pre><code>linker.add_mapping(\n    source=\"get_data\",\n    target=\"save_data\",\n    fields=[\n        # Access nested properties with dot notation\n        {\"source\": \"user.profile.name\", \"target\": \"fullName\"},\n\n        # Access array items with indices\n        {\"source\": \"addresses[0].street\", \"target\": \"primary_address\"},\n\n        # Custom transformer for nested objects\n        {\n            \"source\": \"metadata\",  # This is an object\n            \"target\": \"meta_info\", \n            \"transform\": \"flatten_metadata\"\n        }\n    ]\n)\n\n# Define transformer for nested object\ndef flatten_metadata(value, **kwargs):\n    if not value or not isinstance(value, dict):\n        return {}\n\n    result = {}\n    # Extract selected fields with prefixes\n    for key in [\"created\", \"updated\", \"status\"]:\n        if key in value:\n            result[f\"meta_{key}\"] = value[key]\n\n    # Flatten nested object\n    if \"details\" in value and isinstance(value[\"details\"], dict):\n        for k, v in value[\"details\"].items():\n            result[f\"detail_{k}\"] = v\n\n    return result\n\n# Register the transformer\nlinker.mapper.register_transformer(\"flatten_metadata\", flatten_metadata)\n</code></pre>"},{"location":"user-guide/cookbook/#explanation_2","title":"Explanation","text":"<ul> <li>Dot notation accesses nested properties</li> <li>Array indices access specific items in arrays</li> <li>Custom transformers handle complex transformations</li> </ul>"},{"location":"user-guide/cookbook/#syncing-only-changed-records","title":"Syncing Only Changed Records","text":""},{"location":"user-guide/cookbook/#problem_4","title":"Problem","text":"<p>You want to sync only records that have changed since the last sync.</p>"},{"location":"user-guide/cookbook/#solution_4","title":"Solution","text":"<pre><code># Method 1: Using template variables\nlinker.add_source(\n    type=\"rest\",\n    base_url=\"https://api.example.com\",\n    endpoints={\n        \"get_users\": {\n            \"path\": \"/users\",\n            \"method\": \"GET\",\n            \"params\": {\n                \"updated_since\": \"{{last_sync}}\",  # Template variable\n                \"sort\": \"updated_at\"\n            }\n        }\n    }\n)\n\n# Method 2: Using a filter function\ndef filter_changed_records(data, **kwargs):\n    last_sync = kwargs.get(\"last_sync\")\n    if not last_sync:\n        return data  # Return all if no last sync\n\n    # Convert to datetime for comparison\n    from datetime import datetime\n    last_sync_dt = datetime.fromisoformat(last_sync.replace('Z', '+00:00'))\n\n    # Filter items updated since last sync\n    return [\n        item for item in data\n        if \"updated_at\" in item and \n           datetime.fromisoformat(item[\"updated_at\"].replace('Z', '+00:00')) &gt; last_sync_dt\n    ]\n\n# Register the filter\nlinker.add_source_processor(\"get_users\", filter_changed_records)\n\n# Get last sync time from storage\nlast_sync_time = linker.get_last_sync_time() or \"2023-01-01T00:00:00Z\"\n\n# Run the sync with context\nresult = linker.sync(context={\"last_sync\": last_sync_time})\n</code></pre>"},{"location":"user-guide/cookbook/#explanation_3","title":"Explanation","text":"<ul> <li>Template variables like <code>{{last_sync}}</code> are replaced with values at runtime</li> <li>Source processors can filter or modify data before mapping</li> <li>Context variables can be passed to processors</li> </ul>"},{"location":"user-guide/cookbook/#working-with-files-and-binary-data","title":"Working with Files and Binary Data","text":""},{"location":"user-guide/cookbook/#problem_5","title":"Problem","text":"<p>You need to handle file uploads or downloads.</p>"},{"location":"user-guide/cookbook/#solution_5","title":"Solution","text":"<pre><code># File download\nlinker.add_source(\n    type=\"rest\",\n    base_url=\"https://api.example.com\",\n    endpoints={\n        \"get_document\": {\n            \"path\": \"/documents/{doc_id}\",\n            \"method\": \"GET\",\n            \"response_type\": \"binary\"  # Treat response as binary\n        }\n    }\n)\n\n# Custom transformer to save files\ndef save_file(binary_data, **kwargs):\n    if not binary_data:\n        return None\n\n    filename = kwargs.get(\"filename\", \"document.pdf\")\n    file_path = f\"downloads/{filename}\"\n\n    # Create directory if it doesn't exist\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n\n    # Write binary data to file\n    with open(file_path, \"wb\") as f:\n        f.write(binary_data)\n\n    return {\n        \"path\": file_path,\n        \"size\": len(binary_data),\n        \"status\": \"downloaded\"\n    }\n\n# Register transformer\nlinker.mapper.register_transformer(\"save_file\", save_file)\n\n# Use it in a mapping\nlinker.add_mapping(\n    source=\"get_document\",\n    target=\"log_download\",\n    fields=[\n        {\n            \"source\": \"_response\",  # Special field with raw response\n            \"target\": \"file_info\",\n            \"transform\": \"save_file\",\n            \"filename\": \"report.pdf\"\n        }\n    ]\n)\n\n# File upload\nlinker.add_target(\n    type=\"rest\",\n    base_url=\"https://api.destination.com\",\n    endpoints={\n        \"upload_file\": {\n            \"path\": \"/upload\",\n            \"method\": \"POST\",\n            \"headers\": {\n                \"Content-Type\": \"application/octet-stream\"\n            }\n        }\n    }\n)\n\n# Read file transformer\ndef read_file(file_path, **kwargs):\n    if not file_path or not isinstance(file_path, str):\n        return None\n\n    try:\n        with open(file_path, \"rb\") as f:\n            return f.read()\n    except Exception as e:\n        print(f\"Error reading file: {e}\")\n        return None\n\n# Register transformer\nlinker.mapper.register_transformer(\"read_file\", read_file)\n</code></pre>"},{"location":"user-guide/cookbook/#explanation_4","title":"Explanation","text":"<ul> <li>Set <code>response_type: \"binary\"</code> for file downloads</li> <li>Use <code>Content-Type: \"application/octet-stream\"</code> for file uploads</li> <li>Use custom transformers to handle file operations</li> </ul>"},{"location":"user-guide/cookbook/#conditional-mapping","title":"Conditional Mapping","text":""},{"location":"user-guide/cookbook/#problem_6","title":"Problem","text":"<p>You need to apply different mappings based on data conditions.</p>"},{"location":"user-guide/cookbook/#solution_6","title":"Solution","text":"<pre><code>linker.add_mapping(\n    source=\"get_products\",\n    target=\"create_item\",\n    fields=[\n        # Basic fields always included\n        {\"source\": \"id\", \"target\": \"product_id\"},\n        {\"source\": \"name\", \"target\": \"title\"},\n\n        # Only include if value exists\n        {\n            \"source\": \"description\",\n            \"target\": \"description\",\n            \"condition\": {\n                \"field\": \"description\",\n                \"operator\": \"exists\"\n            }\n        },\n\n        # Apply different mapping based on status\n        {\n            \"source\": \"status\",\n            \"target\": \"status\",\n            \"transform\": \"map_active_status\",\n            \"condition\": {\n                \"field\": \"status\",\n                \"operator\": \"equals\",\n                \"value\": \"active\"\n            }\n        },\n\n        # Apply different mapping for inactive items\n        {\n            \"target\": \"status\",\n            \"value\": \"discontinued\",\n            \"condition\": {\n                \"field\": \"status\",\n                \"operator\": \"equals\",\n                \"value\": \"inactive\"\n            }\n        },\n\n        # Complex condition using multiple fields\n        {\n            \"source\": \"price\",\n            \"target\": \"discount_price\",\n            \"transform\": \"calculate_discount\",\n            \"condition\": {\n                \"field\": \"on_sale\",\n                \"operator\": \"equals\",\n                \"value\": True\n            },\n            \"discount_percent\": 15\n        }\n    ]\n)\n\n# Status mapper transformer\ndef map_active_status(value, **kwargs):\n    status_map = {\n        \"active\": \"in_stock\",\n        \"pending\": \"coming_soon\"\n    }\n    return status_map.get(value, value)\n\n# Calculate discount transformer\ndef calculate_discount(value, **kwargs):\n    if not isinstance(value, (int, float)) or value &lt;= 0:\n        return value\n\n    discount = kwargs.get(\"discount_percent\", 10)\n    return round(value * (1 - discount / 100), 2)\n\n# Register transformers\nlinker.mapper.register_transformer(\"map_active_status\", map_active_status)\nlinker.mapper.register_transformer(\"calculate_discount\", calculate_discount)\n</code></pre>"},{"location":"user-guide/cookbook/#explanation_5","title":"Explanation","text":"<ul> <li>The <code>condition</code> property controls when a field mapping is applied</li> <li>Supported operators: <code>exists</code>, <code>not_exists</code>, <code>equals</code>, <code>not_equals</code>, <code>in</code>, <code>not_in</code></li> <li>You can use fixed values with <code>value</code> property</li> <li>Custom transformers can use additional parameters</li> </ul>"},{"location":"user-guide/cookbook/#using-environment-variables-for-credentials","title":"Using Environment Variables for Credentials","text":""},{"location":"user-guide/cookbook/#problem_7","title":"Problem","text":"<p>You need to securely manage API credentials without hardcoding them.</p>"},{"location":"user-guide/cookbook/#solution_7","title":"Solution","text":"<pre><code># Method 1: Environment variables in configuration\nlinker.add_source(\n    type=\"rest\",\n    base_url=\"https://api.example.com\",\n    auth={\n        \"type\": \"bearer\",\n        \"token\": \"${API_TOKEN}\"  # Will be replaced with API_TOKEN env var\n    }\n)\n\n# Method 2: Load from .env file\nfrom dotenv import load_dotenv\nload_dotenv()  # Loads variables from .env file\n\n# Method 3: Set variables in script (for testing)\nimport os\nos.environ[\"API_TOKEN\"] = \"your_token\"  # Only for testing!\n\n# Method 4: Use a credential manager\ndef get_credential(name):\n    # Implement your secure credential retrieval logic here\n    # Examples: AWS Secrets Manager, HashiCorp Vault, etc.\n    return \"secure_credential\"\n\n# Use retrieved credentials\nlinker.add_source(\n    type=\"rest\",\n    base_url=\"https://api.example.com\",\n    auth={\n        \"type\": \"bearer\",\n        \"token\": get_credential(\"api_token\")\n    }\n)\n</code></pre>"},{"location":"user-guide/cookbook/#explanation_6","title":"Explanation","text":"<ul> <li>Environment variables are the simplest secure method</li> <li>.env files are convenient for development</li> <li>Never hardcode credentials in source code</li> <li>Consider using a dedicated secrets manager for production</li> </ul>"},{"location":"user-guide/cookbook/#error-handling-and-validation","title":"Error Handling and Validation","text":""},{"location":"user-guide/cookbook/#problem_8","title":"Problem","text":"<p>You need to handle errors gracefully and validate data.</p>"},{"location":"user-guide/cookbook/#solution_8","title":"Solution","text":"<pre><code># Custom error handler\ndef handle_sync_error(error, context):\n    import logging\n    logging.error(f\"Sync error: {error}\")\n\n    # Send notification\n    send_notification(f\"Sync failed: {error}\")\n\n    # Determine whether to retry based on error type\n    if \"rate limit\" in str(error).lower():\n        # Wait longer for rate limits\n        import time\n        time.sleep(60)\n        return True  # Retry\n\n    if \"connection\" in str(error).lower():\n        # Retry connection errors up to 3 times\n        attempt = context.get(\"attempt\", 1)\n        if attempt &lt;= 3:\n            context[\"attempt\"] = attempt + 1\n            return True  # Retry\n\n    return False  # Don't retry other errors\n\n# Register error handler\nlinker.add_error_handler(handle_sync_error)\n\n# Data validation\ndef validate_customer_data(data, **kwargs):\n    if not isinstance(data, list):\n        raise ValueError(\"Expected a list of customers\")\n\n    valid_items = []\n    for item in data:\n        # Skip invalid items\n        if not isinstance(item, dict):\n            continue\n\n        # Require email field\n        if \"email\" not in item or not item[\"email\"]:\n            continue\n\n        # Format validation\n        if \"phone\" in item and item[\"phone\"]:\n            # Clean phone number\n            item[\"phone\"] = ''.join(c for c in item[\"phone\"] if c.isdigit())\n\n        # Normalize fields\n        if \"name\" in item and item[\"name\"]:\n            item[\"name\"] = item[\"name\"].strip().title()\n\n        valid_items.append(item)\n\n    return valid_items\n\n# Register validator\nlinker.add_source_processor(\"get_customers\", validate_customer_data)\n</code></pre>"},{"location":"user-guide/cookbook/#explanation_7","title":"Explanation","text":"<ul> <li>Error handlers determine whether to retry after errors</li> <li>Error context is preserved between retries</li> <li>Source processors can validate and normalize data</li> <li>Validators can filter out invalid records</li> </ul>"},{"location":"user-guide/cookbook/#logging-and-debugging","title":"Logging and Debugging","text":""},{"location":"user-guide/cookbook/#problem_9","title":"Problem","text":"<p>You need to track API operations and diagnose issues.</p>"},{"location":"user-guide/cookbook/#solution_9","title":"Solution","text":"<pre><code>import logging\nimport time\n\n# Configure logging for ApiLinker\nlogging.basicConfig(\n    level=logging.DEBUG,  # Set to DEBUG for detailed logs\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(\"apilinker.log\"),\n        logging.StreamHandler()\n    ]\n)\n\n# Initialize ApiLinker with log level\nlinker = ApiLinker(log_level=\"DEBUG\", log_file=\"apilinker.log\")\n\n# Add custom timing measurement\nclass SimpleTimer:\n    def __init__(self):\n        self.start_times = {}\n        self.results = {}\n\n    def start(self, operation_name):\n        self.start_times[operation_name] = time.time()\n\n    def end(self, operation_name):\n        if operation_name in self.start_times:\n            duration = time.time() - self.start_times[operation_name]\n            self.results[operation_name] = duration\n            logging.info(f\"{operation_name} completed in {duration:.2f} seconds\")\n            return duration\n        return None\n\n# Use the timer in your code\ntimer = SimpleTimer()\n\n# Create a wrapper for timing operations\ndef timed_operation(func):\n    def wrapper(*args, **kwargs):\n        operation_name = func.__name__\n        timer.start(operation_name)\n        try:\n            result = func(*args, **kwargs)\n            timer.end(operation_name)\n            return result\n        except Exception as e:\n            logging.error(f\"Error in {operation_name}: {e}\")\n            timer.end(operation_name)\n            raise\n    return wrapper\n\n# Use the decorator for operations you want to time\n@timed_operation\ndef fetch_and_process():\n    # Fetch data\n    logging.info(\"Fetching data from source\")\n    source_data = linker.fetch(\"get_data\")\n\n    # Log response summary (without sensitive data)\n    logging.info(f\"Fetched {len(source_data) if isinstance(source_data, list) else 1} records\")\n\n    # Process data\n    logging.info(\"Processing data\")\n    result = linker.sync()\n\n    # Log processing results\n    logging.info(f\"Processed {result.count} records\")\n    if result.errors:\n        logging.warning(f\"Encountered {len(result.errors)} errors\")\n\n    return result\n\n# Execute with debug logging\ntry:\n    result = fetch_and_process()\n    print(f\"Sync completed successfully: {result.count} records\")\n    print(f\"Operation times: {timer.results}\")\nexcept Exception as e:\n    print(f\"Sync failed: {e}\")\n</code></pre>"},{"location":"user-guide/cookbook/#explanation_8","title":"Explanation","text":"<ul> <li>ApiLinker has built-in logging that can be configured with different levels</li> <li>You can create simple timing utilities to measure performance</li> <li>Use the Python logging module for structured logs</li> <li>Use decorators to consistently measure and log operations</li> </ul>"},{"location":"user-guide/docker/","title":"Docker Usage","text":"<p>ApiLinker provides Docker support for containerized deployments.</p>"},{"location":"user-guide/docker/#quick-start","title":"Quick Start","text":""},{"location":"user-guide/docker/#pull-from-github-container-registry","title":"Pull from GitHub Container Registry","text":"<pre><code>docker pull ghcr.io/kkartas/apilinker:latest\n</code></pre>"},{"location":"user-guide/docker/#run-a-sync","title":"Run a Sync","text":"<pre><code>docker run -v $(pwd)/config.yaml:/app/config.yaml ghcr.io/kkartas/apilinker:latest apilinker sync --config /app/config.yaml\n</code></pre>"},{"location":"user-guide/docker/#building-from-source","title":"Building from Source","text":"<pre><code>git clone https://github.com/kkartas/APILinker.git\ncd APILinker\ndocker build -t apilinker:local .\n</code></pre>"},{"location":"user-guide/docker/#environment-variables","title":"Environment Variables","text":"<p>Pass secrets via environment variables:</p> <pre><code>docker run \\\n  -e SOURCE_API_TOKEN=your_token \\\n  -e TARGET_API_KEY=your_key \\\n  -v $(pwd)/config.yaml:/app/config.yaml \\\n  ghcr.io/kkartas/apilinker:latest \\\n  apilinker sync --config /app/config.yaml\n</code></pre>"},{"location":"user-guide/docker/#docker-compose","title":"Docker Compose","text":"<p>For complex deployments, use Docker Compose:</p> <pre><code>version: '3.8'\nservices:\n  apilinker:\n    image: ghcr.io/kkartas/apilinker:latest\n    volumes:\n      - ./config.yaml:/app/config.yaml\n      - ./logs:/app/logs\n    environment:\n      - SOURCE_API_TOKEN=${SOURCE_API_TOKEN}\n      - TARGET_API_KEY=${TARGET_API_KEY}\n    command: apilinker run --config /app/config.yaml\n</code></pre> <p>Run with:</p> <pre><code>docker-compose up\n</code></pre>"},{"location":"user-guide/docker/#scheduled-syncs","title":"Scheduled Syncs","text":"<p>For long-running scheduled syncs:</p> <pre><code>docker run -d \\\n  --name apilinker-scheduler \\\n  -v $(pwd)/config.yaml:/app/config.yaml \\\n  ghcr.io/kkartas/apilinker:latest \\\n  apilinker run --config /app/config.yaml\n</code></pre>"},{"location":"user-guide/docker/#cicd-integration","title":"CI/CD Integration","text":"<p>Docker images are automatically built and published via GitHub Actions on every push to <code>main</code>.</p> <p>View Dockerfile: <code>Dockerfile</code></p>"},{"location":"user-guide/error-handling/","title":"Error Handling","text":""},{"location":"user-guide/error-handling/#error-handling-and-recovery","title":"Error Handling and Recovery","text":"<p>This page explains how to configure circuit breakers, retries, exponential backoff, and the Dead Letter Queue (DLQ).</p>"},{"location":"user-guide/error-handling/#quick-start","title":"Quick start","text":"<p>Add error handling to your config file:</p> <pre><code>error_handling:\n  circuit_breakers:\n    source_list_items:\n      failure_threshold: 5\n      reset_timeout_seconds: 60\n      half_open_max_calls: 1\n    target_create_item:\n      failure_threshold: 3\n      reset_timeout_seconds: 30\n  recovery_strategies:\n    network: [exponential_backoff]\n    timeout: [exponential_backoff]\n    server: [circuit_breaker, exponential_backoff]\n    client: [fail_fast]\n  dlq:\n    directory: .apilinker_dlq\n</code></pre>"},{"location":"user-guide/error-handling/#programmatic-usage","title":"Programmatic usage","text":"<pre><code>from apilinker import ApiLinker\n\nlinker = ApiLinker(\n    error_handling_config={\n        \"circuit_breakers\": {\"source_list\": {\"failure_threshold\": 5}},\n        \"recovery_strategies\": {\"server\": [\"circuit_breaker\", \"exponential_backoff\"]},\n        \"dlq\": {\"directory\": \".apilinker_dlq\"},\n    }\n)\n\n# Later, inspect analytics\nsummary = linker.get_error_analytics()\nprint(summary)\n\n# Process DLQ items\nresults = linker.process_dlq(limit=10)\n</code></pre> <p>See <code>apilinker/core/error_handling.py</code> for supported strategies and categories.</p>"},{"location":"user-guide/faq/","title":"Frequently Asked Questions (FAQ)","text":"<p>This document addresses common questions about ApiLinker.</p>"},{"location":"user-guide/faq/#general-questions","title":"General Questions","text":""},{"location":"user-guide/faq/#what-is-apilinker","title":"What is ApiLinker?","text":"<p>ApiLinker is a Python library that provides a universal bridge for connecting, mapping, and automating data transfer between any two REST APIs. It allows you to configure API integrations without writing repetitive boilerplate code.</p>"},{"location":"user-guide/faq/#what-python-versions-are-supported","title":"What Python versions are supported?","text":"<p>ApiLinker supports Python 3.8 and above.</p>"},{"location":"user-guide/faq/#is-apilinker-free-to-use","title":"Is ApiLinker free to use?","text":"<p>Yes, ApiLinker is open-source software released under the MIT license, which allows for free use, modification, and distribution.</p>"},{"location":"user-guide/faq/#can-i-use-apilinker-in-commercial-projects","title":"Can I use ApiLinker in commercial projects?","text":"<p>Yes, the MIT license allows for commercial use.</p>"},{"location":"user-guide/faq/#installation-setup","title":"Installation &amp; Setup","text":""},{"location":"user-guide/faq/#how-do-i-install-apilinker","title":"How do I install ApiLinker?","text":"<pre><code>pip install apilinker\n</code></pre>"},{"location":"user-guide/faq/#why-am-i-getting-an-error-during-installation","title":"Why am I getting an error during installation?","text":"<p>Common installation issues include:</p> <ol> <li>Python version: Make sure you're using Python 3.8 or newer</li> <li>Permission issues: Try using <code>pip install --user apilinker</code> or use a virtual environment</li> <li>Dependency conflicts: Create a clean virtual environment and try installing again</li> </ol>"},{"location":"user-guide/faq/#how-can-i-verify-the-installation","title":"How can I verify the installation?","text":"<pre><code>import apilinker\nprint(apilinker.__version__)\n</code></pre>"},{"location":"user-guide/faq/#configuration","title":"Configuration","text":""},{"location":"user-guide/faq/#how-do-i-connect-to-an-api-that-requires-a-custom-authentication-method","title":"How do I connect to an API that requires a custom authentication method?","text":"<p>You can create a custom authentication plugin by extending the <code>AuthPlugin</code> class:</p> <pre><code>from apilinker.core.plugins import AuthPlugin\n\nclass CustomAuth(AuthPlugin):\n    plugin_name = \"custom_auth\"\n\n    def authenticate(self, **kwargs):\n        # Custom authentication logic\n        return {\n            \"headers\": {\"X-Custom-Auth\": generate_auth_header(kwargs)},\n            \"type\": \"custom\"\n        }\n</code></pre>"},{"location":"user-guide/faq/#can-i-use-apilinker-with-graphql-apis","title":"Can I use ApiLinker with GraphQL APIs?","text":"<p>Yes, you can create a custom connector plugin for GraphQL or use the REST connector with POST requests and GraphQL queries in the body.</p>"},{"location":"user-guide/faq/#how-do-i-handle-api-rate-limits","title":"How do I handle API rate limits?","text":"<p>ApiLinker does not include built-in rate limiting. Use provider guidance, exponential backoff, and retries to handle 429 responses.</p> <pre><code>source:\n  type: rest\n  base_url: \"https://api.example.com\"\n  rate_limit:\n    requests_per_second: 5\n  retry:\n    max_attempts: 3\n    delay_seconds: 2\n    backoff_factor: 1.5\n    status_codes: [429, 500, 502, 503, 504]\n</code></pre>"},{"location":"user-guide/faq/#data-mapping","title":"Data Mapping","text":""},{"location":"user-guide/faq/#how-do-i-transform-data-between-apis","title":"How do I transform data between APIs?","text":"<p>Use field mappings with transformers:</p> <pre><code>fields:\n  - source: user.profile.name\n    target: contact.fullName\n    transform: uppercase\n</code></pre>"},{"location":"user-guide/faq/#can-i-apply-multiple-transformations-to-a-single-field","title":"Can I apply multiple transformations to a single field?","text":"<p>Yes, specify them as a list:</p> <pre><code>fields:\n  - source: tags\n    target: categories\n    transform:\n      - lowercase\n      - strip\n      - none_if_empty\n</code></pre>"},{"location":"user-guide/faq/#how-do-i-create-custom-data-transformers","title":"How do I create custom data transformers?","text":"<p>Register a custom transformer function:</p> <pre><code>def phone_formatter(value, **kwargs):\n    if not value:\n        return \"\"\n    digits = ''.join(c for c in value if c.isdigit())\n    if len(digits) == 10:\n        return f\"({digits[0:3]}) {digits[3:6]}-{digits[6:10]}\"\n    return value\n\nlinker.mapper.register_transformer(\"phone_formatter\", phone_formatter)\n</code></pre>"},{"location":"user-guide/faq/#scheduling","title":"Scheduling","text":""},{"location":"user-guide/faq/#how-do-i-schedule-a-sync-to-run-periodically","title":"How do I schedule a sync to run periodically?","text":"<pre><code># Run every hour\nlinker.add_schedule(interval_minutes=60)\n\n# Or use cron expression\nlinker.add_schedule(cron_expression=\"0 */6 * * *\")  # Every 6 hours\n\n# Start the scheduler\nlinker.start_scheduled_sync()\n</code></pre>"},{"location":"user-guide/faq/#how-do-i-stop-a-scheduled-sync","title":"How do I stop a scheduled sync?","text":"<pre><code>linker.stop_scheduled_sync()\n</code></pre>"},{"location":"user-guide/faq/#can-i-run-multiple-schedules-with-different-frequencies","title":"Can I run multiple schedules with different frequencies?","text":"<p>Yes, you can create multiple ApiLinker instances with different schedules.</p>"},{"location":"user-guide/faq/#error-handling","title":"Error Handling","text":""},{"location":"user-guide/faq/#how-do-i-handle-errors-during-sync","title":"How do I handle errors during sync?","text":"<p>You can provide an error handler function:</p> <pre><code>def handle_error(error, context):\n    print(f\"Error during sync: {error}\")\n    print(f\"Context: {context}\")\n    # Log error, send notification, etc.\n    return True  # Return True to retry, False to abort\n\nlinker.add_error_handler(handle_error)\n</code></pre>"},{"location":"user-guide/faq/#how-can-i-debug-issues-with-my-api-connections","title":"How can I debug issues with my API connections?","text":"<p>Enable debug logging:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Or in configuration\nlinker = ApiLinker(debug=True)\n</code></pre>"},{"location":"user-guide/faq/#performance","title":"Performance","text":""},{"location":"user-guide/faq/#how-can-i-optimize-apilinker-for-large-data-transfers","title":"How can I optimize ApiLinker for large data transfers?","text":"<ol> <li>Use pagination settings appropriate for the API</li> <li>Set batch sizes for processing large datasets</li> <li>Consider using async operations for concurrent requests</li> </ol> <pre><code>source:\n  endpoints:\n    get_data:\n      pagination:\n        limit: 1000  # Request larger page sizes\n      batch_size: 500  # Process in batches\n</code></pre>"},{"location":"user-guide/faq/#does-apilinker-support-caching","title":"Does ApiLinker support caching?","text":"<p>Yes, ApiLinker includes response caching capabilities:</p> <pre><code>source:\n  cache:\n    enabled: true\n    ttl: 3600  # Cache TTL in seconds\n</code></pre>"},{"location":"user-guide/faq/#contributing","title":"Contributing","text":""},{"location":"user-guide/faq/#how-can-i-contribute-to-apilinker","title":"How can I contribute to ApiLinker?","text":"<ol> <li>Fork the repository on GitHub</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Add tests for your changes</li> <li>Submit a pull request</li> </ol>"},{"location":"user-guide/faq/#where-can-i-report-bugs-or-request-features","title":"Where can I report bugs or request features?","text":"<p>Report issues on the GitHub Issues page.</p>"},{"location":"user-guide/faq/#advanced-usage","title":"Advanced Usage","text":""},{"location":"user-guide/faq/#can-apilinker-handle-binary-data-or-file-transfers","title":"Can ApiLinker handle binary data or file transfers?","text":"<p>Yes, ApiLinker can handle binary data transfers. Configure the content type and use appropriate encodings:</p> <pre><code>target:\n  endpoints:\n    upload_file:\n      path: /files/upload\n      method: POST\n      headers:\n        Content-Type: application/octet-stream\n</code></pre>"},{"location":"user-guide/faq/#is-it-possible-to-extend-apilinker-with-custom-plugins","title":"Is it possible to extend ApiLinker with custom plugins?","text":"<p>Yes, ApiLinker's plugin architecture allows for extending all major components:</p> <ul> <li>Create custom transformers</li> <li>Create custom connectors for different API types</li> <li>Create custom authentication methods</li> <li>Create custom validation rules</li> </ul> <p>See the Extending with Plugins documentation for details.</p>"},{"location":"user-guide/mapping/","title":"Mapping &amp; Transformation","text":"<p>The mapping engine is the core of ApiLinker, allowing you to transform data structures between APIs.</p>"},{"location":"user-guide/mapping/#field-mapping","title":"Field Mapping","text":"<p>Define mappings in the <code>mapping</code> section:</p> <pre><code>mapping:\n  - source: source_endpoint\n    target: target_endpoint\n    fields:\n      - source: id\n        target: external_id\n</code></pre>"},{"location":"user-guide/mapping/#transformations","title":"Transformations","text":"<p>Apply transformations to modify data during sync.</p> <pre><code>- source: created_at\n  target: timestamp\n  transform: iso_to_timestamp\n</code></pre>"},{"location":"user-guide/mapping/#built-in-transformers","title":"Built-in Transformers","text":"<ul> <li><code>lowercase</code>, <code>uppercase</code>, <code>strip</code></li> <li><code>iso_to_timestamp</code>, <code>timestamp_to_iso</code></li> <li><code>to_int</code>, <code>to_float</code>, <code>to_bool</code></li> <li><code>json_parse</code>, <code>json_stringify</code></li> </ul>"},{"location":"user-guide/mapping/#conditional-mapping","title":"Conditional Mapping","text":"<p>Map fields only when specific conditions are met:</p> <pre><code>- source: status\n  target: active\n  condition:\n    field: status\n    operator: eq\n    value: active\n</code></pre>"},{"location":"user-guide/monitoring/","title":"Monitoring and Alerting","text":"<p>ApiLinker provides a robust monitoring system to track the health of your integrations and alert you when issues arise.</p>"},{"location":"user-guide/monitoring/#overview","title":"Overview","text":"<p>The monitoring system allows you to: - Perform health checks on connectors and components - Define alert rules based on thresholds or status changes - Send alerts to PagerDuty, Slack, and Email</p>"},{"location":"user-guide/monitoring/#basic-usage","title":"Basic Usage","text":"<p>Initialize the monitoring manager and register health checks:</p> <pre><code>from apilinker.core.monitoring import MonitoringManager, HealthStatus\n\nmonitor = MonitoringManager()\n\n# Register a simple health check\ndef check_database():\n    # Your logic here\n    return True\n\nmonitor.register_health_check(\"database\", check_database)\n\n# Run checks\nresults = monitor.run_health_checks()\nprint(results[\"database\"].status)  # HealthStatus.HEALTHY\n</code></pre>"},{"location":"user-guide/monitoring/#connector-health-checks","title":"Connector Health Checks","text":"<p>ApiConnectors have a built-in <code>check_health</code> method that can be registered:</p> <pre><code>from apilinker import ApiConnector\n\nconnector = ApiConnector(\"rest\", \"https://api.example.com\")\nmonitor.register_health_check(\"api_source\", connector.check_health)\n</code></pre>"},{"location":"user-guide/monitoring/#alert-rules","title":"Alert Rules","text":"<p>Define rules to trigger alerts when conditions are met.</p>"},{"location":"user-guide/monitoring/#status-rules","title":"Status Rules","text":"<p>Trigger an alert when a component becomes unhealthy:</p> <pre><code>from apilinker.core.monitoring import StatusAlertRule, AlertSeverity\n\nrule = StatusAlertRule(\n    name=\"api_down\",\n    component=\"api_source\",\n    target_status=HealthStatus.UNHEALTHY,\n    severity=AlertSeverity.CRITICAL\n)\nmonitor.add_rule(rule)\n</code></pre>"},{"location":"user-guide/monitoring/#threshold-rules","title":"Threshold Rules","text":"<p>Trigger an alert when a metric exceeds a threshold:</p> <pre><code>from apilinker.core.monitoring import ThresholdAlertRule\n\n# Assuming you populate context with metrics\nrule = ThresholdAlertRule(\n    name=\"high_latency\",\n    metric=\"api_source_latency\",\n    threshold=1000.0,  # ms\n    operator=\"&gt;\"\n)\nmonitor.add_rule(rule)\n</code></pre>"},{"location":"user-guide/monitoring/#integrations","title":"Integrations","text":"<p>Configure where alerts should be sent.</p>"},{"location":"user-guide/monitoring/#slack","title":"Slack","text":"<pre><code>from apilinker.core.monitoring import SlackIntegration\n\nslack = SlackIntegration(webhook_url=\"https://hooks.slack.com/services/...\")\nmonitor.add_integration(slack)\n</code></pre>"},{"location":"user-guide/monitoring/#pagerduty","title":"PagerDuty","text":"<pre><code>from apilinker.core.monitoring import PagerDutyIntegration\n\npd = PagerDutyIntegration(routing_key=\"your_routing_key\")\nmonitor.add_integration(pd)\n</code></pre>"},{"location":"user-guide/monitoring/#email","title":"Email","text":"<pre><code>from apilinker.core.monitoring import EmailIntegration\n\nemail = EmailIntegration(\n    smtp_host=\"smtp.example.com\",\n    smtp_port=587,\n    sender=\"alerts@example.com\",\n    recipients=[\"admin@example.com\"],\n    username=\"user\",\n    password=\"password\"\n)\nmonitor.add_integration(email)\n</code></pre>"},{"location":"user-guide/research-connectors/","title":"Research Connectors","text":"<p>ApiLinker includes specialized connectors for scientific and research APIs, featuring domain-specific optimizations like citation parsing and academic rate limiting.</p>"},{"location":"user-guide/research-connectors/#supported-connectors","title":"Supported Connectors","text":"Connector Description NCBI PubMed, GenBank, ClinVar, and more. arXiv Academic preprints across sciences. CrossRef Citation data and DOI resolution. Semantic Scholar AI-powered academic search. PubChem Chemical compounds and bioassays. ORCID Researcher profiles."},{"location":"user-guide/research-connectors/#architecture","title":"Architecture","text":"<p>Research connectors inherit from <code>ResearchConnectorBase</code>, which adds:</p> <ul> <li>Ethical Usage: Automatic inclusion of contact info in User-Agent.</li> <li>Rate Limiting: Conservative defaults for academic infrastructure.</li> <li>Citation Parsing: Tools to extract structured metadata.</li> </ul>"},{"location":"user-guide/research-connectors/#example-ncbi-connector","title":"Example: NCBI Connector","text":"<p>The <code>NCBIConnector</code> handles E-utilities authentication and batch processing.</p> <pre><code>ncbi = NCBIConnector(email=\"researcher@university.edu\")\npapers = ncbi.search_pubmed(\"CRISPR\", max_results=50)\n</code></pre>"},{"location":"user-guide/scheduling/","title":"Scheduling","text":"<p>ApiLinker supports scheduled syncs with flexible scheduling options.</p>"},{"location":"user-guide/scheduling/#interval-based-scheduling","title":"Interval-Based Scheduling","text":"<p>Run syncs at regular intervals:</p> <pre><code>from apilinker import ApiLinker\n\nlinker = ApiLinker(config_path=\"config.yaml\")\nlinker.add_schedule(interval_minutes=60)\nlinker.start_scheduled_sync()\n</code></pre>"},{"location":"user-guide/scheduling/#cron-expressions","title":"Cron Expressions","text":"<p>Use cron expressions for complex schedules:</p> <pre><code># Run every day at 2 AM\nlinker.add_schedule(cron=\"0 2 * * *\")\n</code></pre>"},{"location":"user-guide/scheduling/#configuration","title":"Configuration","text":"<p>Add scheduling to your YAML config:</p> <pre><code>schedule:\n  type: interval\n  minutes: 60\n</code></pre> <p>Or for cron:</p> <pre><code>schedule:\n  type: cron\n  expression: \"0 2 * * *\"\n  timezone: \"UTC\"\n</code></pre>"},{"location":"user-guide/secret-management/","title":"Secret Management","text":"<p>ApiLinker provides enterprise-grade secret management with support for multiple cloud secret storage providers. This feature enables you to securely store and retrieve API credentials without hardcoding them in configuration files.</p>"},{"location":"user-guide/secret-management/#supported-providers","title":"Supported Providers","text":"<ul> <li>HashiCorp Vault: Enterprise secret management with KV v1/v2 support.</li> <li>AWS Secrets Manager: AWS native secret storage with automatic rotation.</li> <li>Azure Key Vault: Azure native secret management with managed identity support.</li> <li>Google Secret Manager: GCP native secret storage with workload identity.</li> <li>Environment Variables: Fallback for development.</li> </ul>"},{"location":"user-guide/secret-management/#configuration","title":"Configuration","text":"<p>Configure the secret provider in your YAML configuration:</p> <pre><code>secrets:\n  provider: vault  # or aws, azure, gcp, env\n  vault:\n    url: \"http://localhost:8200\"\n    token: \"hvs.CAESI...\"\n    mount_point: \"secret\"\n    kv_version: 2\n</code></pre>"},{"location":"user-guide/secret-management/#usage","title":"Usage","text":"<p>Reference secrets using the <code>secret://</code> prefix in your source/target configuration:</p> <pre><code>source:\n  auth:\n    type: api_key\n    key: \"secret://apilinker/source-api-key\"\n</code></pre>"},{"location":"user-guide/secret-management/#security-best-practices","title":"Security Best Practices","text":"<ol> <li>Use Managed Identities: Prefer workload identity/IAM roles over static credentials.</li> <li>Enable Rotation: Use automatic rotation for production secrets.</li> <li>Least Privilege: Grant only necessary permissions.</li> <li>Never Commit Secrets: Always use secret references.</li> </ol>"},{"location":"user-guide/security/","title":"Security Considerations","text":"<p>APILinker provides security features to ensure safe handling of API credentials and multi-user access. This document outlines security best practices and describes the available security features.</p>"},{"location":"user-guide/security/#security-features","title":"Security Features","text":""},{"location":"user-guide/security/#secure-credential-storage","title":"Secure Credential Storage","text":"<p>APILinker provides optional encrypted storage for sensitive credentials (for development convenience; consider dedicated secret managers in production):</p> <pre><code># Configure secure credential storage\nlinker = ApiLinker(\n    security_config={\n        \"master_password\": \"your-strong-password\",  # Or use environment variable\n        \"credential_storage_path\": \"./credentials.enc\"\n    }\n)\n\n# Store credentials securely\nlinker.store_credential(\"github_api\", {\n    \"token\": \"ghp_1234567890abcdefghijklmnopqrstuvwxyz\",\n    \"expires_at\": 1735689600  # Optional expiry timestamp\n})\n\n# Retrieve credentials\ncred = linker.get_credential(\"github_api\")\nprint(f\"Token: {cred['token']}\")\n</code></pre> <p>Note: ApiLinker defaults to using HTTPS without custom request/response encryption. Optional request/response encryption helpers exist for advanced scenarios; evaluate carefully and prefer provider-managed security controls.</p>"},{"location":"user-guide/security/#multi-user-access-control","title":"Multi-User Access Control","text":"<p>Set up role-based access control for multi-user environments:</p> <pre><code># Enable access control\nlinker = ApiLinker(\n    security_config={\n        \"enable_access_control\": True,\n        \"users\": [\n            {\"username\": \"admin1\", \"role\": \"admin\"},\n            {\"username\": \"operator1\", \"role\": \"operator\"},\n            {\"username\": \"viewer1\", \"role\": \"viewer\"}\n        ]\n    }\n)\n\n# Add users programmatically\nuser = linker.add_user(\"developer1\", \"developer\")\nprint(f\"API Key: {user['api_key']}\")\n\n# List users\nusers = linker.list_users()\n</code></pre>"},{"location":"user-guide/security/#oauth-enhancements","title":"OAuth Enhancements","text":"<p>APILinker now supports additional OAuth flows:</p> <ul> <li>PKCE Flow: For mobile and single-page applications</li> <li>Device Flow: For devices with limited input capabilities</li> </ul>"},{"location":"user-guide/security/#configuration-file-security","title":"Configuration File Security","text":"<p>When using configuration files, avoid including sensitive values directly. You have several options:</p> <ol> <li>Use environment variable references as shown above</li> <li>Keep separate configuration files for development and production</li> <li>Use APILinker's secure credential storage</li> <li>Store sensitive values in a secret management system</li> </ol>"},{"location":"user-guide/security/#oauth-authentication-flows","title":"OAuth Authentication Flows","text":""},{"location":"user-guide/security/#oauth-client-credentials-flow","title":"OAuth Client Credentials Flow","text":"<pre><code>auth:\n  type: oauth2_client_credentials\n  client_id: \"your-client-id\"\n  client_secret: \"${CLIENT_SECRET}\"  # From environment variable\n  token_url: \"https://auth.example.com/oauth/token\"\n  scope: \"read write\"  # Optional\n</code></pre>"},{"location":"user-guide/security/#oauth-pkce-flow-for-public-clients","title":"OAuth PKCE Flow (for public clients)","text":"<p>For public clients that cannot securely store a client secret:</p> <pre><code>from apilinker.core.auth import OAuth2PKCE\n\n# Initialize auth config\npkce_config = auth_manager.configure_auth({\n    \"type\": \"oauth2_pkce\",\n    \"client_id\": \"your-client-id\",\n    \"redirect_uri\": \"http://localhost:8080/callback\",\n    \"authorization_url\": \"https://auth.example.com/oauth/authorize\",\n    \"token_url\": \"https://auth.example.com/oauth/token\",\n    \"scope\": \"read write\",  # Optional\n    \"storage_key\": \"my_oauth_pkce_creds\"  # For secure storage\n})\n\n# Get authorization URL for user to visit\nauth_url = auth_manager.get_pkce_authorization_url(pkce_config)\nprint(f\"Visit this URL to authorize: {auth_url}\")\n\n# After receiving the authorization code from the redirect\nauth_config = auth_manager.complete_pkce_flow(pkce_config, \"authorization_code_from_redirect\")\n\n# Later, refresh the token when needed\nauth_config = auth_manager.refresh_pkce_token(auth_config)\n</code></pre>"},{"location":"user-guide/security/#oauth-device-flow-for-limited-input-devices","title":"OAuth Device Flow (for limited input devices)","text":"<p>For devices with limited input capabilities:</p> <pre><code>from apilinker.core.auth import OAuth2DeviceFlow\n\n# Initialize device flow\ndevice_config = auth_manager.configure_auth({\n    \"type\": \"oauth2_device_flow\",\n    \"client_id\": \"your-client-id\",\n    \"device_authorization_url\": \"https://auth.example.com/oauth/device/code\",\n    \"token_url\": \"https://auth.example.com/oauth/token\",\n    \"storage_key\": \"my_device_flow_creds\"  # For secure storage\n})\n\n# Start the device flow\ndevice_config = auth_manager.start_device_flow(device_config)\n\n# Show the user the verification code and URL\nprint(f\"Please visit {device_config.verification_uri} and enter code: {device_config.user_code}\")\n\n# Poll for completion\nimport time\nwhile True:\n    completed, updated_config = auth_manager.poll_device_flow(device_config)\n    if completed:\n        print(\"Authorization complete!\")\n        device_config = updated_config\n        break\n    time.sleep(device_config.interval)  # Respect polling interval\n</code></pre>"},{"location":"user-guide/security/#oauth-token-refresh","title":"OAuth Token Refresh","text":"<p>When using OAuth, ApiLinker automatically handles token refreshing when tokens expire.</p>"},{"location":"user-guide/security/#credential-management","title":"Credential Management","text":""},{"location":"user-guide/security/#environment-variables","title":"Environment Variables","text":"<p>Store API keys and tokens as environment variables rather than hardcoding them in your configuration files:</p> <pre><code>auth:\n  type: api_key\n  header: X-API-Key\n  key: ${MY_API_KEY}\n</code></pre> <p>In your Python code, you can set environment variables before running your script:</p> <pre><code>import os\nos.environ[\"MY_API_KEY\"] = \"your_api_key_here\"  # Set this securely\n</code></pre>"},{"location":"user-guide/security/#secure-storage","title":"Secure Storage","text":"<p>For production environments, consider using:</p> <ul> <li>Environment variables set at the system or container level</li> <li>Secret management services like HashiCorp Vault or AWS Secrets Manager</li> <li>Encrypted configuration files with restricted access</li> </ul>"},{"location":"user-guide/security/#authentication-methods","title":"Authentication Methods","text":"<p>ApiLinker supports several authentication methods, each with its own security considerations:</p>"},{"location":"user-guide/security/#api-key-authentication","title":"API Key Authentication","text":"<pre><code>auth:\n  type: api_key\n  header: X-API-Key  # Header name varies by API\n  key: ${API_KEY}\n</code></pre> <p>Security tips: - Rotate API keys regularly - Use keys with the minimum required permissions - Set IP restrictions on API keys when supported by the service</p>"},{"location":"user-guide/security/#bearer-token-authentication","title":"Bearer Token Authentication","text":"<pre><code>auth:\n  type: bearer\n  token: ${BEARER_TOKEN}\n</code></pre> <p>Security tips: - Use short-lived tokens when possible - Implement token refresh logic for long-running processes - Store refresh tokens with additional security measures</p>"},{"location":"user-guide/security/#basic-authentication","title":"Basic Authentication","text":"<pre><code>auth:\n  type: basic\n  username: ${API_USERNAME}\n  password: ${API_PASSWORD}\n</code></pre> <p>Security tips: - Only use over HTTPS connections - Use application-specific passwords when available - Avoid using your main account credentials</p>"},{"location":"user-guide/security/#oauth-20","title":"OAuth 2.0","text":"<pre><code>auth:\n  type: oauth2\n  client_id: ${CLIENT_ID}\n  client_secret: ${CLIENT_SECRET}\n  token_url: \"https://api.example.com/oauth/token\"\n  scope: \"read write\"\n</code></pre> <p>Security tips: - Store client secrets securely - Request only the scopes you need - Implement proper token storage and refresh logic</p>"},{"location":"user-guide/security/#network-security","title":"Network Security","text":""},{"location":"user-guide/security/#request-timeouts","title":"Request Timeouts","text":"<p>Set appropriate timeouts to prevent hanging connections:</p> <pre><code>source:\n  timeout: 30  # Timeout in seconds\n</code></pre>"},{"location":"user-guide/security/#data-protection","title":"Data Protection","text":""},{"location":"user-guide/security/#https","title":"HTTPS","text":"<p>Always use HTTPS for API endpoints to ensure data is encrypted in transit.</p>"},{"location":"user-guide/security/#audit-logging","title":"Audit Logging","text":"<p>Enable audit logging for security-relevant events:</p> <pre><code>logging:\n  level: INFO\n  security_audit: true\n  file: \"apilinker_audit.log\"\n</code></pre>"},{"location":"user-guide/security/#data-handling","title":"Data Handling","text":""},{"location":"user-guide/security/#sensitive-data-filtering","title":"Sensitive Data Filtering","text":"<p>When logging API responses, sensitive data can be filtered:</p> <pre><code>logging:\n  filter_fields:\n    - \"password\"\n    - \"token\"\n    - \"secret\"\n    - \"credit_card\"\n</code></pre>"},{"location":"user-guide/security/#data-validation","title":"Data Validation","text":"<p>Always validate data before processing:</p> <pre><code>mapping:\n  - source: \"get_user\"\n    target: \"create_profile\"\n    validation:\n      required_fields: [\"id\", \"email\"]\n      email_fields: [\"email\"]\n</code></pre>"},{"location":"user-guide/security/#best-practices-for-custom-plugins","title":"Best Practices for Custom Plugins","text":"<p>When developing custom plugins:</p> <ol> <li>Validate all inputs to prevent injection attacks</li> <li>Do not log sensitive information</li> <li>Handle exceptions properly to avoid information leakage</li> <li>Follow the principle of least privilege when accessing resources</li> </ol>"},{"location":"user-guide/security/#access-control-for-multi-user-environments","title":"Access Control for Multi-User Environments","text":"<p>For environments where multiple users need access to APILinker, you can enable role-based access control:</p> <pre><code>security:\n  enable_access_control: true\n  users:\n    - username: \"admin1\"\n      role: \"admin\"\n      api_key: \"optional-predefined-api-key\"\n    - username: \"viewer1\"\n      role: \"viewer\"\n</code></pre>"},{"location":"user-guide/security/#available-roles","title":"Available Roles","text":"<ul> <li><code>admin</code>: Full access to all operations</li> <li><code>operator</code>: Can run syncs and view results</li> <li><code>developer</code>: Can modify configurations but not run syncs</li> <li><code>viewer</code>: Can only view configurations and results</li> </ul>"},{"location":"user-guide/security/#permission-management","title":"Permission Management","text":"<p>Each role has a predefined set of permissions:</p> Permission Admin Operator Developer Viewer view_config \u2705 \u2705 \u2705 \u2705 edit_config \u2705 \u274c \u2705 \u274c run_sync \u2705 \u2705 \u274c \u274c view_results \u2705 \u2705 \u2705 \u2705 manage_users \u2705 \u274c \u274c \u274c manage_credentials \u2705 \u274c \u274c \u274c view_logs \u2705 \u2705 \u2705 \u274c access_analytics \u2705 \u2705 \u274c \u274c"},{"location":"user-guide/security/#logging","title":"Logging","text":"<p>Avoid logging sensitive information like API keys, tokens, or personal data. The ApiLinker logger is configured to avoid logging sensitive data by default.</p> <p>For enhanced security, consider using a separate log file with restricted permissions:</p> <pre><code>logging:\n  level: INFO\n  file: \"/secure/path/apilinker.log\"\n</code></pre>"},{"location":"user-guide/security/#security-configuration-example","title":"Security Configuration Example","text":"<p>A complete security-focused configuration might look like:</p> <pre><code>source:\n  type: rest\n  base_url: \"https://api.example.com\"\n  auth:\n    type: oauth2\n    client_id: ${CLIENT_ID}\n    client_secret: ${CLIENT_SECRET}\n    token_url: \"https://api.example.com/oauth/token\"\n    scope: \"read\"\n  timeout: 30\n  retry:\n    max_attempts: 3\n    backoff_factor: 2\n\nlogging:\n  level: INFO\n  security_audit: true\n  filter_fields:\n    - \"password\"\n    - \"token\"\n    - \"secret\"\n</code></pre> <p>Remember that security is an ongoing process. Regularly review and update your security practices, and stay informed about security updates for ApiLinker and its dependencies.</p>"},{"location":"user-guide/troubleshooting/","title":"Troubleshooting Guide","text":"<p>This guide helps you diagnose and resolve common issues with ApiLinker and explains how to use the robust error handling and recovery system.</p>"},{"location":"user-guide/troubleshooting/#error-handling-recovery-system","title":"Error Handling &amp; Recovery System","text":"<p>APILinker includes a sophisticated error handling and recovery system to make your API integrations more resilient against common failures. The system includes:</p> <ol> <li>Circuit Breakers - Prevent cascading failures during service outages</li> <li>Dead Letter Queues (DLQ) - Store failed operations for later retry</li> <li>Configurable Recovery Strategies - Apply different strategies for different error types</li> <li>Error Analytics - Track error patterns and trends</li> </ol>"},{"location":"user-guide/troubleshooting/#using-the-error-handling-system","title":"Using the Error Handling System","text":"<p>The error handling system is configured in your configuration file under the <code>error_handling</code> section:</p> <pre><code>error_handling:\n  # Configure circuit breakers\n  circuit_breakers:\n    source_customer_api:  # Name of the circuit breaker\n      failure_threshold: 5  # Number of failures before opening circuit\n      reset_timeout_seconds: 60  # Seconds to wait before trying again\n      half_open_max_calls: 1  # Max calls allowed in half-open state\n\n  # Configure recovery strategies by error category\n  recovery_strategies:\n    network:  # Error category\n      - exponential_backoff\n      - circuit_breaker\n    rate_limit:\n      - exponential_backoff\n    server:\n      - circuit_breaker\n      - exponential_backoff\n\n  # Configure Dead Letter Queue\n  dlq:\n    directory: \"./dlq\"  # Directory to store failed operations\n</code></pre>"},{"location":"user-guide/troubleshooting/#diagnostic-decision-tree","title":"Diagnostic Decision Tree","text":"<p>Start here and follow the branches to diagnose your issue:</p> <ol> <li>Installation Issues</li> <li>Package Not Found</li> <li>Version Conflicts</li> <li> <p>ImportError</p> </li> <li> <p>Configuration Issues</p> </li> <li>Invalid Configuration</li> <li>Environment Variables Not Working</li> <li> <p>File Not Found</p> </li> <li> <p>API Connection Issues</p> </li> <li>Connection Failed</li> <li>Authentication Failed</li> <li>SSL/Certificate Errors</li> <li>Timeout Errors</li> <li> <p>Circuit Breaker Open</p> </li> <li> <p>Mapping Issues</p> </li> <li>Missing Fields</li> <li>Transformation Errors</li> <li> <p>Type Errors</p> </li> <li> <p>Runtime Issues</p> </li> <li>Scheduling Problems</li> <li>Memory Usage</li> <li>Performance Problems</li> <li>DLQ Processing Errors</li> </ol>"},{"location":"user-guide/troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"user-guide/troubleshooting/#package-not-found","title":"Package Not Found","text":"<p>Symptoms: <pre><code>ERROR: Could not find a version that satisfies the requirement apilinker\n</code></pre></p> <p>Solutions: 1. Verify your Python version (3.8+ required):    <pre><code>python --version\n</code></pre></p> <ol> <li> <p>Update pip:    <pre><code>pip install --upgrade pip\n</code></pre></p> </li> <li> <p>Check your internet connection and try again.</p> </li> <li> <p>If using a corporate network, check proxy settings:    <pre><code>pip install apilinker --proxy http://your-proxy:port\n</code></pre></p> </li> </ol>"},{"location":"user-guide/troubleshooting/#version-conflicts","title":"Version Conflicts","text":"<p>Symptoms: <pre><code>ERROR: Cannot install apilinker due to dependency conflicts\n</code></pre></p> <p>Solutions: 1. Create a clean virtual environment:    <pre><code>python -m venv apilinker_env\nsource apilinker_env/bin/activate  # On Windows: apilinker_env\\Scripts\\activate\npip install apilinker\n</code></pre></p> <ol> <li>Install with the <code>--no-dependencies</code> flag and handle dependencies manually:    <pre><code>pip install --no-dependencies apilinker\npip install httpx pydantic pyyaml typer\n</code></pre></li> </ol>"},{"location":"user-guide/troubleshooting/#importerror","title":"ImportError","text":"<p>Symptoms: <pre><code>ImportError: No module named apilinker\n</code></pre></p> <p>Solutions: 1. Verify installation:    <pre><code>pip list | grep apilinker\n</code></pre></p> <ol> <li> <p>Check your Python environment:    <pre><code># On Windows\nwhere python\n\n# On Linux/Mac\nwhich python\n</code></pre></p> </li> <li> <p>Try reinstalling:    <pre><code>pip uninstall -y apilinker\npip install apilinker\n</code></pre></p> </li> </ol>"},{"location":"user-guide/troubleshooting/#configuration-issues","title":"Configuration Issues","text":""},{"location":"user-guide/troubleshooting/#invalid-configuration","title":"Invalid Configuration","text":"<p>Symptoms: <pre><code>ConfigError: Invalid configuration at path 'source.auth'\n</code></pre></p> <p>Solutions: 1. Validate your YAML syntax using an online validator.</p> <ol> <li> <p>Check the specific error message for details about what's wrong.</p> </li> <li> <p>Compare with the examples in the documentation.</p> </li> <li> <p>Common issues:</p> </li> <li>Indentation errors</li> <li>Missing required fields</li> <li>Incorrect value types</li> </ol>"},{"location":"user-guide/troubleshooting/#environment-variables","title":"Environment Variables","text":"<p>Symptoms: Environment variables are not being replaced in your configuration.</p> <p>Solutions: 1. Verify the environment variable is set:    <pre><code># Windows\necho %API_KEY%\n\n# Linux/Mac\necho $API_KEY\n</code></pre></p> <ol> <li> <p>Check the syntax in your configuration file:    <pre><code># Correct\nauth:\n  token: ${API_KEY}\n\n# Incorrect\nauth:\n  token: $API_KEY\n  token: \"{API_KEY}\"\n</code></pre></p> </li> <li> <p>Set the environment variable in your script:    <pre><code>import os\nos.environ[\"API_KEY\"] = \"your_api_key\"\nlinker = ApiLinker(config_path=\"config.yaml\")\n</code></pre></p> </li> </ol>"},{"location":"user-guide/troubleshooting/#file-not-found","title":"File Not Found","text":"<p>Symptoms: <pre><code>FileNotFoundError: No such file or directory: 'config.yaml'\n</code></pre></p> <p>Solutions: 1. Check the file path:    <pre><code>import os\nprint(os.getcwd())  # Current working directory\n</code></pre></p> <ol> <li>Use absolute paths:    <pre><code>import os\nconfig_path = os.path.join(os.path.dirname(__file__), \"config.yaml\")\nlinker = ApiLinker(config_path=config_path)\n</code></pre></li> </ol>"},{"location":"user-guide/troubleshooting/#api-connection-issues","title":"API Connection Issues","text":""},{"location":"user-guide/troubleshooting/#connection-failed","title":"Connection Failed","text":"<p>Symptoms: <pre><code>ConnectionError: Failed to establish connection to api.example.com\n</code></pre></p> <p>Solutions: 1. Verify your internet connection.</p> <ol> <li> <p>Check if the API domain is correct and accessible:    <pre><code>ping api.example.com\n</code></pre></p> </li> <li> <p>Try with a different network or disable firewall temporarily.</p> </li> <li> <p>Add timeout and retry settings:    <pre><code>linker.add_source(\n    # Other configuration...\n    timeout=30,  # Seconds\n    retry={\n        \"max_attempts\": 3,\n        \"delay_seconds\": 2\n    }\n)\n</code></pre></p> </li> </ol>"},{"location":"user-guide/troubleshooting/#authentication-failed","title":"Authentication Failed","text":"<p>Symptoms: <pre><code>AuthenticationError: API responded with status code 401 Unauthorized\n</code></pre></p> <p>Solutions: 1. Verify your credentials are correct.</p> <ol> <li> <p>Check if your token or API key has expired.</p> </li> <li> <p>Ensure you're using the correct authentication method.</p> </li> <li> <p>Examine the API documentation for specific auth requirements.</p> </li> <li> <p>Enable debug logging to see the actual request:    <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre></p> </li> </ol>"},{"location":"user-guide/troubleshooting/#sslcertificate-errors","title":"SSL/Certificate Errors","text":"<p>Symptoms: <pre><code>SSLError: SSL certificate verification failed\n</code></pre></p> <p>Solutions: 1. Update your CA certificates.</p> <ol> <li> <p>If necessary (and safe), disable SSL verification:    <pre><code>linker.add_source(\n    # Other configuration...\n    verify_ssl=False  # WARNING: Security risk in production\n)\n</code></pre></p> </li> <li> <p>Specify a custom CA bundle:    <pre><code>linker.add_source(\n    # Other configuration...\n    verify_ssl=\"/path/to/ca-bundle.crt\"\n)\n</code></pre></p> </li> </ol>"},{"location":"user-guide/troubleshooting/#timeout-errors","title":"Timeout Errors","text":"<p>Symptoms: <pre><code>TimeoutError: Request timed out after 30 seconds\n</code></pre></p> <p>Solutions: 1. Increase timeout duration:    <pre><code>linker.add_source(\n    # Other configuration...\n    timeout=60  # Seconds\n)\n</code></pre></p> <ol> <li> <p>Check if the API is experiencing high latency.</p> </li> <li> <p>Consider adding pagination for large data sets.</p> </li> </ol>"},{"location":"user-guide/troubleshooting/#mapping-issues","title":"Mapping Issues","text":""},{"location":"user-guide/troubleshooting/#missing-fields","title":"Missing Fields","text":"<p>Symptoms: <pre><code>KeyError: 'Field not found in source data: user_profile'\n</code></pre></p> <p>Solutions: 1. Print the actual response data to inspect the structure:    <pre><code>data = linker.fetch(\"get_users\")\nprint(data[0])  # Print first record\n</code></pre></p> <ol> <li> <p>Use dot notation for nested fields:    <pre><code>linker.add_mapping(\n    # Other configuration...\n    fields=[\n        {\"source\": \"user.profile.name\", \"target\": \"name\"}\n    ]\n)\n</code></pre></p> </li> <li> <p>Add conditional mapping:    <pre><code>linker.add_mapping(\n    # Other configuration...\n    fields=[\n        {\n            \"source\": \"profile.name\", \n            \"target\": \"name\",\n            \"condition\": {\n                \"field\": \"profile\",\n                \"operator\": \"exists\"\n            }\n        }\n    ]\n)\n</code></pre></p> </li> </ol>"},{"location":"user-guide/troubleshooting/#transformation-errors","title":"Transformation Errors","text":"<p>Symptoms: <pre><code>TransformError: Error transforming value: invalid date format\n</code></pre></p> <p>Solutions: 1. Check the input data format.</p> <ol> <li> <p>Add validation in your transformer:    <pre><code>def date_transformer(value, **kwargs):\n    if not value or not isinstance(value, str):\n        return None\n\n    try:\n        # Transformation logic\n        return transformed_value\n    except ValueError:\n        return kwargs.get(\"default\", None)\n</code></pre></p> </li> <li> <p>Test the transformer directly:    <pre><code>result = linker.mapper.transform(\"2023-01-01\", \"date_transformer\")\nprint(result)\n</code></pre></p> </li> </ol>"},{"location":"user-guide/troubleshooting/#type-errors","title":"Type Errors","text":"<p>Symptoms: <pre><code>TypeError: Cannot process input of type: dict\n</code></pre></p> <p>Solutions: 1. Add type checking:    <pre><code>def my_transformer(value, **kwargs):\n    if isinstance(value, dict):\n        return json.dumps(value)\n    elif isinstance(value, (int, float)):\n        return str(value)\n    return value\n</code></pre></p> <ol> <li>Use a pre-processor:    <pre><code>def pre_process(data):\n    for item in data:\n        if \"amount\" in item and item[\"amount\"] is not None:\n            item[\"amount\"] = float(item[\"amount\"])\n    return data\n\nlinker.add_source_processor(\"get_data\", pre_process)\n</code></pre></li> </ol>"},{"location":"user-guide/troubleshooting/#api-connection-issues_1","title":"API Connection Issues","text":""},{"location":"user-guide/troubleshooting/#connection-failed_1","title":"Connection Failed","text":"<p>Symptoms: <pre><code>ConnectionError: Failed to establish connection to api.example.com\n</code></pre></p> <p>Solutions: 1. Check your internet connection 2. Verify the API is online using a tool like cURL or Postman 3. Check if the API domain resolves correctly:    <pre><code>ping api.example.com\n</code></pre> 4. Check for firewall or proxy issues in your environment</p>"},{"location":"user-guide/troubleshooting/#authentication-failed_1","title":"Authentication Failed","text":"<p>Symptoms: <pre><code>APILinkerError: [AUTHENTICATION] Failed to fetch data: 401 Unauthorized\n</code></pre></p> <p>Solutions: 1. Verify your credentials are correct 2. Check if the token has expired 3. Ensure you're using the correct authentication method for the API 4. Check if your API key has the necessary permissions</p>"},{"location":"user-guide/troubleshooting/#sslcertificate-errors_1","title":"SSL/Certificate Errors","text":"<p>Symptoms: <pre><code>SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed\n</code></pre></p> <p>Solutions: 1. Update your CA certificates 2. If working in a development environment, you can disable verification (not recommended for production):    <pre><code>connector.client.verify = False\n</code></pre> 3. Provide the path to your custom certificate:    <pre><code>connector.client.verify = '/path/to/cert.pem'\n</code></pre></p>"},{"location":"user-guide/troubleshooting/#timeout-errors_1","title":"Timeout Errors","text":"<p>Symptoms: <pre><code>APILinkerError: [TIMEOUT] Failed to fetch data: Request timed out\n</code></pre></p> <p>Solutions: 1. Increase the timeout in your configuration:    <pre><code>source:\n  timeout: 60  # Seconds\n</code></pre> 2. Check if the API endpoint is slow or under heavy load 3. Consider adding exponential backoff retry strategy:    <pre><code>error_handling:\n  recovery_strategies:\n    timeout:\n      - exponential_backoff\n</code></pre></p>"},{"location":"user-guide/troubleshooting/#circuit-breaker-open","title":"Circuit Breaker Open","text":"<p>Symptoms: <pre><code>APILinkerError: Circuit breaker 'source_customer_api' is open\n</code></pre></p> <p>Solutions: 1. Wait for the circuit breaker to reset (typically 60 seconds by default) 2. Check the health of the API service that's failing 3. Adjust your circuit breaker configuration if needed:    <pre><code>error_handling:\n  circuit_breakers:\n    source_customer_api:\n      failure_threshold: 10  # More permissive\n      reset_timeout_seconds: 30  # Quicker reset\n</code></pre> 4. Use error analytics to diagnose recurring problems:    <pre><code>error_stats = linker.get_error_analytics()\nprint(error_stats)\n</code></pre></p>"},{"location":"user-guide/troubleshooting/#runtime-issues","title":"Runtime Issues","text":""},{"location":"user-guide/troubleshooting/#scheduling-problems","title":"Scheduling Problems","text":"<p>Symptoms: Scheduled syncs not running at expected times.</p> <p>Solutions: 1. Check your system time and timezone.</p> <ol> <li> <p>Verify the cron expression format.</p> </li> <li> <p>Ensure your script is kept running:    <pre><code># Add this at the end of your script\ntry:\n    # Keep the process alive\n    while True:\n        time.sleep(60)\nexcept KeyboardInterrupt:\n    print(\"Stopping scheduled syncs\")\n    linker.stop_scheduled_sync()\n</code></pre></p> </li> <li> <p>Use a dedicated task scheduler like systemd, cron, or Windows Task Scheduler.</p> </li> </ol>"},{"location":"user-guide/troubleshooting/#memory-usage","title":"Memory Usage","text":"<p>Symptoms: High memory usage or <code>MemoryError</code> exceptions.</p> <p>Solutions: 1. Process data in batches:    <pre><code>linker.add_mapping(\n    # Other configuration...\n    batch_size=100  # Process 100 records at a time\n)\n</code></pre></p> <ol> <li> <p>Use pagination with limits:    <pre><code>linker.add_source(\n    # Other configuration...\n    endpoints={\n        \"get_data\": {\n            # Other configuration...\n            \"pagination\": {\n                \"limit\": 200,  # Get 200 records per page\n                \"page_param\": \"page\"\n            }\n        }\n    }\n)\n</code></pre></p> </li> <li> <p>Implement a custom stream processor for very large datasets.</p> </li> </ol>"},{"location":"user-guide/troubleshooting/#performance-problems","title":"Performance Problems","text":"<p>Symptoms: - Syncs take longer than expected - High memory usage - Slow response times</p> <p>Solutions: 1. Use batch processing for large datasets 2. Add appropriate indexes to your database 3. Use pagination for large API responses 4. Profile your transformers to identify bottlenecks 5. Consider adding caching for frequently accessed data</p>"},{"location":"user-guide/troubleshooting/#dlq-processing-errors","title":"DLQ Processing Errors","text":"<p>Symptoms: <pre><code>Failed to retry DLQ item: item_id\n</code></pre></p> <p>Solutions: 1. Check the DLQ item's payload and error details:    <pre><code>items = linker.dlq.get_items(limit=10)\nprint(items[0])  # Examine the first item\n</code></pre></p> <ol> <li> <p>Process specific types of failed operations:    <pre><code>results = linker.process_dlq(operation_type=\"source_customer_api\")\nprint(f\"Processed {results['total_processed']} items, {results['successful']} succeeded\")\n</code></pre></p> </li> <li> <p>Manually fix issues and retry:    <pre><code># For specific item\nlinker.dlq.retry_item(\"item_id\", my_operation_function)\n</code></pre></p> </li> <li> <p>Check the error category distribution to identify patterns:    <pre><code>analytics = linker.get_error_analytics()\nprint(analytics[\"error_counts_by_category\"])\n</code></pre></p> </li> </ol>"},{"location":"user-guide/troubleshooting/#error-handling-system-details","title":"Error Handling System Details","text":""},{"location":"user-guide/troubleshooting/#circuit-breaker-pattern","title":"Circuit Breaker Pattern","text":"<p>The circuit breaker pattern prevents cascading failures by temporarily stopping calls to failing services. It has three states:</p> <ul> <li>CLOSED - Normal operation, requests pass through</li> <li>OPEN - Service is failing, requests fail fast without calling the service</li> <li>HALF-OPEN - Testing if service has recovered with limited requests</li> </ul> <p>Configuration options: <pre><code>circuit_breakers:\n  name_of_breaker:\n    failure_threshold: 5      # Failures before opening\n    reset_timeout_seconds: 60 # Time before half-open\n    half_open_max_calls: 1    # Test calls allowed\n</code></pre></p>"},{"location":"user-guide/troubleshooting/#recovery-strategies","title":"Recovery Strategies","text":"<p>APILinker supports these recovery strategies:</p> <ul> <li>RETRY - Simple retry without delay</li> <li>EXPONENTIAL_BACKOFF - Retry with increasing delays</li> <li>CIRCUIT_BREAKER - Use circuit breaker pattern</li> <li>FALLBACK - Use default data instead</li> <li>SKIP - Skip the operation</li> <li>FAIL_FAST - Fail immediately</li> </ul> <p>Configure by error category: <pre><code>recovery_strategies:\n  network:                  # Error category\n    - exponential_backoff   # First strategy\n    - circuit_breaker       # Second strategy\n</code></pre></p> <p>Available error categories: - NETWORK - Network connectivity issues - AUTHENTICATION - Auth failures - VALIDATION - Invalid data - TIMEOUT - Request timeouts - RATE_LIMIT - API returned 429 (rate limited): apply exponential backoff and retry - SERVER - Server errors (5xx) - CLIENT - Client errors (4xx) - MAPPING - Data mapping errors - PLUGIN - Plugin errors - UNKNOWN - Uncategorized errors</p>"},{"location":"user-guide/troubleshooting/#dead-letter-queue-dlq","title":"Dead Letter Queue (DLQ)","text":"<p>The DLQ stores failed operations for later analysis and retry. Each entry contains: - Error details (category, message, status code) - Original payload that caused the failure - Timestamp and operation context - Correlation ID for tracing</p> <p>Access DLQ data: <pre><code># Get failed operations\nitems = linker.dlq.get_items(error_category=ErrorCategory.RATE_LIMIT)\n\n# Retry operations\nlinker.process_dlq(operation_type=\"source_customers\", limit=10)\n</code></pre></p>"},{"location":"user-guide/troubleshooting/#error-analytics","title":"Error Analytics","text":"<p>The error analytics system tracks: - Error counts by category - Error rates over time - Top error types</p> <p>Access analytics: <pre><code>analytics = linker.get_error_analytics()\nprint(f\"Error rate: {analytics['recent_error_rate']} errors/minute\")\nprint(f\"Top errors: {analytics['top_errors']}\")\n</code></pre></p> <p>Symptoms: Syncs taking too long to complete.</p> <p>Solutions: 1. Enable performance logging:    <pre><code>linker = ApiLinker(performance_logging=True)\n</code></pre></p> <ol> <li>Optimize transformers:</li> <li>Avoid unnecessary operations</li> <li>Cache repeated calculations</li> <li> <p>Use built-in functions where possible</p> </li> <li> <p>Use concurrent requests when appropriate:    <pre><code>linker.add_source(\n    # Other configuration...\n    concurrency=5  # Up to 5 concurrent requests\n)\n</code></pre></p> </li> <li> <p>Implement partial syncs with filters:    <pre><code>linker.add_source(\n    # Other configuration...\n    endpoints={\n        \"get_data\": {\n            # Other configuration...\n            \"params\": {\n                \"updated_since\": \"{{last_sync}}\"  # Only get recently changed data\n            }\n        }\n    }\n)\n</code></pre></p> </li> </ol>"},{"location":"user-guide/troubleshooting/#debugging-techniques","title":"Debugging Techniques","text":""},{"location":"user-guide/troubleshooting/#enable-debug-logging","title":"Enable Debug Logging","text":"<pre><code>import logging\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n</code></pre>"},{"location":"user-guide/troubleshooting/#use-dry-run-mode","title":"Use Dry Run Mode","text":"<pre><code># Test the sync without making changes\nresult = linker.sync(dry_run=True)\nprint(f\"Would sync {result.count} records\")\nprint(f\"Preview: {result.preview[:3]}\")  # First 3 records\n</code></pre>"},{"location":"user-guide/troubleshooting/#inspect-api-requests","title":"Inspect API Requests","text":"<pre><code># Install HTTP debugging tool\n# pip install httpx-debug\n\nimport httpx_debug\nhttpx_debug.install()  # Shows all HTTP requests and responses\n</code></pre>"},{"location":"user-guide/troubleshooting/#interactive-debugging","title":"Interactive Debugging","text":"<pre><code># Add this where you want to inspect\nimport pdb; pdb.set_trace()\n# or\nbreakpoint()  # Python 3.7+\n</code></pre> <p>If you're still experiencing issues after trying these solutions, please open an issue on GitHub with detailed information about your problem.</p>"},{"location":"user-guide/webhooks/","title":"Webhooks","text":"<p>APILinker provides first-class webhook support for receiving webhooks and triggering syncs in real-time, event-driven integrations.</p>"},{"location":"user-guide/webhooks/#overview","title":"Overview","text":"<p>The webhook module enables you to:</p> <ul> <li>Run an HTTP server to receive webhooks from external services</li> <li>Register and manage multiple webhook endpoints</li> <li>Verify webhook signatures (HMAC, JWT)</li> <li>Filter and route events to handlers</li> <li>Replay historical events</li> <li>Automatically retry failed processing</li> </ul>"},{"location":"user-guide/webhooks/#installation","title":"Installation","text":"<p>Install APILinker with webhook dependencies:</p> <pre><code>pip install apilinker[webhooks]\n</code></pre> <p>This installs FastAPI, uvicorn, and PyJWT for webhook functionality.</p>"},{"location":"user-guide/webhooks/#quick-start","title":"Quick Start","text":"<pre><code>from apilinker import (\n    WebhookServer,\n    WebhookEndpoint,\n    SignatureType,\n)\n\n# 1. Create and configure endpoint\nendpoint = WebhookEndpoint(\n    path=\"/hooks/github\",\n    secret=\"your-webhook-secret\",\n    signature_type=SignatureType.HMAC_SHA256,\n    signature_header=\"X-Hub-Signature-256\",\n)\n\n# 2. Create server and register endpoint\nserver = WebhookServer(host=\"0.0.0.0\", port=8000)\nserver.register_endpoint(endpoint)\n\n# 3. Add event handler\ndef on_webhook(event):\n    print(f\"Received: {event.payload}\")\n\nserver.add_event_handler(on_webhook)\n\n# 4. Start server\nserver.start(blocking=True)\n</code></pre>"},{"location":"user-guide/webhooks/#webhook-manager","title":"Webhook Manager","text":"<p>For advanced use cases, use <code>WebhookManager</code> which provides routing, retry, and replay:</p> <pre><code>from apilinker import (\n    WebhookManager,\n    WebhookConfig,\n    WebhookEndpoint,\n    WebhookEventFilter,\n    SignatureType,\n)\n\n# Configure the manager\nconfig = WebhookConfig(\n    host=\"0.0.0.0\",\n    port=8000,\n    max_retry_attempts=3,\n    event_history_size=1000,\n)\n\nmanager = WebhookManager(config)\n\n# Register endpoints\nmanager.register_endpoint(WebhookEndpoint(\n    path=\"/hooks/github\",\n    secret=\"github-secret\",\n    signature_type=SignatureType.HMAC_SHA256,\n))\n\nmanager.register_endpoint(WebhookEndpoint(\n    path=\"/hooks/stripe\",\n    secret=\"stripe-secret\",\n    signature_type=SignatureType.HMAC_SHA256,\n    signature_header=\"Stripe-Signature\",\n))\n\n# Add filtered handlers\ndef handle_github_push(event):\n    if event.payload.get(\"action\") == \"push\":\n        # Trigger a sync\n        pass\n\nmanager.add_handler(\n    handle_github_push,\n    filter_=WebhookEventFilter(endpoint_paths=[\"/hooks/github\"]),\n    name=\"github_push_handler\",\n)\n\n# Start the manager\nmanager.start(blocking=True)\n</code></pre>"},{"location":"user-guide/webhooks/#signature-verification","title":"Signature Verification","text":"<p>APILinker supports multiple signature verification methods:</p>"},{"location":"user-guide/webhooks/#hmac-sha256-github-stripe-etc","title":"HMAC-SHA256 (GitHub, Stripe, etc.)","text":"<pre><code>endpoint = WebhookEndpoint(\n    path=\"/hooks/github\",\n    secret=\"your-secret\",\n    signature_type=SignatureType.HMAC_SHA256,\n    signature_header=\"X-Hub-Signature-256\",\n)\n</code></pre>"},{"location":"user-guide/webhooks/#hmac-sha1-legacy-webhooks","title":"HMAC-SHA1 (Legacy webhooks)","text":"<pre><code>endpoint = WebhookEndpoint(\n    path=\"/hooks/legacy\",\n    secret=\"your-secret\",\n    signature_type=SignatureType.HMAC_SHA1,\n    signature_header=\"X-Signature\",\n)\n</code></pre>"},{"location":"user-guide/webhooks/#jwt-verification","title":"JWT Verification","text":"<pre><code>from apilinker.core.webhooks import JWTVerifier\n\nendpoint = WebhookEndpoint(\n    path=\"/hooks/jwt\",\n    secret=\"jwt-secret\",\n    signature_type=SignatureType.JWT,\n    signature_header=\"Authorization\",\n)\n</code></pre>"},{"location":"user-guide/webhooks/#event-filtering","title":"Event Filtering","text":"<p>Filter events based on various criteria:</p> <pre><code>from apilinker import WebhookEventFilter\n\n# Filter by endpoint path (supports wildcards)\nfilter_github = WebhookEventFilter(\n    endpoint_paths=[\"/hooks/github\", \"/hooks/gitlab\"]\n)\n\n# Filter by HTTP method\nfilter_posts = WebhookEventFilter(methods=[\"POST\"])\n\n# Filter by header patterns (regex)\nfilter_push = WebhookEventFilter(\n    header_patterns={\"x-github-event\": \"push|pull_request\"}\n)\n\n# Filter by payload content (JSON path)\nfilter_opened = WebhookEventFilter(\n    payload_patterns={\n        \"action\": \"opened\",\n        \"repository.name\": \"my-repo.*\",\n    }\n)\n\n# Combine filters\nfilter_combined = WebhookEventFilter(\n    endpoint_paths=[\"/hooks/github\"],\n    methods=[\"POST\"],\n    payload_patterns={\"action\": \"opened\"},\n)\n</code></pre>"},{"location":"user-guide/webhooks/#retry-mechanism","title":"Retry Mechanism","text":"<p>Failed webhook processing automatically retries with exponential backoff:</p> <pre><code>config = WebhookConfig(\n    max_retry_attempts=3,      # Maximum retries\n    retry_delay_seconds=1.0,   # Initial delay\n    retry_backoff_multiplier=2.0,  # Backoff multiplier\n)\n</code></pre> <p>The retry sequence would be: 1s \u2192 2s \u2192 4s (max 3 attempts).</p>"},{"location":"user-guide/webhooks/#event-replay","title":"Event Replay","text":"<p>Replay historical events for debugging or reprocessing:</p> <pre><code># Get event history\nhistory = manager.get_event_history(\n    endpoint_path=\"/hooks/github\",\n    limit=100,\n)\n\n# Replay a specific event\nif history:\n    result = manager.replay_event(history[0].id)\n    print(f\"Replay result: {result.success}\")\n</code></pre>"},{"location":"user-guide/webhooks/#integration-with-apilinker-syncs","title":"Integration with APILinker Syncs","text":"<p>Trigger APILinker syncs from webhooks:</p> <pre><code>from apilinker import ApiLinker, WebhookManager, WebhookEventFilter\n\n# Setup your ApiLinker\nlinker = ApiLinker(\n    source_config={...},\n    target_config={...},\n    field_mapping={...},\n)\n\n# Create webhook handler that triggers sync\ndef trigger_sync(event):\n    result = linker.sync()\n    return {\"synced\": result.items_processed}\n\n# Register with webhook manager\nmanager = WebhookManager()\nmanager.add_handler(\n    trigger_sync,\n    filter_=WebhookEventFilter(endpoint_paths=[\"/hooks/trigger\"]),\n    name=\"sync_trigger\",\n)\n</code></pre>"},{"location":"user-guide/webhooks/#security-considerations","title":"Security Considerations","text":"<p>[!WARNING] Always use signature verification in production to prevent unauthorized webhook calls.</p> <p>Best practices:</p> <ol> <li>Always verify signatures - Use <code>signature_type</code> and <code>secret</code> for all endpoints</li> <li>Use HTTPS - Deploy behind a reverse proxy with TLS</li> <li>Validate payloads - Use filters to ensure expected payload structure</li> <li>Limit exposure - Only expose necessary endpoints</li> <li>Monitor events - Use event history for auditing</li> </ol>"},{"location":"user-guide/webhooks/#api-reference","title":"API Reference","text":""},{"location":"user-guide/webhooks/#webhookendpoint","title":"WebhookEndpoint","text":"Parameter Type Default Description <code>path</code> str required URL path for the endpoint <code>name</code> str auto Human-readable name <code>secret</code> str None Secret for signature verification <code>signature_type</code> SignatureType NONE Verification type <code>signature_header</code> str X-Hub-Signature-256 Header containing signature <code>methods</code> List[str] [\"POST\"] Allowed HTTP methods <code>enabled</code> bool True Enable/disable endpoint"},{"location":"user-guide/webhooks/#webhookconfig","title":"WebhookConfig","text":"Parameter Type Default Description <code>host</code> str 0.0.0.0 Host to bind server <code>port</code> int 8000 Port to listen on <code>max_retry_attempts</code> int 3 Maximum retry attempts <code>retry_delay_seconds</code> float 1.0 Initial retry delay <code>event_history_size</code> int 1000 Max events in history"}]}