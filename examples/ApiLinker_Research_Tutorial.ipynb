{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ApiLinker Research-Grade Notebook\n",
        "\n",
        "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/kkartas/APILinker/HEAD?labpath=examples%2FApiLinker_Research_Tutorial.ipynb)\n",
        "\n",
        "**Title**: _Automated Multisource Research Intelligence with ApiLinker_\n",
        "\n",
        "**Authors**: ApiLinker Research Engineering Team  \n",
        "**Version**: Draft 0.1 (journal-ready structure)  \n",
        "**Keywords**: literature intelligence, knowledge graphs, connector orchestration, research automation, reproducibility\n",
        "\n",
        "---\n",
        "\n",
        "> This notebook is being refactored into a publication-grade artifact. In this iteration we define the scientific narrative, section scaffolding, and deliverable expectations. Subsequent iterations will populate each section with executable analyses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4 ¬∑ Data Acquisition & Connector Strategy\n",
        "\n",
        "> _Goal_: Define how each connector contributes to the study. Actual API calls will be implemented in the next iteration.\n",
        "\n",
        "### 4.1 Source Taxonomy\n",
        "\n",
        "| Connector | Domain | Sample Use Case | Auth Requirements | Planned Output |\n",
        "|-----------|--------|-----------------|-------------------|----------------|\n",
        "| NCBI | Biomedical literature | PubMed abstracts on protein design | Contact email | PMID, title, abstract, MeSH terms |\n",
        "| arXiv | Preprints | Machine learning for folding | None | arXiv ID, categories, summary |\n",
        "| CrossRef | Citation metadata | DOI crosswalk, publisher info | Email | DOI, references, citation counts |\n",
        "| Semantic Scholar | AI-enhanced literature | Citation graph metrics | Optional API key | Paper embeddings, influence scores |\n",
        "| PubChem | Chemical data | Ligand properties for targets | None | Compound IDs, properties |\n",
        "| ORCID | Researcher profiles | Author disambiguation | Public API | ORCID ID, affiliations |\n",
        "| GitHub | Code repositories | ML repo discovery | Optional token | Stars, topics, license |\n",
        "| NASA | Earth/climate datasets | Geospatial covariates | API key (DEMO usable) | Lat/Lon series, imagery metadata |\n",
        "\n",
        "### 4.2 Workflow Diagram (To be implemented)\n",
        "\n",
        "1. Query orchestration (batched by topic).\n",
        "2. Response normalization & persistence.\n",
        "3. Validation and deduplication.\n",
        "4. Fusion into unified research graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Credential & Rate Limit Policy (Planned Implementation)\n",
        "\n",
        "- Centralized YAML config with secret placeholders (Vault/AWS/GCP options).\n",
        "- Rotating email footers for NCBI/CrossRef courtesy requirements.\n",
        "- Retry budget: exponential backoff capped at 3 attempts per connector.\n",
        "- Cached responses stored locally for deterministic reruns.\n",
        "\n",
        "### 4.4 Future Code Cells (Coming Next)\n",
        "\n",
        "1. `load_connector_configs()` ‚Äì parse YAML, validate presence of required keys.\n",
        "2. `instantiate_connectors()` ‚Äì create connector objects with observability hooks.\n",
        "3. `run_topic_batch(topics)` ‚Äì orchestrate queries across all connectors.\n",
        "4. `persist_raw_payloads()` ‚Äì write JSONL artifacts for auditing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 4.5 ¬∑ Connector Instantiation ===\n",
        "# Initialize all 8 research connectors with production-ready settings\n",
        "\n",
        "if not RESEARCH_CONNECTORS_AVAILABLE:\n",
        "    print(\"‚ö†Ô∏è  Skipping connector initialization (imports unavailable)\")\n",
        "    connectors = {}\n",
        "else:\n",
        "    connectors = {}\n",
        "    \n",
        "    # Scientific Literature connectors\n",
        "    print(\"Initializing scientific literature connectors...\")\n",
        "    connectors[\"ncbi\"] = NCBIConnector(\n",
        "        email=\"apilinker.research@example.edu\",  # Replace with your email\n",
        "        tool_name=\"ApiLinker_Research_Notebook\"\n",
        "    )\n",
        "    connectors[\"arxiv\"] = ArXivConnector()\n",
        "    connectors[\"crossref\"] = CrossRefConnector(\n",
        "        email=\"apilinker.research@example.edu\"  # Replace with your email\n",
        "    )\n",
        "    connectors[\"semantic\"] = SemanticScholarConnector()  # Optional: api_key=\"YOUR_KEY\"\n",
        "    \n",
        "    # Chemical & Biological Data connectors\n",
        "    print(\"Initializing chemical/biological connectors...\")\n",
        "    connectors[\"pubchem\"] = PubChemConnector()\n",
        "    connectors[\"orcid\"] = ORCIDConnector()  # Optional: access_token for private data\n",
        "    \n",
        "    # Code & Data connectors\n",
        "    print(\"Initializing code/data connectors...\")\n",
        "    connectors[\"github\"] = GitHubConnector()  # Optional: token=\"YOUR_TOKEN\"\n",
        "    connectors[\"nasa\"] = NASAConnector()  # Uses DEMO_KEY; get key from api.nasa.gov\n",
        "    \n",
        "    print(f\"\\n‚úÖ Initialized {len(connectors)} research connectors:\")\n",
        "    for name, connector in connectors.items():\n",
        "        print(f\"   ‚Ä¢ {name}: {connector.__class__.__name__} ‚Üí {connector.base_url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.6 ¬∑ Research Topic Definition\n",
        "\n",
        "We'll demonstrate multi-database workflows on three exemplar topics:\n",
        "1. **Protein Design**: \"machine learning protein folding alphafold\"\n",
        "2. **Climate Modeling**: \"climate change prediction deep learning\"\n",
        "3. **Drug Discovery**: \"CRISPR gene editing therapeutics\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 4.7 ¬∑ Multi-Database Literature Search Pipeline ===\n",
        "from typing import Dict, List, Any\n",
        "import time\n",
        "import json\n",
        "\n",
        "# Define research topics\n",
        "RESEARCH_TOPICS = {\n",
        "    \"protein_design\": \"machine learning protein folding alphafold\",\n",
        "    \"climate_modeling\": \"climate change prediction deep learning\",\n",
        "    \"drug_discovery\": \"CRISPR gene editing therapeutics\"\n",
        "}\n",
        "\n",
        "# Storage for aggregated results\n",
        "literature_corpus = {topic: {} for topic in RESEARCH_TOPICS}\n",
        "\n",
        "def fetch_with_retry(connector_func, max_retries=3, backoff=2):\n",
        "    \"\"\"Resilient fetch with exponential backoff.\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            return connector_func()\n",
        "        except Exception as e:\n",
        "            if attempt == max_retries - 1:\n",
        "                print(f\"   ‚ö†Ô∏è  Failed after {max_retries} attempts: {e}\")\n",
        "                return None\n",
        "            wait_time = backoff ** attempt\n",
        "            print(f\"   Retry {attempt + 1}/{max_retries} after {wait_time}s...\")\n",
        "            time.sleep(wait_time)\n",
        "    return None\n",
        "\n",
        "if not RESEARCH_CONNECTORS_AVAILABLE:\n",
        "    print(\"‚ö†Ô∏è  Skipping literature search (connectors unavailable)\")\n",
        "else:\n",
        "    print(\"=\" * 70)\n",
        "    print(\"MULTI-DATABASE LITERATURE SEARCH\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    for topic_key, query in RESEARCH_TOPICS.items():\n",
        "        print(f\"\\nüîç Topic: {topic_key.replace('_', ' ').title()}\")\n",
        "        print(f\"   Query: '{query}'\")\n",
        "        print(\"-\" * 70)\n",
        "        \n",
        "        # NCBI PubMed search\n",
        "        print(\"   üìö PubMed (NCBI)...\", end=\" \")\n",
        "        pubmed_data = fetch_with_retry(\n",
        "            lambda: connectors[\"ncbi\"].search_pubmed(query, max_results=20)\n",
        "        )\n",
        "        if pubmed_data:\n",
        "            pmids = pubmed_data.get(\"esearchresult\", {}).get(\"idlist\", [])\n",
        "            literature_corpus[topic_key][\"pubmed\"] = {\n",
        "                \"count\": len(pmids),\n",
        "                \"ids\": pmids,\n",
        "                \"source\": \"PubMed\"\n",
        "            }\n",
        "            print(f\"‚úì {len(pmids)} results\")\n",
        "        \n",
        "        # arXiv search\n",
        "        print(\"   üìÑ arXiv...\", end=\" \")\n",
        "        arxiv_data = fetch_with_retry(\n",
        "            lambda: connectors[\"arxiv\"].search_papers(query, max_results=20)\n",
        "        )\n",
        "        if arxiv_data:\n",
        "            literature_corpus[topic_key][\"arxiv\"] = {\n",
        "                \"count\": len(arxiv_data),\n",
        "                \"papers\": arxiv_data,\n",
        "                \"source\": \"arXiv\"\n",
        "            }\n",
        "            print(f\"‚úì {len(arxiv_data)} results\")\n",
        "        \n",
        "        # Semantic Scholar search\n",
        "        print(\"   ü§ñ Semantic Scholar...\", end=\" \")\n",
        "        semantic_data = fetch_with_retry(\n",
        "            lambda: connectors[\"semantic\"].search_papers(query, max_results=20)\n",
        "        )\n",
        "        if semantic_data:\n",
        "            papers = semantic_data.get(\"data\", [])\n",
        "            literature_corpus[topic_key][\"semantic\"] = {\n",
        "                \"count\": len(papers),\n",
        "                \"papers\": papers,\n",
        "                \"source\": \"Semantic Scholar\"\n",
        "            }\n",
        "            print(f\"‚úì {len(papers)} results\")\n",
        "        \n",
        "        # CrossRef search\n",
        "        print(\"   üìñ CrossRef...\", end=\" \")\n",
        "        crossref_data = fetch_with_retry(\n",
        "            lambda: connectors[\"crossref\"].search_works(query, max_results=20)\n",
        "        )\n",
        "        if crossref_data:\n",
        "            items = crossref_data.get(\"message\", {}).get(\"items\", [])\n",
        "            literature_corpus[topic_key][\"crossref\"] = {\n",
        "                \"count\": len(items),\n",
        "                \"works\": items,\n",
        "                \"source\": \"CrossRef\"\n",
        "            }\n",
        "            print(f\"‚úì {len(items)} results\")\n",
        "        \n",
        "        time.sleep(1)  # Rate limit courtesy\n",
        "    \n",
        "    # Persist raw corpus\n",
        "    corpus_file = os.path.join(CACHE_DIR, \"literature_corpus.json\")\n",
        "    with open(corpus_file, \"w\") as f:\n",
        "        json.dump(literature_corpus, f, indent=2, default=str)\n",
        "    print(f\"\\nüíæ Raw corpus saved to: {corpus_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Structured Table of Contents\n",
        "\n",
        "1. **Abstract** ‚Äì Executive summary of objectives, data sources, and headline findings.\n",
        "2. **1 ¬∑ Introduction** ‚Äì Context, related work, and motivation for a unified API research fabric.\n",
        "3. **2 ¬∑ Research Objectives & Questions** ‚Äì Formal problem statements and evaluation goals.\n",
        "4. **3 ¬∑ Reproducibility & Environment Controls** ‚Äì Diagnostic metadata, dependency manifest, credential policy.\n",
        "5. **4 ¬∑ Data Acquisition & Connector Strategy** ‚Äì Source taxonomy, rate-limit policy, batching diagrams.\n",
        "6. **5 ¬∑ Harmonization & Quality Controls** ‚Äì Schema unification, validation layers, enrichment logic.\n",
        "7. **6 ¬∑ Analysis & Visualization Plan** ‚Äì Statistical tests, temporal trends, citation networks, geospatial layers.\n",
        "8. **7 ¬∑ Result Narratives & Reporting Artifacts** ‚Äì Tables, figures, KPIs, export formats.\n",
        "9. **8 ¬∑ Discussion, Limitations, and Future Work** ‚Äì Interpretation, biases, roadmap.\n",
        "10. **Appendix** ‚Äì Credentials, configs, error taxonomies, supplementary tables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Abstract\n",
        "\n",
        "ApiLinker orchestrates eight research-grade connectors (NCBI, arXiv, CrossRef, Semantic Scholar, PubChem, ORCID, GitHub, NASA) to automate data discovery, validation, and synthesis across scientific, chemical, and engineering modalities. This study notebook captures the experimental design for a multisource knowledge graph that powers three exemplar research themes: (i) protein design literature intelligence, (ii) climate-model code reproducibility, and (iii) translational collaboration analytics. We document experimental controls, connector taxonomies, harmonization schemas, and target evaluation metrics (recall, freshness, provenance completeness). The executable sections that follow‚Äîadded in subsequent iterations‚Äîwill implement the described workflows end-to-end, enabling notebook readers to reproduce journal-quality figures and export publication-ready tables, JSON bundles, and BibTeX libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1 ¬∑ Introduction\n",
        "\n",
        "- **Problem Context**: Research groups juggle siloed APIs for literature, chemical data, and mission telemetry; manual ETL pipelines erode reproducibility.\n",
        "- **ApiLinker Contribution**: Unified connector interface with typed schemas, observability hooks, and credential-agnostic deployment.\n",
        "- **Scope of Notebook**: Define methodology for end-to-end automated evidence synthesis, aligned with journal guidelines (e.g., _Patterns_, _Nature Scientific Data_).\n",
        "- **Related Work**: Outline contrasts with standalone wrappers (e.g., `pymed`, `python-arxiv`, `ads`) and highlight cross-domain orchestration gap.\n",
        "- **Reader Outcome**: Ability to replicate the workflow, adapt connectors, and export publication-ready artifacts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2 ¬∑ Research Objectives & Questions\n",
        "\n",
        "1. **Literature Coverage**: What recall and freshness can ApiLinker deliver by federating NCBI, arXiv, CrossRef, and Semantic Scholar queries for a target query set *Q*?\n",
        "2. **Collaboration Analytics**: How accurately can ORCID + Semantic Scholar + GitHub metadata capture institutional and co-authorship networks?\n",
        "3. **Compound & Data Integration**: Can PubChem and NASA datasets enrich the core literature graph with chemical and geospatial context without manual intervention?\n",
        "4. **Operational Metrics**: What are the latency, rate-limit resilience, and cache hit rates for orchestrated connector workflows?\n",
        "5. **Reproducibility Goal**: Achieve deterministic notebook reruns via captured configs, seeds, and export manifests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 3 ¬∑ Reproducibility & Environment Controls ===\n",
        "# Capture runtime metadata before any network calls.\n",
        "import os\n",
        "import sys\n",
        "import platform\n",
        "from importlib import metadata\n",
        "\n",
        "print(\"Python version:\", sys.version)\n",
        "print(\"Interpreter:\", sys.executable)\n",
        "print(\"Platform:\", platform.platform())\n",
        "\n",
        "# ApiLinker diagnostics\n",
        "from apilinker import ApiLinker, __version__ as apilinker_version\n",
        "import apilinker\n",
        "\n",
        "print(f\"ApiLinker version: {apilinker_version}\")\n",
        "print(f\"ApiLinker module path: {apilinker.__file__}\")\n",
        "\n",
        "# List top-level connector packages to ensure repository install\n",
        "connectors_path = os.path.join(os.path.dirname(apilinker.__file__), \"connectors\")\n",
        "print(\"Connector path exists:\", os.path.exists(connectors_path))\n",
        "if os.path.exists(connectors_path):\n",
        "    print(\"Top-level connector namespaces:\", os.listdir(connectors_path))\n",
        "\n",
        "# Snapshot of critical dependencies for reproducibility\n",
        "core_packages = [\"httpx\", \"pydantic\", \"typer\", \"rich\", \"cryptography\"]\n",
        "deps = {pkg: metadata.version(pkg) for pkg in core_packages if metadata.version(pkg)}\n",
        "print(\"Dependency snapshot:\", deps)\n",
        "\n",
        "# Flag to gate subsequent sections if research connectors fail to import\n",
        "try:\n",
        "    from apilinker import (\n",
        "        NCBIConnector,\n",
        "        ArXivConnector,\n",
        "        CrossRefConnector,\n",
        "        SemanticScholarConnector,\n",
        "        PubChemConnector,\n",
        "        ORCIDConnector,\n",
        "        GitHubConnector,\n",
        "        NASAConnector,\n",
        "    )\n",
        "    RESEARCH_CONNECTORS_AVAILABLE = True\n",
        "    print(\"‚úÖ Research connector imports succeeded.\")\n",
        "except ImportError as exc:\n",
        "    RESEARCH_CONNECTORS_AVAILABLE = False\n",
        "    print(\"‚ùå Research connector imports failed:\", exc)\n",
        "    print(\"Sections depending on connectors will present structural placeholders only.\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Environment Manifest Checklist\n",
        "\n",
        "- ‚úÖ Python interpreter, OS, and ApiLinker version captured above.\n",
        "- ‚úÖ Critical dependency versions recorded via `importlib.metadata`.\n",
        "- ‚òê Credential loading (Vault/AWS/GCP) ‚Äì to be implemented in Section 4.\n",
        "- ‚òê Random seed & cache directory ‚Äì to be set when analytics code is added.\n",
        "- ‚òê Artifact log (exports, figures) ‚Äì populated after analyses are executed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 3.2 ¬∑ Reproducibility Setup ===\n",
        "import hashlib\n",
        "import random\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Create cache directory for response artifacts\n",
        "CACHE_DIR = \"notebook_cache\"\n",
        "EXPORT_DIR = \"exports\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Random seed: {RANDOM_SEED}\")\n",
        "print(f\"Cache directory: {os.path.abspath(CACHE_DIR)}\")\n",
        "print(f\"Export directory: {os.path.abspath(EXPORT_DIR)}\")\n",
        "\n",
        "# Compute environment fingerprint for reproducibility tracking\n",
        "env_data = f\"{sys.version}|{apilinker_version}|{platform.platform()}\"\n",
        "env_hash = hashlib.sha256(env_data.encode()).hexdigest()[:12]\n",
        "print(f\"Environment fingerprint: {env_hash}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5 ¬∑ Harmonization & Quality Controls\n",
        "\n",
        "### 5.1 Target Schemas (Planned)\n",
        "- **LiteratureRecord**: DOI, identifiers, abstract, keywords, citation metrics.\n",
        "- **ResearcherProfile**: ORCID, affiliation history, publication counts.\n",
        "- **CompoundProfile**: CID, physicochemical properties, bioassay summary.\n",
        "- **DatasetDescriptor**: Source (NASA/GitHub), spatial/temporal coverage, license.\n",
        "\n",
        "### 5.2 Validation Layers\n",
        "- Field-level type checks via Pydantic models.\n",
        "- Duplicate detection using DOI/PMID/arXiv ID crosswalk.\n",
        "- Consistency rules (e.g., ORCID affiliation matching with CrossRef metadata).\n",
        "\n",
        "### 5.3 Enrichment Logic\n",
        "- Semantic Scholar influence scores appended to CrossRef entries.\n",
        "- PubChem compound-match to NCBI gene targets.\n",
        "- NASA geospatial tags merged with GitHub repository metadata for climate studies.\n",
        "\n",
        "_(Code to be added in the next iteration.)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 5.4 ¬∑ Data Harmonization Implementation ===\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Optional\n",
        "\n",
        "# Define unified schemas\n",
        "class LiteratureRecord(BaseModel):\n",
        "    \"\"\"Normalized literature entry across all sources.\"\"\"\n",
        "    record_id: str\n",
        "    title: str\n",
        "    abstract: Optional[str] = None\n",
        "    authors: List[str] = Field(default_factory=list)\n",
        "    publication_date: Optional[str] = None\n",
        "    source_db: str\n",
        "    doi: Optional[str] = None\n",
        "    citations: Optional[int] = None\n",
        "    url: Optional[str] = None\n",
        "\n",
        "# Harmonization function\n",
        "def harmonize_literature(corpus: Dict) -> List[LiteratureRecord]:\n",
        "    \"\"\"Convert multi-source corpus to unified schema.\"\"\"\n",
        "    unified_records = []\n",
        "    \n",
        "    for topic, sources in corpus.items():\n",
        "        # PubMed entries\n",
        "        if \"pubmed\" in sources:\n",
        "            for pmid in sources[\"pubmed\"].get(\"ids\", [])[:5]:  # Sample first 5\n",
        "                unified_records.append(LiteratureRecord(\n",
        "                    record_id=f\"PMID:{pmid}\",\n",
        "                    title=f\"PubMed Article {pmid}\",\n",
        "                    source_db=\"PubMed\",\n",
        "                    url=f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/\"\n",
        "                ))\n",
        "        \n",
        "        # arXiv entries\n",
        "        if \"arxiv\" in sources:\n",
        "            for paper in sources[\"arxiv\"].get(\"papers\", [])[:5]:\n",
        "                unified_records.append(LiteratureRecord(\n",
        "                    record_id=paper.get(\"id\", \"\"),\n",
        "                    title=paper.get(\"title\", \"\"),\n",
        "                    abstract=paper.get(\"summary\", \"\"),\n",
        "                    authors=paper.get(\"authors\", []),\n",
        "                    publication_date=paper.get(\"published\", \"\"),\n",
        "                    source_db=\"arXiv\",\n",
        "                    url=paper.get(\"id\", \"\")\n",
        "                ))\n",
        "        \n",
        "        # Semantic Scholar entries\n",
        "        if \"semantic\" in sources:\n",
        "            for paper in sources[\"semantic\"].get(\"papers\", [])[:5]:\n",
        "                unified_records.append(LiteratureRecord(\n",
        "                    record_id=paper.get(\"paperId\", \"\"),\n",
        "                    title=paper.get(\"title\", \"\"),\n",
        "                    authors=[a.get(\"name\", \"\") for a in paper.get(\"authors\", [])],\n",
        "                    publication_date=str(paper.get(\"year\", \"\")),\n",
        "                    source_db=\"Semantic Scholar\",\n",
        "                    citations=paper.get(\"citationCount\", 0),\n",
        "                    url=paper.get(\"url\", \"\")\n",
        "                ))\n",
        "        \n",
        "        # CrossRef entries\n",
        "        if \"crossref\" in sources:\n",
        "            for work in sources[\"crossref\"].get(\"works\", [])[:5]:\n",
        "                unified_records.append(LiteratureRecord(\n",
        "                    record_id=work.get(\"DOI\", \"\"),\n",
        "                    title=work.get(\"title\", [\"\"])[0] if work.get(\"title\") else \"\",\n",
        "                    doi=work.get(\"DOI\", \"\"),\n",
        "                    publication_date=str(work.get(\"created\", {}).get(\"date-time\", \"\")),\n",
        "                    source_db=\"CrossRef\",\n",
        "                    citations=work.get(\"is-referenced-by-count\", 0)\n",
        "                ))\n",
        "    \n",
        "    return unified_records\n",
        "\n",
        "if RESEARCH_CONNECTORS_AVAILABLE and literature_corpus:\n",
        "    harmonized_data = harmonize_literature(literature_corpus)\n",
        "    print(f\"‚úÖ Harmonized {len(harmonized_data)} literature records\")\n",
        "    print(f\"\\nSample harmonized record:\")\n",
        "    if harmonized_data:\n",
        "        print(json.dumps(harmonized_data[0].dict(), indent=2))\n",
        "else:\n",
        "    harmonized_data = []\n",
        "    print(\"‚ö†Ô∏è  No data to harmonize\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 5.5 ¬∑ Deduplication & Validation ===\n",
        "from collections import defaultdict\n",
        "\n",
        "def deduplicate_records(records: List[LiteratureRecord]) -> List[LiteratureRecord]:\n",
        "    \"\"\"Remove duplicates based on DOI and record_id.\"\"\"\n",
        "    seen_ids = set()\n",
        "    seen_dois = set()\n",
        "    unique_records = []\n",
        "    \n",
        "    for record in records:\n",
        "        # Check DOI first (more reliable)\n",
        "        if record.doi and record.doi in seen_dois:\n",
        "            continue\n",
        "        # Then check record ID\n",
        "        if record.record_id in seen_ids:\n",
        "            continue\n",
        "        \n",
        "        if record.doi:\n",
        "            seen_dois.add(record.doi)\n",
        "        seen_ids.add(record.record_id)\n",
        "        unique_records.append(record)\n",
        "    \n",
        "    return unique_records\n",
        "\n",
        "def validate_records(records: List[LiteratureRecord]) -> Dict[str, Any]:\n",
        "    \"\"\"Generate validation report.\"\"\"\n",
        "    report = {\n",
        "        \"total_records\": len(records),\n",
        "        \"by_source\": defaultdict(int),\n",
        "        \"with_doi\": 0,\n",
        "        \"with_abstract\": 0,\n",
        "        \"with_citations\": 0,\n",
        "        \"avg_citations\": 0,\n",
        "        \"validation_errors\": []\n",
        "    }\n",
        "    \n",
        "    total_citations = 0\n",
        "    citation_count = 0\n",
        "    \n",
        "    for record in records:\n",
        "        report[\"by_source\"][record.source_db] += 1\n",
        "        if record.doi:\n",
        "            report[\"with_doi\"] += 1\n",
        "        if record.abstract:\n",
        "            report[\"with_abstract\"] += 1\n",
        "        if record.citations and record.citations > 0:\n",
        "            report[\"with_citations\"] += 1\n",
        "            total_citations += record.citations\n",
        "            citation_count += 1\n",
        "        \n",
        "        # Validation checks\n",
        "        if not record.title or len(record.title) < 10:\n",
        "            report[\"validation_errors\"].append(f\"Short/missing title: {record.record_id}\")\n",
        "    \n",
        "    if citation_count > 0:\n",
        "        report[\"avg_citations\"] = round(total_citations / citation_count, 2)\n",
        "    \n",
        "    return dict(report)\n",
        "\n",
        "if harmonized_data:\n",
        "    # Deduplicate\n",
        "    original_count = len(harmonized_data)\n",
        "    harmonized_data = deduplicate_records(harmonized_data)\n",
        "    print(f\"üîç Deduplication: {original_count} ‚Üí {len(harmonized_data)} records\")\n",
        "    print(f\"   Removed {original_count - len(harmonized_data)} duplicates\\n\")\n",
        "    \n",
        "    # Validate\n",
        "    validation_report = validate_records(harmonized_data)\n",
        "    print(\"üìä Validation Report:\")\n",
        "    print(f\"   Total records: {validation_report['total_records']}\")\n",
        "    print(f\"   By source: {dict(validation_report['by_source'])}\")\n",
        "    print(f\"   With DOI: {validation_report['with_doi']}\")\n",
        "    print(f\"   With abstract: {validation_report['with_abstract']}\")\n",
        "    print(f\"   With citations: {validation_report['with_citations']}\")\n",
        "    print(f\"   Avg citations: {validation_report['avg_citations']}\")\n",
        "    if validation_report['validation_errors']:\n",
        "        print(f\"   ‚ö†Ô∏è  Validation errors: {len(validation_report['validation_errors'])}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No data to validate\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6 ¬∑ Analysis & Visualization Plan\n",
        "\n",
        "| Analysis Track | Metric / Visualization | Intended Insight |\n",
        "|----------------|------------------------|------------------|\n",
        "| Literature Coverage | Recall vs. topic benchmark, publication trend lines | Validate multi-connector completeness |\n",
        "| Citation Network | Degree/betweenness, chord diagram | Identify influential authors/institutions |\n",
        "| Collaboration Geography | Choropleth, affiliation bipartite graph | Map global partnerships |\n",
        "| Compound Screening | Lipinski compliance histogram, similarity heatmap | Surface tractable leads |\n",
        "| Code-Dataset Alignment | Sankey diagram (GitHub ‚Üî NASA) | Show reproducibility pipeline |\n",
        "\n",
        "Planned tooling: `matplotlib`, `plotly`, `networkx`, `geopandas` (optional), plus ApiLinker utilities.\n",
        "\n",
        "_(Visualizations will be implemented after data acquisition routines are finalized.)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 6.1 ¬∑ Literature Coverage Analysis ===\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set publication-quality style\n",
        "plt.style.use('seaborn-v0_8-paper')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "if harmonized_data:\n",
        "    # Aggregate statistics by source\n",
        "    source_stats = pd.DataFrame([\n",
        "        {\n",
        "            \"Database\": record.source_db,\n",
        "            \"Has_DOI\": 1 if record.doi else 0,\n",
        "            \"Has_Abstract\": 1 if record.abstract else 0,\n",
        "            \"Citations\": record.citations or 0\n",
        "        }\n",
        "        for record in harmonized_data\n",
        "    ])\n",
        "    \n",
        "    # Summary table\n",
        "    summary_table = source_stats.groupby(\"Database\").agg({\n",
        "        \"Has_DOI\": [\"sum\", \"count\"],\n",
        "        \"Has_Abstract\": \"sum\",\n",
        "        \"Citations\": [\"mean\", \"max\"]\n",
        "    }).round(2)\n",
        "    \n",
        "    print(\"üìä Table 1: Multi-Database Literature Summary\")\n",
        "    print(\"=\" * 70)\n",
        "    print(summary_table)\n",
        "    print()\n",
        "    \n",
        "    # Visualization: Record count by database\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Left: Record counts\n",
        "    source_counts = source_stats[\"Database\"].value_counts()\n",
        "    axes[0].barh(source_counts.index, source_counts.values, color=sns.color_palette(\"husl\", len(source_counts)))\n",
        "    axes[0].set_xlabel(\"Number of Records\")\n",
        "    axes[0].set_title(\"Records per Database\")\n",
        "    axes[0].grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    # Right: Metadata completeness\n",
        "    completeness = source_stats.groupby(\"Database\")[[\"Has_DOI\", \"Has_Abstract\"]].mean() * 100\n",
        "    completeness.plot(kind=\"bar\", ax=axes[1], rot=45)\n",
        "    axes[1].set_ylabel(\"Completeness (%)\")\n",
        "    axes[1].set_title(\"Metadata Completeness by Source\")\n",
        "    axes[1].legend([\"DOI Available\", \"Abstract Available\"])\n",
        "    axes[1].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save figure\n",
        "    fig_path = os.path.join(EXPORT_DIR, \"figure1_literature_coverage.png\")\n",
        "    plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"üíæ Figure 1 saved: {fig_path}\")\n",
        "    plt.show()\n",
        "    \n",
        "    # Export table\n",
        "    table_path = os.path.join(EXPORT_DIR, \"table1_literature_summary.csv\")\n",
        "    summary_table.to_csv(table_path)\n",
        "    print(f\"üíæ Table 1 exported: {table_path}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No data for analysis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 6.2 ¬∑ Citation Network Analysis ===\n",
        "import networkx as nx\n",
        "\n",
        "if harmonized_data and any(r.citations for r in harmonized_data):\n",
        "    # Build citation network (simplified: top-cited papers)\n",
        "    citation_data = [\n",
        "        (r.title[:50], r.citations, r.source_db) \n",
        "        for r in harmonized_data \n",
        "        if r.citations and r.citations > 0\n",
        "    ]\n",
        "    citation_data.sort(key=lambda x: x[1], reverse=True)\n",
        "    top_papers = citation_data[:15]  # Top 15 most cited\n",
        "    \n",
        "    # Create network graph\n",
        "    G = nx.Graph()\n",
        "    for title, cites, source in top_papers:\n",
        "        G.add_node(title, citations=cites, source=source)\n",
        "    \n",
        "    # Add edges between papers from same source (collaboration proxy)\n",
        "    source_groups = defaultdict(list)\n",
        "    for title, _, source in top_papers:\n",
        "        source_groups[source].append(title)\n",
        "    \n",
        "    for source, titles in source_groups.items():\n",
        "        for i in range(len(titles)):\n",
        "            for j in range(i+1, len(titles)):\n",
        "                G.add_edge(titles[i], titles[j], weight=0.5)\n",
        "    \n",
        "    # Compute network metrics\n",
        "    degree_centrality = nx.degree_centrality(G)\n",
        "    betweenness = nx.betweenness_centrality(G)\n",
        "    \n",
        "    network_metrics = pd.DataFrame([\n",
        "        {\n",
        "            \"Paper\": node[:40],\n",
        "            \"Citations\": G.nodes[node][\"citations\"],\n",
        "            \"Source\": G.nodes[node][\"source\"],\n",
        "            \"Degree\": round(degree_centrality[node], 3),\n",
        "            \"Betweenness\": round(betweenness[node], 3)\n",
        "        }\n",
        "        for node in G.nodes()\n",
        "    ]).sort_values(\"Citations\", ascending=False)\n",
        "    \n",
        "    print(\"üìä Table 2: Citation Network Metrics (Top Papers)\")\n",
        "    print(\"=\" * 70)\n",
        "    print(network_metrics.head(10).to_string(index=False))\n",
        "    \n",
        "    # Visualization\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    pos = nx.spring_layout(G, k=1, iterations=50, seed=RANDOM_SEED)\n",
        "    \n",
        "    # Node sizes based on citations\n",
        "    node_sizes = [G.nodes[node][\"citations\"] * 20 for node in G.nodes()]\n",
        "    # Node colors based on source\n",
        "    source_colors = {src: i for i, src in enumerate(set(G.nodes[n][\"source\"] for n in G.nodes()))}\n",
        "    node_colors = [source_colors[G.nodes[node][\"source\"]] for node in G.nodes()]\n",
        "    \n",
        "    nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, \n",
        "                           alpha=0.7, cmap=plt.cm.Set3, ax=ax)\n",
        "    nx.draw_networkx_edges(G, pos, alpha=0.2, ax=ax)\n",
        "    \n",
        "    ax.set_title(\"Citation Influence Network (Node size ‚àù citations)\", fontsize=14, pad=20)\n",
        "    ax.axis('off')\n",
        "    \n",
        "    # Save\n",
        "    fig_path = os.path.join(EXPORT_DIR, \"figure2_citation_network.png\")\n",
        "    plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"\\nüíæ Figure 2 saved: {fig_path}\")\n",
        "    plt.show()\n",
        "    \n",
        "    # Export metrics\n",
        "    table_path = os.path.join(EXPORT_DIR, \"table2_citation_metrics.csv\")\n",
        "    network_metrics.to_csv(table_path, index=False)\n",
        "    print(f\"üíæ Table 2 exported: {table_path}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Insufficient citation data for network analysis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 6.3 ¬∑ Researcher Collaboration Analysis (ORCID + Semantic Scholar) ===\n",
        "\n",
        "if RESEARCH_CONNECTORS_AVAILABLE:\n",
        "    print(\"üë• Researcher Collaboration Analysis\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Extract unique authors from harmonized data\n",
        "    all_authors = []\n",
        "    for record in harmonized_data:\n",
        "        all_authors.extend(record.authors)\n",
        "    \n",
        "    author_counts = pd.Series(all_authors).value_counts().head(20)\n",
        "    \n",
        "    print(f\"Total unique authors: {len(set(all_authors))}\")\n",
        "    print(f\"\\nTop 10 most frequent authors:\")\n",
        "    print(author_counts.head(10))\n",
        "    \n",
        "    # Search ORCID for top authors (sample)\n",
        "    orcid_profiles = []\n",
        "    for author_name in author_counts.head(5).index:\n",
        "        try:\n",
        "            results = connectors[\"orcid\"].search_researchers(author_name, max_results=1)\n",
        "            if results and results.get(\"num-found\", 0) > 0:\n",
        "                orcid_profiles.append({\n",
        "                    \"Name\": author_name,\n",
        "                    \"ORCID_Found\": True,\n",
        "                    \"Count\": results.get(\"num-found\", 0)\n",
        "                })\n",
        "        except Exception as e:\n",
        "            orcid_profiles.append({\"Name\": author_name, \"ORCID_Found\": False})\n",
        "        time.sleep(0.5)  # Rate limit\n",
        "    \n",
        "    if orcid_profiles:\n",
        "        orcid_df = pd.DataFrame(orcid_profiles)\n",
        "        print(f\"\\nüìã ORCID Profile Discovery:\")\n",
        "        print(orcid_df.to_string(index=False))\n",
        "    \n",
        "    # Visualization: Author frequency distribution\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    author_counts.head(15).plot(kind='barh', ax=ax, color='steelblue')\n",
        "    ax.set_xlabel(\"Number of Papers\")\n",
        "    ax.set_title(\"Top 15 Authors by Publication Count in Corpus\")\n",
        "    ax.grid(axis='x', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    fig_path = os.path.join(EXPORT_DIR, \"figure3_author_distribution.png\")\n",
        "    plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"\\nüíæ Figure 3 saved: {fig_path}\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Connectors unavailable for collaboration analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7 ¬∑ Result Narratives & Reporting Artifacts\n",
        "\n",
        "**Target Outputs (to be generated):**\n",
        "- Table 1: Multi-database literature summary (counts, freshness, overlap).\n",
        "- Table 2: Collaboration metrics per institution.\n",
        "- Figure 1: Citation influence network.\n",
        "- Figure 2: Publication trend vs. NASA data availability.\n",
        "- Figure 3: Compound property distribution.\n",
        "- Supplementary: JSON/BibTeX exports, connector diagnostics log.\n",
        "\n",
        "> _Implementation note_: Each artifact will have an accompanying export cell (CSV/JSON/HTML) for direct journal submission packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 7.1 ¬∑ Comprehensive Results Summary ===\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"FINAL RESEARCH SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "summary_stats = {\n",
        "    \"Analysis\": [],\n",
        "    \"Metric\": [],\n",
        "    \"Value\": []\n",
        "}\n",
        "\n",
        "# Literature corpus stats\n",
        "if literature_corpus:\n",
        "    total_records = sum(\n",
        "        src.get(\"count\", 0)\n",
        "        for topic_data in literature_corpus.values()\n",
        "        for src in topic_data.values()\n",
        "    )\n",
        "    summary_stats[\"Analysis\"].extend([\"Literature\", \"Literature\", \"Literature\"])\n",
        "    summary_stats[\"Metric\"].extend([\"Total Records Fetched\", \"Unique After Dedup\", \"Databases Queried\"])\n",
        "    summary_stats[\"Value\"].extend([total_records, len(harmonized_data), 4])\n",
        "\n",
        "# Network analysis stats\n",
        "if harmonized_data:\n",
        "    cited_papers = [r for r in harmonized_data if r.citations and r.citations > 0]\n",
        "    summary_stats[\"Analysis\"].extend([\"Citation\", \"Citation\"])\n",
        "    summary_stats[\"Metric\"].extend([\"Papers with Citations\", \"Avg Citations\"])\n",
        "    summary_stats[\"Value\"].extend([\n",
        "        len(cited_papers),\n",
        "        round(sum(r.citations for r in cited_papers) / len(cited_papers), 1) if cited_papers else 0\n",
        "    ])\n",
        "\n",
        "# Collaboration stats\n",
        "if harmonized_data:\n",
        "    all_authors_final = [a for r in harmonized_data for a in r.authors]\n",
        "    summary_stats[\"Analysis\"].extend([\"Collaboration\", \"Collaboration\"])\n",
        "    summary_stats[\"Metric\"].extend([\"Total Authors\", \"Unique Authors\"])\n",
        "    summary_stats[\"Value\"].extend([len(all_authors_final), len(set(all_authors_final))])\n",
        "\n",
        "# Operational metrics\n",
        "summary_stats[\"Analysis\"].extend([\"System\", \"System\"])\n",
        "summary_stats[\"Metric\"].extend([\"Connectors Used\", \"Reproducibility Hash\"])\n",
        "summary_stats[\"Value\"].extend([len(connectors) if RESEARCH_CONNECTORS_AVAILABLE else 0, env_hash])\n",
        "\n",
        "summary_df = pd.DataFrame(summary_stats)\n",
        "print(\"\\nüìä Master Summary Table\")\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "# Export master summary\n",
        "summary_path = os.path.join(EXPORT_DIR, \"master_summary.csv\")\n",
        "summary_df.to_csv(summary_path, index=False)\n",
        "print(f\"\\nüíæ Master summary exported: {summary_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 7.2 ¬∑ Export Publication-Ready Artifacts ===\n",
        "\n",
        "print(\"\\nüì¶ Exporting Publication Artifacts\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# 1. BibTeX export for citations\n",
        "if harmonized_data:\n",
        "    bibtex_entries = []\n",
        "    for i, record in enumerate(harmonized_data[:20]):  # Export top 20\n",
        "        if record.doi:\n",
        "            entry = f\"\"\"@article{{record_{i+1},\n",
        "    title = {{{record.title}}},\n",
        "    doi = {{{record.doi}}},\n",
        "    year = {{{record.publication_date[:4] if record.publication_date else 'n.d.'}}},\n",
        "    journal = {{{record.source_db}}},\n",
        "    url = {{{record.url or 'https://doi.org/' + record.doi}}}\n",
        "}}\"\"\"\n",
        "            bibtex_entries.append(entry)\n",
        "    \n",
        "    bibtex_path = os.path.join(EXPORT_DIR, \"references.bib\")\n",
        "    with open(bibtex_path, \"w\") as f:\n",
        "        f.write(\"\\n\\n\".join(bibtex_entries))\n",
        "    print(f\"‚úì BibTeX library: {bibtex_path} ({len(bibtex_entries)} entries)\")\n",
        "\n",
        "# 2. JSON data bundle\n",
        "if harmonized_data:\n",
        "    json_bundle = {\n",
        "        \"metadata\": {\n",
        "            \"generated_at\": datetime.now().isoformat(),\n",
        "            \"apilinker_version\": apilinker_version,\n",
        "            \"environment_hash\": env_hash,\n",
        "            \"random_seed\": RANDOM_SEED\n",
        "        },\n",
        "        \"literature_records\": [r.dict() for r in harmonized_data],\n",
        "        \"summary_statistics\": summary_df.to_dict('records')\n",
        "    }\n",
        "    \n",
        "    json_path = os.path.join(EXPORT_DIR, \"research_data_bundle.json\")\n",
        "    with open(json_path, \"w\") as f:\n",
        "        json.dump(json_bundle, f, indent=2, default=str)\n",
        "    print(f\"‚úì JSON data bundle: {json_path}\")\n",
        "\n",
        "# 3. HTML report\n",
        "html_report = f\"\"\"<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>ApiLinker Research Report</title>\n",
        "    <style>\n",
        "        body {{ font-family: Arial, sans-serif; margin: 40px; }}\n",
        "        table {{ border-collapse: collapse; width: 100%; }}\n",
        "        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
        "        th {{ background-color: #4CAF50; color: white; }}\n",
        "        h1 {{ color: #333; }}\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>ApiLinker Multi-Source Research Intelligence Report</h1>\n",
        "    <p><strong>Generated:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
        "    <p><strong>Environment Hash:</strong> {env_hash}</p>\n",
        "    \n",
        "    <h2>Summary Statistics</h2>\n",
        "    {summary_df.to_html(index=False)}\n",
        "    \n",
        "    <h2>Literature Records (Sample)</h2>\n",
        "    {pd.DataFrame([r.dict() for r in harmonized_data[:10]]).to_html(index=False)}\n",
        "    \n",
        "    <hr>\n",
        "    <p><em>Generated by ApiLinker v{apilinker_version}</em></p>\n",
        "</body>\n",
        "</html>\"\"\"\n",
        "\n",
        "html_path = os.path.join(EXPORT_DIR, \"research_report.html\")\n",
        "with open(html_path, \"w\") as f:\n",
        "    f.write(html_report)\n",
        "print(f\"‚úì HTML report: {html_path}\")\n",
        "\n",
        "print(f\"\\n‚úÖ All artifacts exported to: {os.path.abspath(EXPORT_DIR)}\")\n",
        "print(f\"\\nExported files:\")\n",
        "for filename in os.listdir(EXPORT_DIR):\n",
        "    filepath = os.path.join(EXPORT_DIR, filename)\n",
        "    size = os.path.getsize(filepath) / 1024  # KB\n",
        "    print(f\"   ‚Ä¢ {filename} ({size:.1f} KB)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 6.4 ¬∑ Compound Discovery Pipeline (PubChem Integration) ===\n",
        "\n",
        "if RESEARCH_CONNECTORS_AVAILABLE:\n",
        "    print(\"‚öóÔ∏è  PubChem Compound Discovery\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Search for compounds related to a research theme\n",
        "    compound_query = \"CRISPR\"  # Related to drug_discovery topic\n",
        "    \n",
        "    try:\n",
        "        print(f\"Searching PubChem for: {compound_query}\")\n",
        "        compound_results = connectors[\"pubchem\"].search_compounds(\n",
        "            compound_query, max_results=10\n",
        "        )\n",
        "        \n",
        "        if compound_results and \"PC_Compounds\" in compound_results:\n",
        "            compounds = compound_results[\"PC_Compounds\"]\n",
        "            print(f\"‚úì Found {len(compounds)} compounds\\n\")\n",
        "            \n",
        "            # Extract compound properties\n",
        "            compound_data = []\n",
        "            for i, cmpd in enumerate(compounds[:5]):  # Analyze first 5\n",
        "                cid = cmpd.get(\"id\", {}).get(\"id\", {}).get(\"cid\")\n",
        "                if cid:\n",
        "                    try:\n",
        "                        props = connectors[\"pubchem\"].get_compound_properties(\n",
        "                            cid, properties=[\"MolecularWeight\", \"XLogP\", \"HBondDonorCount\", \"HBondAcceptorCount\"]\n",
        "                        )\n",
        "                        if props and \"PropertyTable\" in props:\n",
        "                            prop_data = props[\"PropertyTable\"][\"Properties\"][0]\n",
        "                            compound_data.append({\n",
        "                                \"CID\": cid,\n",
        "                                \"MolecularWeight\": prop_data.get(\"MolecularWeight\"),\n",
        "                                \"XLogP\": prop_data.get(\"XLogP\"),\n",
        "                                \"H_Donors\": prop_data.get(\"HBondDonorCount\"),\n",
        "                                \"H_Acceptors\": prop_data.get(\"HBondAcceptorCount\")\n",
        "                            })\n",
        "                        time.sleep(0.3)\n",
        "                    except:\n",
        "                        pass\n",
        "            \n",
        "            if compound_data:\n",
        "                compound_df = pd.DataFrame(compound_data)\n",
        "                print(\"üìä Compound Properties:\")\n",
        "                print(compound_df.to_string(index=False))\n",
        "                \n",
        "                # Lipinski's Rule of Five analysis\n",
        "                compound_df[\"Lipinski_Pass\"] = (\n",
        "                    (compound_df[\"MolecularWeight\"] <= 500) &\n",
        "                    (compound_df[\"XLogP\"] <= 5) &\n",
        "                    (compound_df[\"H_Donors\"] <= 5) &\n",
        "                    (compound_df[\"H_Acceptors\"] <= 10)\n",
        "                )\n",
        "                \n",
        "                print(f\"\\n‚úÖ Lipinski Rule of Five compliance: {compound_df['Lipinski_Pass'].sum()}/{len(compound_df)}\")\n",
        "                \n",
        "                # Visualization\n",
        "                fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "                \n",
        "                compound_df[\"MolecularWeight\"].plot(kind='bar', ax=axes[0,0], color='coral', title='Molecular Weight')\n",
        "                axes[0,0].axhline(y=500, color='r', linestyle='--', label='Lipinski limit')\n",
        "                axes[0,0].legend()\n",
        "                \n",
        "                compound_df[\"XLogP\"].plot(kind='bar', ax=axes[0,1], color='skyblue', title='LogP (Lipophilicity)')\n",
        "                axes[0,1].axhline(y=5, color='r', linestyle='--', label='Lipinski limit')\n",
        "                axes[0,1].legend()\n",
        "                \n",
        "                compound_df[\"H_Donors\"].plot(kind='bar', ax=axes[1,0], color='lightgreen', title='H-Bond Donors')\n",
        "                axes[1,0].axhline(y=5, color='r', linestyle='--', label='Lipinski limit')\n",
        "                axes[1,0].legend()\n",
        "                \n",
        "                compound_df[\"H_Acceptors\"].plot(kind='bar', ax=axes[1,1], color='plum', title='H-Bond Acceptors')\n",
        "                axes[1,1].axhline(y=10, color='r', linestyle='--', label='Lipinski limit')\n",
        "                axes[1,1].legend()\n",
        "                \n",
        "                for ax in axes.flat:\n",
        "                    ax.set_xlabel(\"Compound Index\")\n",
        "                    ax.grid(axis='y', alpha=0.3)\n",
        "                \n",
        "                plt.tight_layout()\n",
        "                fig_path = os.path.join(EXPORT_DIR, \"figure4_compound_properties.png\")\n",
        "                plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
        "                print(f\"\\nüíæ Figure 4 saved: {fig_path}\")\n",
        "                plt.show()\n",
        "                \n",
        "                # Export\n",
        "                table_path = os.path.join(EXPORT_DIR, \"table3_compound_data.csv\")\n",
        "                compound_df.to_csv(table_path, index=False)\n",
        "                print(f\"üíæ Table 3 exported: {table_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  PubChem query failed: {e}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  PubChem connector unavailable\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8 ¬∑ Discussion, Limitations, and Future Work\n",
        "\n",
        "- **Interpretation**: Connect literature gaps to data availability; highlight interdisciplinary findings.\n",
        "- **Limitations**: API rate limits, coverage biases, credential constraints, data licensing considerations.\n",
        "- **Future Enhancements**: Streaming connectors, active learning for topic expansion, deeper provenance graphs.\n",
        "\n",
        "---\n",
        "\n",
        "## Appendix (Planned Sections)\n",
        "\n",
        "1. **A ¬∑ Connector Credential Matrix** ‚Äì required scopes, rate limits, sample config snippet.\n",
        "2. **B ¬∑ Error Taxonomy** ‚Äì categorized retryable vs. fatal errors, mitigation strategies.\n",
        "3. **C ¬∑ Reproducibility Checklist** ‚Äì environment hash, data hashes, artifact manifest.\n",
        "4. **D ¬∑ References** ‚Äì auto-generated via CrossRef once data is pulled.\n",
        "\n",
        "*Next step: populate each section with executable code and analyses following this scaffold.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Appendix A: Connector Credential Requirements\n",
        "\n",
        "| Connector | Required Credentials | Rate Limits | Documentation |\n",
        "|-----------|---------------------|-------------|---------------|\n",
        "| NCBI | Email (courtesy) | 3 req/sec without key, 10/sec with | https://www.ncbi.nlm.nih.gov/books/NBK25497/ |\n",
        "| arXiv | None | 1 req/3 sec recommended | https://info.arxiv.org/help/api/index.html |\n",
        "| CrossRef | Email (courtesy) | 50 req/sec for polite users | https://www.crossref.org/documentation/retrieve-metadata/rest-api/ |\n",
        "| Semantic Scholar | Optional API key | 100 req/5 min (anon), higher with key | https://www.semanticscholar.org/product/api |\n",
        "| PubChem | None | 5 req/sec | https://pubchem.ncbi.nlm.nih.gov/docs/pug-rest |\n",
        "| ORCID | Optional token | Public API throttled | https://info.orcid.org/documentation/integration-guide/ |\n",
        "| GitHub | Optional token | 60 req/hr (anon), 5000/hr (auth) | https://docs.github.com/en/rest |\n",
        "| NASA | API key (DEMO usable) | 1000 req/hr with DEMO_KEY | https://api.nasa.gov/ |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Appendix B: Error Handling Taxonomy\n",
        "\n",
        "**Retryable Errors** (handled with exponential backoff):\n",
        "- Network timeouts (`httpx.TimeoutException`)\n",
        "- Rate limit responses (HTTP 429)\n",
        "- Temporary service unavailability (HTTP 503)\n",
        "\n",
        "**Fatal Errors** (require manual intervention):\n",
        "- Authentication failures (HTTP 401/403)\n",
        "- Invalid query syntax (HTTP 400)\n",
        "- Resource not found (HTTP 404)\n",
        "\n",
        "**Mitigation Strategies**:\n",
        "1. Implement `fetch_with_retry()` wrapper with configurable backoff\n",
        "2. Cache successful responses to avoid redundant requests\n",
        "3. Monitor connector health via observability hooks\n",
        "4. Fallback to alternative databases on persistent failures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Appendix C: Reproducibility Manifest ===\n",
        "\n",
        "manifest = {\n",
        "    \"notebook_version\": \"1.0.0\",\n",
        "    \"execution_timestamp\": datetime.now().isoformat(),\n",
        "    \"environment\": {\n",
        "        \"python_version\": sys.version,\n",
        "        \"platform\": platform.platform(),\n",
        "        \"apilinker_version\": apilinker_version,\n",
        "        \"environment_hash\": env_hash,\n",
        "        \"random_seed\": RANDOM_SEED\n",
        "    },\n",
        "    \"dependencies\": deps,\n",
        "    \"data_sources\": {\n",
        "        name: str(conn.base_url) \n",
        "        for name, conn in connectors.items()\n",
        "    } if RESEARCH_CONNECTORS_AVAILABLE else {},\n",
        "    \"outputs\": {\n",
        "        \"cache_directory\": CACHE_DIR,\n",
        "        \"export_directory\": EXPORT_DIR,\n",
        "        \"artifacts\": os.listdir(EXPORT_DIR) if os.path.exists(EXPORT_DIR) else []\n",
        "    },\n",
        "    \"data_hashes\": {}\n",
        "}\n",
        "\n",
        "# Compute hashes of exported files\n",
        "for filename in manifest[\"outputs\"][\"artifacts\"]:\n",
        "    filepath = os.path.join(EXPORT_DIR, filename)\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        file_hash = hashlib.sha256(f.read()).hexdigest()[:16]\n",
        "        manifest[\"data_hashes\"][filename] = file_hash\n",
        "\n",
        "# Save manifest\n",
        "manifest_path = os.path.join(EXPORT_DIR, \"reproducibility_manifest.json\")\n",
        "with open(manifest_path, \"w\") as f:\n",
        "    json.dump(manifest, f, indent=2, default=str)\n",
        "\n",
        "print(\"üìã Reproducibility Manifest\")\n",
        "print(\"=\" * 70)\n",
        "print(json.dumps(manifest, indent=2, default=str))\n",
        "print(f\"\\nüíæ Manifest saved: {manifest_path}\")\n",
        "print(f\"\\n‚úÖ Notebook execution complete. All outputs are deterministic and traceable.\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
