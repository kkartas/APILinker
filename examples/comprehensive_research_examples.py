"""
Comprehensive Research Examples using ApiLinker's Full Connector Ecosystem.

This example demonstrates how to use all 8 research connectors together
for complex, multi-domain research workflows.
"""

from apilinker import (
    # Scientific connectors
    NCBIConnector, ArXivConnector, CrossRefConnector, 
    SemanticScholarConnector, PubChemConnector, ORCIDConnector,
    # General research connectors
    GitHubConnector, NASAConnector,
    # Core
    ApiLinker
)


def demo_all_connectors():
    """
    Demonstrate initialization and basic usage of all research connectors.
    """
    print("üöÄ ApiLinker Comprehensive Research Connector Demo")
    print("=" * 60)
    
    # Scientific connectors
    print("\nüß¨ Scientific Connectors:")
    
    # 1. NCBI - Biomedical databases
    ncbi = NCBIConnector(email="researcher@university.edu")
    print(f"‚úÖ NCBI: {ncbi.base_url}")
    
    # 2. arXiv - Academic preprints
    arxiv = ArXivConnector()
    print(f"‚úÖ arXiv: {arxiv.base_url}")
    
    # 3. CrossRef - Citation data
    crossref = CrossRefConnector(email="researcher@university.edu")
    print(f"‚úÖ CrossRef: {crossref.base_url}")
    
    # 4. Semantic Scholar - AI-powered academic search
    semantic = SemanticScholarConnector(api_key="optional_key")
    print(f"‚úÖ Semantic Scholar: {semantic.base_url}")
    
    # 5. PubChem - Chemical compounds
    pubchem = PubChemConnector()
    print(f"‚úÖ PubChem: {pubchem.base_url}")
    
    # 6. ORCID - Researcher profiles
    orcid = ORCIDConnector()
    print(f"‚úÖ ORCID: {orcid.base_url}")
    
    # General research connectors
    print("\nüåê General Research Connectors:")
    
    # 7. GitHub - Code and repositories
    github = GitHubConnector(token="optional_token")
    print(f"‚úÖ GitHub: {github.base_url}")
    
    # 8. NASA - Earth science and space data
    nasa = NASAConnector(api_key="nasa_api_key")
    print(f"‚úÖ NASA: {nasa.base_url}")
    
    print(f"\nAll 8 research connectors initialized successfully!")


def cross_platform_drug_discovery_research():
    """
    Comprehensive drug discovery research across multiple platforms.
    """
    print("\nüíä Cross-Platform Drug Discovery Research")
    print("=" * 50)
    
    # Initialize connectors
    ncbi = NCBIConnector(email="drug.researcher@pharma.com")
    pubchem = PubChemConnector()
    semantic = SemanticScholarConnector()
    crossref = CrossRefConnector(email="drug.researcher@pharma.com")
    github = GitHubConnector()
    
    target_protein = "BRCA1"
    
    print(f"\nüéØ Researching drug targets for: {target_protein}")
    
    # 1. Literature research
    print(f"\nüìö Step 1: Literature Research")
    print(f"   ‚Ä¢ Searching PubMed for {target_protein} drug studies...")
    print(f"   ‚Ä¢ Searching Semantic Scholar for AI-based drug discovery...")
    print(f"   ‚Ä¢ CrossRef for citation analysis...")
    
    # 2. Chemical compound research
    print(f"\n‚öóÔ∏è  Step 2: Chemical Compound Research")
    print(f"   ‚Ä¢ Searching PubChem for {target_protein} inhibitors...")
    print(f"   ‚Ä¢ Analyzing drug-like properties (Lipinski's Rule of Five)...")
    print(f"   ‚Ä¢ Finding similar compounds and bioassays...")
    
    # 3. Computational resources
    print(f"\nüíª Step 3: Computational Resources")
    print(f"   ‚Ä¢ Searching GitHub for {target_protein} analysis tools...")
    print(f"   ‚Ä¢ Finding drug discovery machine learning repositories...")
    print(f"   ‚Ä¢ Identifying computational drug design software...")
    
    research_workflow = {
        'target': target_protein,
        'literature_sources': ['PubMed', 'Semantic Scholar', 'CrossRef'],
        'chemical_databases': ['PubChem'],
        'code_repositories': ['GitHub'],
        'analysis_types': [
            'Literature review',
            'Compound similarity analysis', 
            'Drug-likeness assessment',
            'Computational tool discovery'
        ]
    }
    
    print(f"\nüìä Research Workflow Summary:")
    for key, value in research_workflow.items():
        print(f"   {key}: {value}")
    
    return research_workflow


def climate_science_interdisciplinary_research():
    """
    Climate science research combining NASA data, literature, and code.
    """
    print("\nüåç Climate Science Interdisciplinary Research")
    print("=" * 50)
    
    # Initialize connectors
    nasa = NASAConnector(api_key="nasa_api_key")
    arxiv = ArXivConnector()
    github = GitHubConnector()
    semantic = SemanticScholarConnector()
    
    research_topic = "climate change machine learning"
    
    print(f"\nüî¨ Researching: {research_topic}")
    
    # 1. Earth observation data
    print(f"\nüõ∞Ô∏è  Step 1: NASA Earth Observation Data")
    print(f"   ‚Ä¢ Accessing satellite imagery and climate data...")
    print(f"   ‚Ä¢ Getting earth science datasets...")
    print(f"   ‚Ä¢ Analyzing climate indicators...")
    
    # 2. Academic research
    print(f"\nüìñ Step 2: Academic Literature")
    print(f"   ‚Ä¢ Searching arXiv for ML climate models...")
    print(f"   ‚Ä¢ Semantic Scholar for AI climate research...")
    print(f"   ‚Ä¢ Cross-referencing methodologies...")
    
    # 3. Implementation resources
    print(f"\n‚ö° Step 3: Implementation Resources")
    print(f"   ‚Ä¢ GitHub repositories for climate ML...")
    print(f"   ‚Ä¢ Data processing pipelines...")
    print(f"   ‚Ä¢ Visualization and analysis tools...")
    
    climate_research_framework = {
        'data_sources': {
            'nasa': 'Earth observation data, climate datasets',
            'arxiv': 'Latest ML methods for climate science',
            'semantic_scholar': 'Cross-disciplinary AI research',
            'github': 'Implementation code and tools'
        },
        'research_pipeline': [
            'Data collection (NASA APIs)',
            'Literature review (arXiv, Semantic Scholar)',
            'Method identification (Academic papers)',
            'Implementation discovery (GitHub)',
            'Integration and analysis'
        ],
        'interdisciplinary_aspects': [
            'Climate science + Machine learning',
            'Earth observation + Data science',
            'Environmental policy + Technology'
        ]
    }
    
    print(f"\nüå°Ô∏è  Climate Research Framework:")
    for category, details in climate_research_framework.items():
        print(f"   {category}:")
        if isinstance(details, dict):
            for source, description in details.items():
                print(f"      ‚Ä¢ {source}: {description}")
        elif isinstance(details, list):
            for item in details:
                print(f"      ‚Ä¢ {item}")
    
    return climate_research_framework


def researcher_collaboration_network_analysis():
    """
    Analyze researcher collaboration networks across platforms.
    """
    print("\nüë• Researcher Collaboration Network Analysis")
    print("=" * 50)
    
    # Initialize connectors
    orcid = ORCIDConnector()
    semantic = SemanticScholarConnector()
    github = GitHubConnector()
    arxiv = ArXivConnector()
    
    research_field = "bioinformatics"
    
    print(f"\nüîç Analyzing collaboration networks in: {research_field}")
    
    # 1. Researcher identification
    print(f"\nüë§ Step 1: Researcher Identification")
    print(f"   ‚Ä¢ ORCID: Finding researchers by affiliation and keywords...")
    print(f"   ‚Ä¢ Semantic Scholar: Identifying prolific authors...")
    print(f"   ‚Ä¢ arXiv: Recent paper authors in the field...")
    
    # 2. Collaboration analysis
    print(f"\nü§ù Step 2: Collaboration Analysis")
    print(f"   ‚Ä¢ Paper co-authorship patterns...")
    print(f"   ‚Ä¢ Institutional collaborations...")
    print(f"   ‚Ä¢ Cross-disciplinary partnerships...")
    
    # 3. Code collaboration
    print(f"\nüíª Step 3: Code Collaboration")
    print(f"   ‚Ä¢ GitHub: Open source project contributors...")
    print(f"   ‚Ä¢ Software development collaborations...")
    print(f"   ‚Ä¢ Academic-industry partnerships in code...")
    
    collaboration_metrics = {
        'researcher_identification': {
            'orcid_profiles': 'Professional researcher profiles',
            'semantic_scholar': 'Publication-based author profiles',
            'arxiv_authors': 'Recent publication authors'
        },
        'collaboration_patterns': {
            'co_authorship': 'Paper collaboration networks',
            'institutional': 'Cross-institutional research',
            'temporal': 'Collaboration evolution over time'
        },
        'software_collaboration': {
            'github_contributors': 'Open source development teams',
            'academic_software': 'Research software collaborations',
            'reproducibility': 'Code sharing for research'
        },
        'network_metrics': [
            'Centrality measures',
            'Clustering coefficients', 
            'Community detection',
            'Collaboration strength'
        ]
    }
    
    print(f"\nüìä Collaboration Analysis Framework:")
    for category, details in collaboration_metrics.items():
        print(f"   {category}:")
        if isinstance(details, dict):
            for metric, description in details.items():
                print(f"      ‚Ä¢ {metric}: {description}")
        elif isinstance(details, list):
            for item in details:
                print(f"      ‚Ä¢ {item}")
    
    return collaboration_metrics


def technology_transfer_research():
    """
    Track technology transfer from academic research to industry implementation.
    """
    print("\nüîÑ Technology Transfer Research")
    print("=" * 40)
    
    # Initialize connectors
    arxiv = ArXivConnector()
    semantic = SemanticScholarConnector()
    github = GitHubConnector()
    crossref = CrossRefConnector(email="tech.transfer@university.edu")
    
    technology = "transformer neural networks"
    
    print(f"\n‚ö° Tracking technology transfer: {technology}")
    
    # 1. Academic origins
    print(f"\nüéì Step 1: Academic Origins")
    print(f"   ‚Ä¢ arXiv: Original research papers...")
    print(f"   ‚Ä¢ Semantic Scholar: Citation impact analysis...")
    print(f"   ‚Ä¢ CrossRef: Publication timeline and influence...")
    
    # 2. Industry adoption
    print(f"\nüè≠ Step 2: Industry Adoption")
    print(f"   ‚Ä¢ GitHub: Open source implementations...")
    print(f"   ‚Ä¢ Corporate research repositories...")
    print(f"   ‚Ä¢ Production deployment examples...")
    
    # 3. Transfer analysis
    print(f"\nüìà Step 3: Transfer Analysis")
    print(f"   ‚Ä¢ Time from publication to implementation...")
    print(f"   ‚Ä¢ Key researchers and organizations...")
    print(f"   ‚Ä¢ Adoption patterns and variations...")
    
    transfer_analysis = {
        'technology': technology,
        'academic_phase': {
            'initial_papers': 'Foundational research publications',
            'theoretical_development': 'Method refinement and validation',
            'academic_adoption': 'Widespread academic use'
        },
        'transition_phase': {
            'industry_attention': 'Corporate research interest',
            'proof_of_concept': 'Industry pilot implementations',
            'open_source_tools': 'Community-driven implementations'
        },
        'industry_phase': {
            'production_deployment': 'Large-scale commercial use',
            'standardization': 'Industry standard practices',
            'ecosystem_development': 'Supporting tools and services'
        },
        'transfer_metrics': [
            'Publication to implementation time',
            'Academic-industry collaboration frequency',
            'Open source project popularity',
            'Commercial adoption indicators'
        ]
    }
    
    print(f"\nüîÑ Technology Transfer Analysis:")
    for phase, details in transfer_analysis.items():
        if phase != 'technology':
            print(f"   {phase}:")
            if isinstance(details, dict):
                for stage, description in details.items():
                    print(f"      ‚Ä¢ {stage}: {description}")
            elif isinstance(details, list):
                for item in details:
                    print(f"      ‚Ä¢ {item}")
    
    return transfer_analysis


def integrated_research_workflow_demo():
    """
    Demonstrate a complete integrated research workflow using all connectors.
    """
    print("\nüåü Integrated Research Workflow Demo")
    print("=" * 45)
    
    # Initialize ApiLinker with all connectors
    linker = ApiLinker()
    
    print(f"\nüîß Setting up comprehensive research infrastructure...")
    
    # Research scenario: AI applications in healthcare
    research_topic = "artificial intelligence healthcare diagnostics"
    
    workflow_steps = {
        'literature_discovery': {
            'connectors': ['NCBI (PubMed)', 'arXiv', 'Semantic Scholar', 'CrossRef'],
            'purpose': 'Find relevant papers and analyze citations',
            'outputs': 'Comprehensive literature database'
        },
        'researcher_network': {
            'connectors': ['ORCID', 'Semantic Scholar'],
            'purpose': 'Identify key researchers and collaborations',
            'outputs': 'Researcher collaboration network'
        },
        'technology_analysis': {
            'connectors': ['GitHub', 'arXiv'],
            'purpose': 'Find implementations and code resources',
            'outputs': 'Technology landscape mapping'
        },
        'data_integration': {
            'connectors': ['NASA (for health-environment data)', 'PubChem (for drug interactions)'],
            'purpose': 'Integrate supporting datasets',
            'outputs': 'Multi-modal research dataset'
        },
        'synthesis_analysis': {
            'connectors': ['All connectors'],
            'purpose': 'Cross-reference and synthesize findings',
            'outputs': 'Comprehensive research synthesis'
        }
    }
    
    print(f"\nüìã Research Workflow: {research_topic}")
    for step_name, step_details in workflow_steps.items():
        print(f"\n   {step_name.replace('_', ' ').title()}:")
        for aspect, details in step_details.items():
            if isinstance(details, list):
                print(f"      {aspect}: {', '.join(details)}")
            else:
                print(f"      {aspect}: {details}")
    
    # Demonstrate the power of integrated research
    integration_benefits = [
        "Cross-database validation of research findings",
        "Comprehensive researcher collaboration mapping",
        "Technology implementation tracking",
        "Multi-modal data integration",
        "Automated research synthesis",
        "Real-time research monitoring",
        "Interdisciplinary connection discovery"
    ]
    
    print(f"\n‚ú® Integration Benefits:")
    for benefit in integration_benefits:
        print(f"   ‚Ä¢ {benefit}")
    
    research_impact = {
        'scope': 'Multi-database, multi-domain research',
        'efficiency': 'Automated multi-database searches',
        'completeness': 'Coverage across multiple research sources',
        'reproducibility': 'Documented and repeatable workflows',
        'collaboration': 'Supports research collaboration',
        'innovation': 'Helps discover connections across databases'
    }
    
    print(f"\nüöÄ Research Impact Metrics:")
    for metric, description in research_impact.items():
        print(f"   {metric}: {description}")
    
    return workflow_steps, integration_benefits, research_impact


if __name__ == "__main__":
    """
    Run comprehensive research demonstrations.
    """
    print("üî¨ ApiLinker Comprehensive Research Ecosystem Demo")
    print("=" * 70)
    print("Demonstrating all 8 research connectors in integrated workflows")
    print()
    
    try:
        # Basic connector demo
        demo_all_connectors()
        
        # Domain-specific workflows
        drug_discovery = cross_platform_drug_discovery_research()
        climate_research = climate_science_interdisciplinary_research()
        collaboration_analysis = researcher_collaboration_network_analysis()
        tech_transfer = technology_transfer_research()
        
        # Integrated workflow
        workflow, benefits, impact = integrated_research_workflow_demo()
        
        print("\n" + "=" * 70)
        print("Comprehensive Research Demo Completed Successfully!")
        print()
        print("Key Features Demonstrated:")
        print("   ‚Ä¢ 8 research connectors available")
        print("   ‚Ä¢ 5 example workflows provided")
        print("   ‚Ä¢ Cross-domain research capabilities")
        print("   ‚Ä¢ Research integration possibilities")
        print()
        print("Suitable for:")
        print("   ‚Ä¢ Research projects requiring multiple data sources")
        print("   ‚Ä¢ Cross-database literature searches")
        print("   ‚Ä¢ Interdisciplinary research workflows")
        print("   ‚Ä¢ Academic and research partnerships")
        print()
        print("ApiLinker: Simplifying research data integration")
        
    except Exception as e:
        print(f"\n‚ùå Demo error: {e}")
        print("This is expected in a demonstration environment.")
        print("The workflow structure demonstrates the research capabilities.")